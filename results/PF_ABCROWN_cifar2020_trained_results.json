{"instances": [{"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_0_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "SAT", "took": "7.054397344589233", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp95ffefse.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:53:58 2024 on Cerberus\nInternal results will be saved to /tmp/tmp95ffefse.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.67527163, -1.52271450,  0.63711810,  4.41396284,  0.79158354,\n          3.94307709,  1.38516212, -1.23928893, -1.21486461, -1.90701199]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.59891230, -1.67729628,  0.56920362,  4.30161333,  0.84799862,\n           4.37968922,  1.18421865, -1.15600204, -1.47534275, -1.94664717],\n         [-0.59891230, -1.67729628,  0.56920362,  4.30161333,  0.84799862,\n           4.37968922,  1.18421865, -1.15600204, -1.47534275, -1.94664717]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 4.90052557,  5.97890949,  3.73240972,  3.45361471, -0.07807589,\n           3.11739469,  5.45761538,  5.77695608,  6.24826050]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.6624 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.951476573944092\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_9_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "12.680113554000854", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp482rd092.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:54:05 2024 on Cerberus\nInternal results will be saved to /tmp/tmp482rd092.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n         -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.38077497,  7.11155891, -0.81550092,  0.36877590, -1.63488841,\n          -0.19252896,  0.41075173, -2.10415339,  2.38559103,  6.26932716],\n         [ 1.38077497,  7.11155891, -0.81550092,  0.36877590, -1.63488841,\n          -0.19252896,  0.41075173, -2.10415339,  2.38559103,  6.26932716]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.73078394, 7.92705965, 6.74278307, 8.74644756, 7.30408764,\n          6.70080709, 9.21571255, 4.72596788, 0.84223175]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.4843 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n         -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1386]; unstable size 1386; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [725]; unstable size 725; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [675]; unstable size 675; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.91427517,  4.55403137,  4.00052071,  5.54376125,  4.45263481,\n          3.79798126,  5.70426273,  1.83534622, -0.87034988]], device='cuda:0') None\nbest_l after optimization: 35.21476745605469 with beta sum per layer: []\nalpha/beta optimization time: 5.160121440887451\ninitial alpha-CROWN bounds: tensor([[ 3.25364780,  4.91604614,  4.39051819,  5.89016533,  4.82441044,\n          4.12997246,  6.11451912,  2.23822498, -0.54273891]], device='cuda:0')\nWorst class: (+ rhs) -0.5427389144897461\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 290.1574945449829\n##### Instance 0 first 10 spec matrices: [[[-1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.253647804260254.\n\nProperties batch 1, size 1\nRemaining timeout: 290.12203192710876\n##### Instance 0 first 10 spec matrices: [[[ 0.  1. -1.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.916046142578125.\n\nProperties batch 2, size 1\nRemaining timeout: 290.0996639728546\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0. -1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.3905181884765625.\n\nProperties batch 3, size 1\nRemaining timeout: 290.07846212387085\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 5.890165328979492.\n\nProperties batch 4, size 1\nRemaining timeout: 290.05746960639954\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0. -1.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.824410438537598.\n\nProperties batch 5, size 1\nRemaining timeout: 290.0371537208557\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0. -1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.129972457885742.\n\nProperties batch 6, size 1\nRemaining timeout: 290.0152428150177\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0. -1.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 6.114519119262695.\n\nProperties batch 7, size 1\nRemaining timeout: 289.9893229007721\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0.  0. -1.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.238224983215332.\n\nProperties batch 8, size 1\nRemaining timeout: 289.96138739585876\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0.  0.  0. -1.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.54273891]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n        -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.54273891]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1386\nlayer 1 size torch.Size([8192]) unstable 713\nlayer 2 size torch.Size([8192]) unstable 658\nlayer 3 size torch.Size([250]) unstable 51\n-----------------\n# of unstable neurons: 2808\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 56] \nsplit level 1: [3, 27] \nsplit level 2: [3, 169] \nsplit level 3: [3, 203] \nsplit level 4: [3, 43] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 30 / 32 = 0.9375\npruning-in-iteration extra time: 0.024997472763061523\nTensors transferred: pre=3.0153M lA=0.0942M alpha=0.1733M beta=0.0002M\nThis batch time : update_bounds func: 0.6805\t prepare: 0.0021\t bound: 0.6731\t transfer: 0.0048\t finalize: 0.0005\nAccumulated time: update_bounds func: 0.6805\t prepare: 0.0021\t bound: 0.6731\t transfer: 0.0048\t finalize: 0.0005\nbatch bounding time:  0.6805148124694824\nCurrent worst splitting domains lb-rhs (depth):\n-0.04987 (5), -0.02502 (5), \nlength of domains: 2\nTotal time: 0.9029\t pickout: 0.0009\t decision: 0.2164\t get_bound: 0.6838\t add_domain: 0.0018\nAccumulated time:\t pickout: 0.0009\t decision: 0.2164\t get_bound: 0.6838\t add_domain: 0.0018\nCurrent (lb-rhs): -0.04987049102783203\n30 domains visited\nCumulative time: 1.0785176753997803\n\nbatch:  torch.Size([2, 32, 32, 32]) pre split depth:  4\nbatch:  torch.Size([2, 32, 32, 32]) post split depth:  4\nsplitting decisions: \nsplit level 0: [3, 40] [3, 40] \nsplit level 1: [3, 112] [3, 112] \nsplit level 2: [3, 207] [3, 207] \nsplit level 3: [3, 116] [3, 116] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 32 / 32 = 1.0\npruning-in-iteration extra time: 0.0001380443572998047\nTensors transferred: pre=3.0153M lA=1.5076M alpha=0.1733M beta=0.0003M\nThis batch time : update_bounds func: 0.0122\t prepare: 0.0021\t bound: 0.0057\t transfer: 0.0040\t finalize: 0.0004\nAccumulated time: update_bounds func: 0.6926\t prepare: 0.0042\t bound: 0.6787\t transfer: 0.0087\t finalize: 0.0009\nbatch bounding time:  0.012232780456542969\nlength of domains: 0\nTotal time: 0.0392\t pickout: 0.0010\t decision: 0.0223\t get_bound: 0.0154\t add_domain: 0.0006\nAccumulated time:\t pickout: 0.0019\t decision: 0.2387\t get_bound: 0.6992\t add_domain: 0.0023\nNo domains left, verification finished!\n62 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 1.1184711456298828\n\nResult: unsat\nTime: 11.241088628768921\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_17_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.038635015487671", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp_40ijvbf.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:54:19 2024 on Cerberus\nInternal results will be saved to /tmp/tmp_40ijvbf.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.71794301, -2.73549056,  0.46490240,  2.03653789,  2.18022227,\n          1.30874860, -0.23802117,  5.58075476, -1.34880483, -1.73270965]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.48575890, -2.88859630,  0.55571985,  2.02314472,  2.39839435,\n           1.23517108, -0.19022425,  4.85715294, -1.25886095, -1.80295110],\n         [-0.48575890, -2.88859630,  0.55571985,  2.02314472,  2.39839435,\n           1.23517108, -0.19022425,  4.85715294, -1.25886095, -1.80295110]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.34291172, 7.74574947, 4.30143309, 2.83400822, 2.45875859,\n          3.62198186, 5.04737711, 6.11601400, 6.66010380]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5064 seconds.\nPGD attack failed\nModel prediction is: tensor([[-0.71794301, -2.73549056,  0.46490240,  2.03653789,  2.18022227,\n          1.30874860, -0.23802117,  5.58075476, -1.34880483, -1.73270965]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1626]; unstable size 1626; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [939]; unstable size 939; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [715]; unstable size 715; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [67]; unstable size 67; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[2.33360004, 3.98111701, 2.04257393, 1.23024154, 1.41948557, 1.87243569,\n         3.18333292, 3.42235184, 2.19061089]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.676571369171143\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_27_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "TIMEOUT", "took": "300", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpx7y_b86x.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:54:27 2024 on Cerberus\nInternal results will be saved to /tmp/tmpx7y_b86x.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n          0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 2.89373541, -4.66593504,  1.82701683,  0.85894787,  2.86403251,\n           0.92236614, -0.28302976,  1.52670109, -3.56230187, -0.78883207],\n         [ 2.89373541, -4.66593504,  1.82701683,  0.85894787,  2.86403251,\n           0.92236614, -0.28302976,  1.52670109, -3.56230187, -0.78883207]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[7.55967045, 1.06671858, 2.03478765, 0.02970290, 1.97136927,\n          3.17676520, 1.36703432, 6.45603752, 3.68256760]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.4552 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n          0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1437]; unstable size 1437; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [675]; unstable size 675; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [555]; unstable size 555; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 5.91306973, -0.15005469,  0.85581923, -0.97932100,  0.71283913,\n          1.95830727, -0.64448214,  4.80564022,  1.95628071]], device='cuda:0') None\nbest_l after optimization: 15.584519386291504 with beta sum per layer: []\nalpha/beta optimization time: 5.0196521282196045\ninitial alpha-CROWN bounds: tensor([[ 6.05828190, -0.03196764,  0.96516776, -0.86231685,  0.83091259,\n          2.05264521, -0.49636316,  4.94511318,  2.12304711]], device='cuda:0')\nWorst class: (+ rhs) -0.8623168468475342\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 290.38555216789246\n##### Instance 0 first 10 spec matrices: [[[ 1. -1.  0.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 6.058281898498535.\n\nProperties batch 1, size 1\nRemaining timeout: 290.35041093826294\n##### Instance 0 first 10 spec matrices: [[[ 1.  0. -1.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.03196764]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n         0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.03196764]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1437\nlayer 1 size torch.Size([8192]) unstable 665\nlayer 2 size torch.Size([8192]) unstable 541\nlayer 3 size torch.Size([250]) unstable 53\n-----------------\n# of unstable neurons: 2696\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 123] \nsplit level 1: [3, 247] \nsplit level 2: [3, 248] \nsplit level 3: [3, 126] \nsplit level 4: [3, 55] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 32 / 32 = 1.0\npruning-in-iteration extra time: 0.00018906593322753906\nTensors transferred: pre=3.0153M lA=1.5076M alpha=0.1661M beta=0.0002M\nThis batch time : update_bounds func: 0.0162\t prepare: 0.0028\t bound: 0.0097\t transfer: 0.0032\t finalize: 0.0004\nAccumulated time: update_bounds func: 0.0162\t prepare: 0.0028\t bound: 0.0097\t transfer: 0.0032\t finalize: 0.0004\nbatch bounding time:  0.01626443862915039\nlength of domains: 0\nTotal time: 0.2335\t pickout: 0.0009\t decision: 0.2123\t get_bound: 0.0196\t add_domain: 0.0006\nAccumulated time:\t pickout: 0.0009\t decision: 0.2123\t get_bound: 0.0196\t add_domain: 0.0006\nNo domains left, verification finished!\n32 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 0.4096529483795166\n\n\nProperties batch 2, size 1\nRemaining timeout: 289.86429023742676\n##### Instance 0 first 10 spec matrices: [[[ 1.  0.  0. -1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.965167760848999.\n\nProperties batch 3, size 1\nRemaining timeout: 289.83277654647827\n##### Instance 0 first 10 spec matrices: [[[ 1.  0.  0.  0. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.86231685]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n         0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.86231685]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1437\nlayer 1 size torch.Size([8192]) unstable 665\nlayer 2 size torch.Size([8192]) unstable 541\nlayer 3 size torch.Size([250]) unstable 53\n-----------------\n# of unstable neurons: 2696\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 134] \nsplit level 1: [3, 247] \nsplit level 2: [3, 123] \nsplit level 3: [3, 71] \nsplit level 4: [3, 52] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 11 / 32 = 0.34375\npruning-in-iteration extra time: 0.020896434783935547\nTensors transferred: pre=3.0153M lA=0.9894M alpha=0.1661M beta=0.0002M\nThis batch time : update_bounds func: 0.6883\t prepare: 0.0026\t bound: 0.6819\t transfer: 0.0033\t finalize: 0.0004\nAccumulated time: update_bounds func: 0.7045\t prepare: 0.0054\t bound: 0.6916\t transfer: 0.0065\t finalize: 0.0008\nbatch bounding time:  0.6883549690246582\nCurrent worst splitting domains lb-rhs (depth):\n-0.51872 (5), -0.50867 (5), -0.47914 (5), -0.47639 (5), -0.46053 (5), -0.44768 (5), -0.40704 (5), -0.39984 (5), -0.26980 (5), -0.25875 (5), -0.25381 (5), -0.24612 (5), -0.24009 (5), -0.22822 (5), -0.22390 (5), -0.21487 (5), -0.19789 (5), -0.18600 (5), -0.16737 (5), -0.14893 (5), \nlength of domains: 21\nTotal time: 0.7234\t pickout: 0.0007\t decision: 0.0276\t get_bound: 0.6916\t add_domain: 0.0034\nAccumulated time:\t pickout: 0.0007\t decision: 0.0276\t get_bound: 0.6916\t add_domain: 0.0034\nCurrent (lb-rhs): -0.5187232494354248\n11 domains visited\nCumulative time: 0.7291169166564941\n\nbatch:  torch.Size([21, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([21, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] \n(42, 3, 32, 32) torch.Size([42, 1, 10]) torch.Size([42, 1])\npruning_in_iteration open status: True\nratio of positive domain = 14 / 42 = 0.33333333333333337\npruning-in-iteration extra time: 0.017802000045776367\nTensors transferred: pre=3.9575M lA=1.3192M alpha=0.2180M beta=0.0002M\nThis batch time : update_bounds func: 0.4829\t prepare: 0.0025\t bound: 0.4760\t transfer: 0.0039\t finalize: 0.0005\nAccumulated time: update_bounds func: 1.1874\t prepare: 0.0079\t bound: 1.1676\t transfer: 0.0104\t finalize: 0.0014\nbatch bounding time:  0.4829270839691162\nCurrent worst splitting domains lb-rhs (depth):\n-0.47139 (6), -0.46284 (6), -0.43350 (6), -0.43137 (6), -0.41344 (6), -0.40248 (6), -0.36403 (6), -0.36149 (6), -0.35578 (6), -0.33286 (6), -0.29661 (6), -0.28733 (6), -0.27547 (6), -0.24269 (6), -0.22407 (6), -0.21333 (6), -0.20804 (6), -0.19957 (6), -0.19530 (6), -0.19490 (6), \nlength of domains: 28\nTotal time: 0.5106\t pickout: 0.0012\t decision: 0.0227\t get_bound: 0.4830\t add_domain: 0.0036\nAccumulated time:\t pickout: 0.0019\t decision: 0.0503\t get_bound: 1.1746\t add_domain: 0.0071\nCurrent (lb-rhs): -0.4713864326477051\n25 domains visited\nCumulative time: 1.2399427890777588\n\nbatch:  torch.Size([28, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([28, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] \n(56, 3, 32, 32) torch.Size([56, 1, 10]) torch.Size([56, 1])\npruning_in_iteration open status: True\nratio of positive domain = 12 / 56 = 0.2142857142857143\npruning-in-iteration extra time: 0.00014972686767578125\nTensors transferred: pre=5.2767M lA=2.6384M alpha=0.2906M beta=0.0004M\nThis batch time : update_bounds func: 0.4466\t prepare: 0.0028\t bound: 0.4374\t transfer: 0.0055\t finalize: 0.0008\nAccumulated time: update_bounds func: 1.6340\t prepare: 0.0107\t bound: 1.6050\t transfer: 0.0159\t finalize: 0.0021\nbatch bounding time:  0.44664692878723145\nCurrent worst splitting domains lb-rhs (depth):\n-0.43693 (7), -0.42708 (7), -0.39961 (7), -0.39623 (7), -0.37680 (7), -0.36551 (7), -0.34704 (7), -0.33570 (7), -0.32635 (7), -0.32498 (7), -0.31908 (7), -0.29468 (7), -0.28918 (7), -0.28457 (7), -0.27956 (7), -0.27038 (7), -0.25666 (7), -0.24449 (7), -0.23416 (7), -0.22628 (7), \nlength of domains: 44\nTotal time: 0.4834\t pickout: 0.0012\t decision: 0.0306\t get_bound: 0.4467\t add_domain: 0.0049\nAccumulated time:\t pickout: 0.0031\t decision: 0.0809\t get_bound: 1.6213\t add_domain: 0.0119\nCurrent (lb-rhs): -0.4369349479675293\n37 domains visited\nCumulative time: 1.7235627174377441\n\nbatch:  torch.Size([44, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([44, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] \n(88, 3, 32, 32) torch.Size([88, 1, 10]) torch.Size([88, 1])\npruning_in_iteration open status: True\nratio of positive domain = 25 / 88 = 0.28409090909090906\npruning-in-iteration extra time: 0.017210721969604492\nTensors transferred: pre=8.2920M lA=2.9681M alpha=0.4567M beta=0.0007M\nThis batch time : update_bounds func: 0.5126\t prepare: 0.0051\t bound: 0.4968\t transfer: 0.0095\t finalize: 0.0011\nAccumulated time: update_bounds func: 2.1466\t prepare: 0.0158\t bound: 2.1018\t transfer: 0.0254\t finalize: 0.0032\nbatch bounding time:  0.5126876831054688\nCurrent worst splitting domains lb-rhs (depth):\n-0.40897 (8), -0.39871 (8), -0.37132 (8), -0.36745 (8), -0.34902 (8), -0.33765 (8), -0.31770 (8), -0.30631 (8), -0.29811 (8), -0.29695 (8), -0.29110 (8), -0.29056 (8), -0.28809 (8), -0.26677 (8), -0.26006 (8), -0.25461 (8), -0.25300 (8), -0.24933 (8), -0.24835 (8), -0.24094 (8), \nlength of domains: 63\nTotal time: 0.5574\t pickout: 0.0016\t decision: 0.0366\t get_bound: 0.5127\t add_domain: 0.0065\nAccumulated time:\t pickout: 0.0047\t decision: 0.1175\t get_bound: 2.1340\t add_domain: 0.0185\nCurrent (lb-rhs): -0.4089670181274414\n62 domains visited\nCumulative time: 2.28133487701416\n\nbatch:  torch.Size([63, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([63, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] \n(126, 3, 32, 32) torch.Size([126, 1, 10]) torch.Size([126, 1])\npruning_in_iteration open status: False\nratio of positive domain = 13 / 126 = 0.10317460317460314\npruning-in-iteration extra time: 0.0001494884490966797\nTensors transferred: pre=11.8726M lA=5.9363M alpha=0.6539M beta=0.0011M\nThis batch time : update_bounds func: 0.5041\t prepare: 0.0050\t bound: 0.4837\t transfer: 0.0140\t finalize: 0.0013\nAccumulated time: update_bounds func: 2.6507\t prepare: 0.0208\t bound: 2.5855\t transfer: 0.0394\t finalize: 0.0045\nbatch bounding time:  0.5041143894195557\nCurrent worst splitting domains lb-rhs (depth):\n-0.38090 (9), -0.37239 (9), -0.37092 (9), -0.36096 (9), -0.34137 (9), -0.33827 (9), -0.33674 (9), -0.33229 (9), -0.32070 (9), -0.31301 (9), -0.30942 (9), -0.30053 (9), -0.28946 (9), -0.27811 (9), -0.27461 (9), -0.26701 (9), -0.26687 (9), -0.26371 (9), -0.26330 (9), -0.26261 (9), \nlength of domains: 113\nTotal time: 0.5600\t pickout: 0.0021\t decision: 0.0439\t get_bound: 0.5042\t add_domain: 0.0099\nAccumulated time:\t pickout: 0.0068\t decision: 0.1614\t get_bound: 2.6382\t add_domain: 0.0283\nCurrent (lb-rhs): -0.38089609146118164\n75 domains visited\nCumulative time: 2.841729164123535\n\nbatch:  torch.Size([113, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([113, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] \n(226, 3, 32, 32) torch.Size([226, 1, 10]) torch.Size([226, 1])\npruning_in_iteration open status: True\nratio of positive domain = 110 / 226 = 0.48672566371681414\npruning-in-iteration extra time: 0.026251792907714844\nTensors transferred: pre=21.2953M lA=5.4652M alpha=1.1729M beta=0.0022M\nThis batch time : update_bounds func: 0.6222\t prepare: 0.0080\t bound: 0.5868\t transfer: 0.0251\t finalize: 0.0021\nAccumulated time: update_bounds func: 3.2729\t prepare: 0.0289\t bound: 3.1722\t transfer: 0.0645\t finalize: 0.0066\nbatch bounding time:  0.6222772598266602\nCurrent worst splitting domains lb-rhs (depth):\n-0.35947 (10), -0.35085 (10), -0.34871 (10), -0.33884 (10), -0.31798 (10), -0.31492 (10), -0.31355 (10), -0.30911 (10), -0.29946 (10), -0.29199 (10), -0.28763 (10), -0.27889 (10), -0.26958 (10), -0.25749 (10), -0.25432 (10), -0.24742 (10), -0.24447 (10), -0.24395 (10), -0.23997 (10), -0.23954 (10), \nlength of domains: 116\nTotal time: 0.7079\t pickout: 0.0034\t decision: 0.0700\t get_bound: 0.6223\t add_domain: 0.0121\nAccumulated time:\t pickout: 0.0102\t decision: 0.2314\t get_bound: 3.2605\t add_domain: 0.0405\nCurrent (lb-rhs): -0.3594703674316406\n185 domains visited\nCumulative time: 3.5507476329803467\n\nbatch:  torch.Size([116, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([116, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] \n(232, 3, 32, 32) torch.Size([232, 1, 10]) torch.Size([232, 1])\npruning_in_iteration open status: False\nratio of positive domain = 39 / 232 = 0.1681034482758621\npruning-in-iteration extra time: 0.00014638900756835938\nTensors transferred: pre=21.8606M lA=10.9303M alpha=1.2041M beta=0.0024M\nThis batch time : update_bounds func: 0.6280\t prepare: 0.0108\t bound: 0.5918\t transfer: 0.0230\t finalize: 0.0022\nAccumulated time: update_bounds func: 3.9009\t prepare: 0.0397\t bound: 3.7640\t transfer: 0.0875\t finalize: 0.0089\nbatch bounding time:  0.6280949115753174\nCurrent worst splitting domains lb-rhs (depth):\n-0.33915 (11), -0.33049 (11), -0.32864 (11), -0.32759 (11), -0.32070 (11), -0.31964 (11), -0.31777 (11), -0.30983 (11), -0.29697 (11), -0.29359 (11), -0.29238 (11), -0.28871 (11), -0.28763 (11), -0.28691 (11), -0.28414 (11), -0.28057 (11), -0.27859 (11), -0.27084 (11), -0.27044 (11), -0.26671 (11), \nlength of domains: 193\nTotal time: 0.7121\t pickout: 0.0037\t decision: 0.0623\t get_bound: 0.6281\t add_domain: 0.0179\nAccumulated time:\t pickout: 0.0138\t decision: 0.2937\t get_bound: 3.8887\t add_domain: 0.0584\nCurrent (lb-rhs): -0.3391540050506592\n224 domains visited\nCumulative time: 4.263686656951904\n\nbatch:  torch.Size([193, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([193, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] \n(386, 3, 32, 32) torch.Size([386, 1, 10]) torch.Size([386, 1])\npruning_in_iteration open status: True\nratio of positive domain = 190 / 386 = 0.49222797927461137\npruning-in-iteration extra time: 0.027266979217529297\nTensors transferred: pre=36.3716M lA=9.2342M alpha=2.0033M beta=0.0044M\nThis batch time : update_bounds func: 0.7885\t prepare: 0.0125\t bound: 0.7274\t transfer: 0.0447\t finalize: 0.0036\nAccumulated time: update_bounds func: 4.6894\t prepare: 0.0522\t bound: 4.4915\t transfer: 0.1321\t finalize: 0.0124\nbatch bounding time:  0.7885620594024658\nCurrent worst splitting domains lb-rhs (depth):\n-0.31952 (12), -0.31131 (12), -0.31026 (12), -0.30937 (12), -0.30279 (12), -0.30224 (12), -0.29969 (12), -0.29263 (12), -0.27878 (12), -0.27646 (12), -0.27428 (12), -0.27156 (12), -0.27053 (12), -0.27051 (12), -0.26702 (12), -0.26425 (12), -0.25997 (12), -0.25235 (12), -0.25228 (12), -0.24906 (12), \nlength of domains: 196\nTotal time: 0.9272\t pickout: 0.0057\t decision: 0.1122\t get_bound: 0.7886\t add_domain: 0.0207\nAccumulated time:\t pickout: 0.0195\t decision: 0.4059\t get_bound: 4.6773\t add_domain: 0.0791\nCurrent (lb-rhs): -0.3195207118988037\n414 domains visited\nCumulative time: 5.191999435424805\n\nbatch:  torch.Size([196, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([196, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] \n(392, 3, 32, 32) torch.Size([392, 1, 10]) torch.Size([392, 1])\npruning_in_iteration open status: True\nratio of positive domain = 222 / 392 = 0.5663265306122449\npruning-in-iteration extra time: 0.028594017028808594\nTensors transferred: pre=36.9369M lA=8.0093M alpha=2.0344M beta=0.0049M\nThis batch time : update_bounds func: 0.7262\t prepare: 0.0136\t bound: 0.6777\t transfer: 0.0301\t finalize: 0.0044\nAccumulated time: update_bounds func: 5.4155\t prepare: 0.0658\t bound: 5.1692\t transfer: 0.1622\t finalize: 0.0169\nbatch bounding time:  0.7262556552886963\nCurrent worst splitting domains lb-rhs (depth):\n-0.30481 (13), -0.29672 (13), -0.29490 (13), -0.29435 (13), -0.28757 (13), -0.28673 (13), -0.28491 (13), -0.27730 (13), -0.26466 (13), -0.26204 (13), -0.26015 (13), -0.25702 (13), -0.25625 (13), -0.25574 (13), -0.25263 (13), -0.24975 (13), -0.24498 (13), -0.23744 (13), -0.23679 (13), -0.23403 (13), \nlength of domains: 170\nTotal time: 0.8546\t pickout: 0.0057\t decision: 0.1034\t get_bound: 0.7263\t add_domain: 0.0192\nAccumulated time:\t pickout: 0.0252\t decision: 0.5092\t get_bound: 5.4036\t add_domain: 0.0983\nCurrent (lb-rhs): -0.3048114776611328\n636 domains visited\nCumulative time: 6.048779487609863\n\nbatch:  torch.Size([170, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([170, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 241] [3, 1] \n(340, 3, 32, 32) torch.Size([340, 1, 10]) torch.Size([340, 1])\npruning_in_iteration open status: True\nratio of positive domain = 157 / 340 = 0.46176470588235297\npruning-in-iteration extra time: 0.0280303955078125\nTensors transferred: pre=32.0371M lA=8.6218M alpha=1.7646M beta=0.0049M\nThis batch time : update_bounds func: 0.7626\t prepare: 0.0142\t bound: 0.7156\t transfer: 0.0293\t finalize: 0.0033\nAccumulated time: update_bounds func: 6.1781\t prepare: 0.0800\t bound: 5.8848\t transfer: 0.1915\t finalize: 0.0202\nbatch bounding time:  0.7626729011535645\nCurrent worst splitting domains lb-rhs (depth):\n-0.29670 (14), -0.28881 (14), -0.28705 (14), -0.28641 (14), -0.27990 (14), -0.27885 (14), -0.27709 (14), -0.26964 (14), -0.25611 (14), -0.25369 (14), -0.25188 (14), -0.24857 (14), -0.24808 (14), -0.24733 (14), -0.24442 (14), -0.24156 (14), -0.23736 (14), -0.22968 (14), -0.22925 (14), -0.22633 (14), \nlength of domains: 183\nTotal time: 0.8804\t pickout: 0.0052\t decision: 0.0907\t get_bound: 0.7627\t add_domain: 0.0218\nAccumulated time:\t pickout: 0.0304\t decision: 0.5999\t get_bound: 6.1664\t add_domain: 0.1201\nCurrent (lb-rhs): -0.29670262336730957\n793 domains visited\nCumulative time: 6.930448770523071\n\nbatch:  torch.Size([183, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([183, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 241] [3, 241] [3, 241] [3, 241] [3, 241] [3, 241] [3, 1] [3, 241] [3, 1] [3, 1] \n(366, 3, 32, 32) torch.Size([366, 1, 10]) torch.Size([366, 1])\npruning_in_iteration open status: True\nratio of positive domain = 183 / 366 = 0.5\npruning-in-iteration extra time: 0.029592275619506836\nTensors transferred: pre=34.4870M lA=8.6689M alpha=1.8995M beta=0.0056M\nThis batch time : update_bounds func: 0.7898\t prepare: 0.0153\t bound: 0.7314\t transfer: 0.0389\t finalize: 0.0038\nAccumulated time: update_bounds func: 6.9679\t prepare: 0.0953\t bound: 6.6162\t transfer: 0.2304\t finalize: 0.0239\nbatch bounding time:  0.7899093627929688\nCurrent worst splitting domains lb-rhs (depth):\n-0.28925 (15), -0.28126 (15), -0.27959 (15), -0.27886 (15), -0.27232 (15), -0.27128 (15), -0.26941 (15), -0.26195 (15), -0.24884 (15), -0.24620 (15), -0.24452 (15), -0.24128 (15), -0.24048 (15), -0.23979 (15), -0.23700 (15), -0.23390 (15), -0.22978 (15), -0.22231 (15), -0.22158 (15), -0.21881 (15), \nlength of domains: 183\nTotal time: 0.9146\t pickout: 0.0056\t decision: 0.0976\t get_bound: 0.7900\t add_domain: 0.0215\nAccumulated time:\t pickout: 0.0359\t decision: 0.6976\t get_bound: 6.9563\t add_domain: 0.1415\nCurrent (lb-rhs): -0.28924560546875\n976 domains visited\nCumulative time: 7.8463709354400635\n\nbatch:  torch.Size([183, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([183, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [2, 1206] [3, 126] [3, 126] [3, 241] [3, 241] [3, 1] [3, 1] [3, 126] [3, 126] \n(366, 3, 32, 32) torch.Size([366, 1, 10]) torch.Size([366, 1])\npruning_in_iteration open status: True\nratio of positive domain = 100 / 366 = 0.27322404371584696\npruning-in-iteration extra time: 0.02663898468017578\nTensors transferred: pre=34.4870M lA=12.5322M alpha=1.8995M beta=0.0063M\nThis batch time : update_bounds func: 0.8716\t prepare: 0.0166\t bound: 0.8206\t transfer: 0.0303\t finalize: 0.0037\nAccumulated time: update_bounds func: 7.8396\t prepare: 0.1119\t bound: 7.4368\t transfer: 0.2607\t finalize: 0.0276\nbatch bounding time:  0.8717551231384277\nCurrent worst splitting domains lb-rhs (depth):\n-0.28220 (16), -0.28135 (16), -0.27413 (16), -0.27373 (16), -0.27290 (16), -0.27183 (16), -0.27070 (16), -0.26854 (16), -0.26549 (16), -0.26459 (16), -0.26268 (16), -0.26225 (16), -0.26180 (16), -0.26056 (16), -0.25507 (16), -0.25287 (16), -0.24245 (16), -0.24022 (16), -0.23832 (16), -0.23811 (16), \nlength of domains: 266\nTotal time: 1.0179\t pickout: 0.0053\t decision: 0.1136\t get_bound: 0.8718\t add_domain: 0.0271\nAccumulated time:\t pickout: 0.0413\t decision: 0.8112\t get_bound: 7.8281\t add_domain: 0.1686\nCurrent (lb-rhs): -0.2821986675262451\n1076 domains visited\nCumulative time: 8.865478754043579\n\nbatch:  torch.Size([266, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([266, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [2, 3243] [2, 7594] [2, 7054] [3, 1] [2, 2190] [2, 6307] [3, 56] [2, 2190] [2, 1206] \n(532, 3, 32, 32) torch.Size([532, 1, 10]) torch.Size([532, 1])\npruning_in_iteration open status: False\nratio of positive domain = 85 / 532 = 0.15977443609022557\npruning-in-iteration extra time: 0.000164031982421875\nTensors transferred: pre=50.1287M lA=25.0643M alpha=2.7610M beta=0.0096M\nThis batch time : update_bounds func: 1.2548\t prepare: 0.0194\t bound: 1.1628\t transfer: 0.0671\t finalize: 0.0051\nAccumulated time: update_bounds func: 9.0944\t prepare: 0.1313\t bound: 8.5996\t transfer: 0.3279\t finalize: 0.0327\nbatch bounding time:  1.2549726963043213\nCurrent worst splitting domains lb-rhs (depth):\n-0.27554 (17), -0.27521 (17), -0.27478 (17), -0.27388 (17), -0.26763 (17), -0.26734 (17), -0.26732 (17), -0.26678 (17), -0.26653 (17), -0.26589 (17), -0.26547 (17), -0.26499 (17), -0.26431 (17), -0.26397 (17), -0.26379 (17), -0.25926 (17), -0.25827 (17), -0.25810 (17), -0.25799 (17), -0.25676 (17), \nlength of domains: 447\nTotal time: 1.4594\t pickout: 0.0076\t decision: 0.1495\t get_bound: 1.2550\t add_domain: 0.0473\nAccumulated time:\t pickout: 0.0489\t decision: 0.9607\t get_bound: 9.0832\t add_domain: 0.2159\nCurrent (lb-rhs): -0.27553629875183105\n1161 domains visited\nCumulative time: 10.326499700546265\n\nbatch:  torch.Size([447, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([447, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 6307] [2, 2190] [2, 1206] [3, 126] [3, 56] [3, 230] [2, 6307] [3, 230] [2, 6307] \n(894, 3, 32, 32) torch.Size([894, 1, 10]) torch.Size([894, 1])\npruning_in_iteration open status: True\nratio of positive domain = 194 / 894 = 0.21700223713646527\npruning-in-iteration extra time: 0.019492387771606445\nTensors transferred: pre=84.2388M lA=32.9794M alpha=4.6398M beta=0.0179M\nThis batch time : update_bounds func: 1.9648\t prepare: 0.0292\t bound: 1.8218\t transfer: 0.1046\t finalize: 0.0085\nAccumulated time: update_bounds func: 11.0592\t prepare: 0.1605\t bound: 10.4214\t transfer: 0.4325\t finalize: 0.0413\nbatch bounding time:  1.965038537979126\nCurrent worst splitting domains lb-rhs (depth):\n-0.26964 (18), -0.26939 (18), -0.26930 (18), -0.26886 (18), -0.26860 (18), -0.26831 (18), -0.26787 (18), -0.26186 (18), -0.26156 (18), -0.26148 (18), -0.26128 (18), -0.26094 (18), -0.26049 (18), -0.26047 (18), -0.26028 (18), -0.26027 (18), -0.26019 (18), -0.25964 (18), -0.25964 (18), -0.25961 (18), \nlength of domains: 700\nTotal time: 2.6266\t pickout: 0.0123\t decision: 0.5583\t get_bound: 1.9651\t add_domain: 0.0909\nAccumulated time:\t pickout: 0.0612\t decision: 1.5191\t get_bound: 11.0483\t add_domain: 0.3068\nCurrent (lb-rhs): -0.2696382999420166\n1355 domains visited\nCumulative time: 12.95572566986084\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6307] [3, 56] [2, 6307] [3, 56] [2, 1206] [3, 56] [2, 1206] [3, 56] [3, 126] [3, 56] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 325 / 1024 = 0.3173828125\npruning-in-iteration extra time: 0.02815532684326172\nTensors transferred: pre=96.4883M lA=32.9323M alpha=5.3145M beta=0.0215M\nThis batch time : update_bounds func: 2.1053\t prepare: 0.0351\t bound: 1.9367\t transfer: 0.1226\t finalize: 0.0101\nAccumulated time: update_bounds func: 13.1645\t prepare: 0.1956\t bound: 12.3582\t transfer: 0.5551\t finalize: 0.0514\nbatch bounding time:  2.105517625808716\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26504 (19), -0.26475 (19), -0.26431 (19), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25834 (18), -0.25742 (19), -0.25731 (18), -0.25717 (19), -0.25709 (19), -0.25661 (19), -0.25660 (19), -0.25602 (19), -0.25579 (19), -0.25579 (19), \nlength of domains: 887\nTotal time: 4.1316\t pickout: 0.0143\t decision: 1.9218\t get_bound: 2.1056\t add_domain: 0.0899\nAccumulated time:\t pickout: 0.0754\t decision: 3.4409\t get_bound: 13.1539\t add_domain: 0.3967\nCurrent (lb-rhs): -0.2693939208984375\n1680 domains visited\nCumulative time: 17.090436697006226\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1963] [2, 5139] [3, 230] [3, 230] [3, 56] [3, 126] [2, 2190] [3, 56] [2, 6307] [3, 230] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 363 / 1024 = 0.3544921875\npruning-in-iteration extra time: 0.02670001983642578\nTensors transferred: pre=96.4883M lA=31.1420M alpha=5.3145M beta=0.0234M\nThis batch time : update_bounds func: 2.0205\t prepare: 0.0344\t bound: 1.8678\t transfer: 0.1075\t finalize: 0.0100\nAccumulated time: update_bounds func: 15.1850\t prepare: 0.2300\t bound: 14.2260\t transfer: 0.6626\t finalize: 0.0613\nbatch bounding time:  2.0206892490386963\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25999 (20), -0.25984 (20), -0.25964 (18), -0.25964 (20), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), \nlength of domains: 1036\nTotal time: 3.1312\t pickout: 0.0140\t decision: 0.7640\t get_bound: 2.0208\t add_domain: 0.3324\nAccumulated time:\t pickout: 0.0895\t decision: 4.2049\t get_bound: 15.1746\t add_domain: 0.7291\nCurrent (lb-rhs): -0.2693939208984375\n2043 domains visited\nCumulative time: 20.22483491897583\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 4715] [2, 5518] [3, 230] [2, 5139] [2, 1204] [2, 2732] [3, 230] [3, 230] [2, 2190] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 368 / 1024 = 0.359375\npruning-in-iteration extra time: 0.028758525848388672\nTensors transferred: pre=96.4883M lA=30.9535M alpha=5.3145M beta=0.0244M\nThis batch time : update_bounds func: 2.0024\t prepare: 0.0341\t bound: 1.8326\t transfer: 0.1236\t finalize: 0.0113\nAccumulated time: update_bounds func: 17.1874\t prepare: 0.2640\t bound: 16.0586\t transfer: 0.7862\t finalize: 0.0726\nbatch bounding time:  2.0026121139526367\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25546 (21), -0.25525 (21), -0.25506 (18), -0.25495 (19), -0.25486 (21), -0.25362 (19), -0.25348 (18), \nlength of domains: 1180\nTotal time: 2.9010\t pickout: 0.0142\t decision: 0.7930\t get_bound: 2.0027\t add_domain: 0.0911\nAccumulated time:\t pickout: 0.1037\t decision: 4.9979\t get_bound: 17.1773\t add_domain: 0.8202\nCurrent (lb-rhs): -0.2693939208984375\n2411 domains visited\nCumulative time: 23.129989624023438\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 227] [2, 5518] [3, 126] [2, 1204] [2, 7475] [3, 20] [2, 6379] [2, 5965] [3, 227] [3, 84] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 287 / 1024 = 0.2802734375\npruning-in-iteration extra time: 0.027468204498291016\nTensors transferred: pre=96.4883M lA=34.7697M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 2.1451\t prepare: 0.0366\t bound: 1.9669\t transfer: 0.1302\t finalize: 0.0106\nAccumulated time: update_bounds func: 19.3325\t prepare: 0.3006\t bound: 18.0255\t transfer: 0.9164\t finalize: 0.0832\nbatch bounding time:  2.145284414291382\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 1405\nTotal time: 3.0690\t pickout: 0.0148\t decision: 0.7826\t get_bound: 2.1454\t add_domain: 0.1262\nAccumulated time:\t pickout: 0.1185\t decision: 5.7806\t get_bound: 19.3227\t add_domain: 0.9464\nCurrent (lb-rhs): -0.2693939208984375\n2698 domains visited\nCumulative time: 26.202237606048584\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 227] [3, 20] [2, 5518] [2, 1963] [2, 4715] [2, 5518] [2, 1206] [3, 84] [3, 20] [3, 227] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 261 / 1024 = 0.2548828125\npruning-in-iteration extra time: 0.02724599838256836\nTensors transferred: pre=96.4883M lA=35.9475M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 2.1823\t prepare: 0.0359\t bound: 2.0205\t transfer: 0.1148\t finalize: 0.0102\nAccumulated time: update_bounds func: 21.5148\t prepare: 0.3365\t bound: 20.0460\t transfer: 1.0312\t finalize: 0.0934\nbatch bounding time:  2.1825153827667236\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 1656\nTotal time: 3.0609\t pickout: 0.0142\t decision: 0.7601\t get_bound: 2.1826\t add_domain: 0.1040\nAccumulated time:\t pickout: 0.1327\t decision: 6.5406\t get_bound: 21.5053\t add_domain: 1.0503\nCurrent (lb-rhs): -0.2693939208984375\n2959 domains visited\nCumulative time: 29.267180681228638\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 5518] [2, 4883] [2, 2454] [2, 4883] [3, 84] [2, 6307] [3, 20] [2, 7140] [3, 20] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 243 / 1024 = 0.2373046875\npruning-in-iteration extra time: 0.022957801818847656\nTensors transferred: pre=96.4883M lA=36.7956M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 2.2124\t prepare: 0.0340\t bound: 2.0321\t transfer: 0.1351\t finalize: 0.0103\nAccumulated time: update_bounds func: 23.7272\t prepare: 0.3705\t bound: 22.0781\t transfer: 1.1663\t finalize: 0.1038\nbatch bounding time:  2.212686538696289\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 1925\nTotal time: 3.1411\t pickout: 0.0148\t decision: 0.7646\t get_bound: 2.2128\t add_domain: 0.1489\nAccumulated time:\t pickout: 0.1475\t decision: 7.3053\t get_bound: 23.7180\t add_domain: 1.1993\nCurrent (lb-rhs): -0.2693939208984375\n3202 domains visited\nCumulative time: 32.412402629852295\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 2190] [2, 7054] [2, 5518] [2, 3931] [2, 7064] [3, 84] [2, 6379] [2, 7140] [2, 7054] [2, 5518] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 268 / 1024 = 0.26171875\npruning-in-iteration extra time: 0.025891542434692383\nTensors transferred: pre=96.4883M lA=35.6177M alpha=5.3145M beta=0.0273M\nThis batch time : update_bounds func: 2.1655\t prepare: 0.0337\t bound: 1.9866\t transfer: 0.1342\t finalize: 0.0101\nAccumulated time: update_bounds func: 25.8927\t prepare: 0.4042\t bound: 24.0647\t transfer: 1.3005\t finalize: 0.1139\nbatch bounding time:  2.165839672088623\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2169\nTotal time: 3.6113\t pickout: 0.0145\t decision: 0.7624\t get_bound: 2.1659\t add_domain: 0.6685\nAccumulated time:\t pickout: 0.1620\t decision: 8.0676\t get_bound: 25.8839\t add_domain: 1.8678\nCurrent (lb-rhs): -0.2693939208984375\n3470 domains visited\nCumulative time: 36.027339935302734\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 7054] [2, 7064] [2, 2454] [3, 227] [2, 2454] [3, 20] [2, 7054] [2, 2732] [2, 7064] [2, 2732] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 252 / 1024 = 0.24609375\npruning-in-iteration extra time: 0.02340841293334961\nTensors transferred: pre=96.4883M lA=36.3716M alpha=5.3145M beta=0.0283M\nThis batch time : update_bounds func: 2.2681\t prepare: 0.0434\t bound: 2.0145\t transfer: 0.1606\t finalize: 0.0100\nAccumulated time: update_bounds func: 28.1608\t prepare: 0.4475\t bound: 26.0793\t transfer: 1.4611\t finalize: 0.1239\nbatch bounding time:  2.26828932762146\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2429\nTotal time: 3.1703\t pickout: 0.0151\t decision: 0.7798\t get_bound: 2.2684\t add_domain: 0.1071\nAccumulated time:\t pickout: 0.1771\t decision: 8.8474\t get_bound: 28.1523\t add_domain: 1.9749\nCurrent (lb-rhs): -0.2693939208984375\n3722 domains visited\nCumulative time: 39.20165205001831\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 7054] [2, 2454] [2, 7064] [2, 6379] [3, 20] [2, 2454] [2, 2454] [2, 7054] [2, 4883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 297 / 1024 = 0.2900390625\npruning-in-iteration extra time: 0.02639913558959961\nTensors transferred: pre=96.4883M lA=34.2515M alpha=5.3145M beta=0.0293M\nThis batch time : update_bounds func: 2.1062\t prepare: 0.0359\t bound: 1.9284\t transfer: 0.1307\t finalize: 0.0102\nAccumulated time: update_bounds func: 30.2670\t prepare: 0.4835\t bound: 28.0077\t transfer: 1.5918\t finalize: 0.1341\nbatch bounding time:  2.1064445972442627\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2644\nTotal time: 2.9862\t pickout: 0.0152\t decision: 0.7655\t get_bound: 2.1065\t add_domain: 0.0989\nAccumulated time:\t pickout: 0.1922\t decision: 9.6129\t get_bound: 30.2588\t add_domain: 2.0738\nCurrent (lb-rhs): -0.2693939208984375\n4019 domains visited\nCumulative time: 42.192294120788574\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 500] [2, 2454] [2, 1961] [2, 1961] [2, 5654] [3, 84] [2, 1961] [2, 1961] [2, 2454] [2, 2454] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 316 / 1024 = 0.30859375\npruning-in-iteration extra time: 0.030606985092163086\nTensors transferred: pre=96.4883M lA=33.3563M alpha=5.3145M beta=0.0312M\nThis batch time : update_bounds func: 2.0852\t prepare: 0.0399\t bound: 1.9132\t transfer: 0.1207\t finalize: 0.0106\nAccumulated time: update_bounds func: 32.3522\t prepare: 0.5233\t bound: 29.9209\t transfer: 1.7124\t finalize: 0.1448\nbatch bounding time:  2.0854570865631104\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2840\nTotal time: 2.9648\t pickout: 0.0148\t decision: 0.7626\t get_bound: 2.0855\t add_domain: 0.1018\nAccumulated time:\t pickout: 0.2071\t decision: 10.3755\t get_bound: 32.3444\t add_domain: 2.1757\nCurrent (lb-rhs): -0.2693939208984375\n4335 domains visited\nCumulative time: 45.16087532043457\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 5654] [2, 6307] [3, 227] [2, 5518] [2, 2190] [3, 84] [2, 2190] [2, 5654] [2, 5139] [2, 2190] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 338 / 1024 = 0.330078125\npruning-in-iteration extra time: 0.0272066593170166\nTensors transferred: pre=96.4883M lA=32.3198M alpha=5.3145M beta=0.0332M\nThis batch time : update_bounds func: 2.0618\t prepare: 0.0363\t bound: 1.8979\t transfer: 0.1151\t finalize: 0.0117\nAccumulated time: update_bounds func: 34.4140\t prepare: 0.5596\t bound: 31.8188\t transfer: 1.8275\t finalize: 0.1564\nbatch bounding time:  2.062039613723755\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3014\nTotal time: 2.9357\t pickout: 0.0148\t decision: 0.7637\t get_bound: 2.0621\t add_domain: 0.0951\nAccumulated time:\t pickout: 0.2219\t decision: 11.1392\t get_bound: 34.4065\t add_domain: 2.2707\nCurrent (lb-rhs): -0.2693939208984375\n4673 domains visited\nCumulative time: 48.10027241706848\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [3, 84] [2, 4883] [2, 5139] [3, 84] [2, 5139] [2, 2731] [3, 104] [2, 2454] [2, 5139] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 324 / 1024 = 0.31640625\npruning-in-iteration extra time: 0.028159618377685547\nTensors transferred: pre=96.4883M lA=32.9794M alpha=5.3145M beta=0.0352M\nThis batch time : update_bounds func: 2.0222\t prepare: 0.0358\t bound: 1.8681\t transfer: 0.1065\t finalize: 0.0111\nAccumulated time: update_bounds func: 36.4362\t prepare: 0.5954\t bound: 33.6869\t transfer: 1.9340\t finalize: 0.1675\nbatch bounding time:  2.0224685668945312\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3202\nTotal time: 2.9006\t pickout: 0.0142\t decision: 0.7624\t get_bound: 2.0225\t add_domain: 0.1015\nAccumulated time:\t pickout: 0.2360\t decision: 11.9016\t get_bound: 36.4290\t add_domain: 2.3723\nCurrent (lb-rhs): -0.2693939208984375\n4997 domains visited\nCumulative time: 51.004411458969116\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 7064] [2, 7054] [2, 1242] [2, 7054] [2, 6330] [2, 6330] [2, 7054] [2, 7140] [3, 84] [2, 7054] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 298 / 1024 = 0.291015625\npruning-in-iteration extra time: 0.03080010414123535\nTensors transferred: pre=96.4883M lA=34.2043M alpha=5.3145M beta=0.0361M\nThis batch time : update_bounds func: 2.1155\t prepare: 0.0354\t bound: 1.9352\t transfer: 0.1329\t finalize: 0.0111\nAccumulated time: update_bounds func: 38.5518\t prepare: 0.6308\t bound: 35.6221\t transfer: 2.0669\t finalize: 0.1786\nbatch bounding time:  2.1157619953155518\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3416\nTotal time: 2.9971\t pickout: 0.0149\t decision: 0.7625\t get_bound: 2.1158\t add_domain: 0.1039\nAccumulated time:\t pickout: 0.2509\t decision: 12.6641\t get_bound: 38.5448\t add_domain: 2.4762\nCurrent (lb-rhs): -0.2693939208984375\n5295 domains visited\nCumulative time: 54.00582218170166\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6330] [2, 5139] [2, 2732] [2, 7054] [2, 6330] [2, 6330] [2, 1961] [3, 104] [2, 7054] [3, 84] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 251 / 1024 = 0.2451171875\npruning-in-iteration extra time: 0.030178546905517578\nTensors transferred: pre=96.4883M lA=36.4187M alpha=5.3145M beta=0.0371M\nThis batch time : update_bounds func: 2.1948\t prepare: 0.0352\t bound: 2.0228\t transfer: 0.1248\t finalize: 0.0110\nAccumulated time: update_bounds func: 40.7466\t prepare: 0.6661\t bound: 37.6449\t transfer: 2.1917\t finalize: 0.1897\nbatch bounding time:  2.1950528621673584\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3677\nTotal time: 3.0908\t pickout: 0.0146\t decision: 0.7644\t get_bound: 2.1951\t add_domain: 0.1166\nAccumulated time:\t pickout: 0.2656\t decision: 13.4285\t get_bound: 40.7400\t add_domain: 2.5928\nCurrent (lb-rhs): -0.2693939208984375\n5546 domains visited\nCumulative time: 57.10070872306824\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 7064] [2, 4715] [3, 227] [2, 3889] [2, 6330] [3, 227] [2, 3870] [2, 7534] [2, 7064] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 183 / 1024 = 0.1787109375\npruning-in-iteration extra time: 0.00016617774963378906\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0381M\nThis batch time : update_bounds func: 2.3810\t prepare: 0.0361\t bound: 2.1278\t transfer: 0.2025\t finalize: 0.0137\nAccumulated time: update_bounds func: 43.1277\t prepare: 0.7022\t bound: 39.7727\t transfer: 2.3943\t finalize: 0.2034\nbatch bounding time:  2.3816075325012207\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 4006\nTotal time: 3.3067\t pickout: 0.0146\t decision: 0.7684\t get_bound: 2.3817\t add_domain: 0.1421\nAccumulated time:\t pickout: 0.2802\t decision: 14.1969\t get_bound: 43.1217\t add_domain: 2.7349\nCurrent (lb-rhs): -0.2693939208984375\n5729 domains visited\nCumulative time: 60.412537813186646\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 7534] [3, 84] [2, 7064] [3, 84] [2, 4715] [3, 73] [2, 1250] [2, 4715] [2, 4715] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 211 / 1024 = 0.2060546875\npruning-in-iteration extra time: 0.021020174026489258\nTensors transferred: pre=96.4883M lA=38.3032M alpha=5.3145M beta=0.0400M\nThis batch time : update_bounds func: 2.3833\t prepare: 0.0530\t bound: 2.1337\t transfer: 0.1809\t finalize: 0.0146\nAccumulated time: update_bounds func: 45.5109\t prepare: 0.7552\t bound: 41.9065\t transfer: 2.5751\t finalize: 0.2179\nbatch bounding time:  2.3835761547088623\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 4307\nTotal time: 4.4289\t pickout: 0.0151\t decision: 0.8112\t get_bound: 2.3837\t add_domain: 1.2188\nAccumulated time:\t pickout: 0.2953\t decision: 15.0081\t get_bound: 45.5054\t add_domain: 3.9537\nCurrent (lb-rhs): -0.2693939208984375\n5940 domains visited\nCumulative time: 64.84535145759583\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 104] [2, 4715] [2, 4715] [2, 4715] [2, 4715] [3, 219] [3, 219] [3, 104] [2, 7054] [2, 6330] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 255 / 1024 = 0.2490234375\npruning-in-iteration extra time: 0.024969816207885742\nTensors transferred: pre=96.4883M lA=36.2302M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.1740\t prepare: 0.0343\t bound: 2.0048\t transfer: 0.1233\t finalize: 0.0108\nAccumulated time: update_bounds func: 47.6850\t prepare: 0.7895\t bound: 43.9113\t transfer: 2.6984\t finalize: 0.2287\nbatch bounding time:  2.17423152923584\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 4564\nTotal time: 3.0693\t pickout: 0.0156\t decision: 0.7661\t get_bound: 2.1743\t add_domain: 0.1132\nAccumulated time:\t pickout: 0.3109\t decision: 15.7742\t get_bound: 47.6797\t add_domain: 4.0669\nCurrent (lb-rhs): -0.2693939208984375\n6195 domains visited\nCumulative time: 67.91923379898071\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [3, 84] [2, 4715] [2, 3931] [2, 3931] [2, 5518] [3, 84] [3, 104] [2, 5518] [2, 3870] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 221 / 1024 = 0.2158203125\npruning-in-iteration extra time: 0.021599769592285156\nTensors transferred: pre=96.4883M lA=37.8321M alpha=5.3145M beta=0.0381M\nThis batch time : update_bounds func: 2.3055\t prepare: 0.0368\t bound: 2.0590\t transfer: 0.1487\t finalize: 0.0109\nAccumulated time: update_bounds func: 49.9904\t prepare: 0.8262\t bound: 45.9702\t transfer: 2.8471\t finalize: 0.2396\nbatch bounding time:  2.3057000637054443\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 4855\nTotal time: 3.2037\t pickout: 0.0147\t decision: 0.7645\t get_bound: 2.3058\t add_domain: 0.1187\nAccumulated time:\t pickout: 0.3257\t decision: 16.5387\t get_bound: 49.9855\t add_domain: 4.1856\nCurrent (lb-rhs): -0.2693939208984375\n6416 domains visited\nCumulative time: 71.12747478485107\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [3, 84] [2, 5518] [3, 84] [2, 5518] [3, 84] [2, 5518] [2, 3889] [2, 3889] [2, 5518] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 221 / 1024 = 0.2158203125\npruning-in-iteration extra time: 0.01988840103149414\nTensors transferred: pre=96.4883M lA=37.8321M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.2522\t prepare: 0.0358\t bound: 2.0755\t transfer: 0.1289\t finalize: 0.0111\nAccumulated time: update_bounds func: 52.2426\t prepare: 0.8620\t bound: 48.0458\t transfer: 2.9760\t finalize: 0.2506\nbatch bounding time:  2.2524149417877197\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 5146\nTotal time: 3.1507\t pickout: 0.0150\t decision: 0.7626\t get_bound: 2.2525\t add_domain: 0.1206\nAccumulated time:\t pickout: 0.3407\t decision: 17.3013\t get_bound: 52.2380\t add_domain: 4.3062\nCurrent (lb-rhs): -0.2693939208984375\n6637 domains visited\nCumulative time: 74.28182888031006\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 5518] [2, 5518] [2, 6330] [2, 5518] [2, 5518] [2, 5518] [2, 5518] [2, 5518] [2, 4883] [2, 7064] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 150 / 1024 = 0.146484375\npruning-in-iteration extra time: 0.0001723766326904297\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.3051\t prepare: 0.0375\t bound: 2.1115\t transfer: 0.1447\t finalize: 0.0105\nAccumulated time: update_bounds func: 54.5478\t prepare: 0.8995\t bound: 50.1573\t transfer: 3.1206\t finalize: 0.2612\nbatch bounding time:  2.3054802417755127\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 5508\nTotal time: 3.2723\t pickout: 0.0144\t decision: 0.7679\t get_bound: 2.3056\t add_domain: 0.1844\nAccumulated time:\t pickout: 0.3551\t decision: 18.0692\t get_bound: 54.5435\t add_domain: 4.4907\nCurrent (lb-rhs): -0.2693939208984375\n6787 domains visited\nCumulative time: 77.55783009529114\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 3889] [2, 7064] [2, 1963] [2, 1963] [2, 4883] [2, 1963] [2, 6330] [3, 73] [2, 6330] [2, 1963] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 167 / 1024 = 0.1630859375\npruning-in-iteration extra time: 0.000171661376953125\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.2988\t prepare: 0.0347\t bound: 2.1152\t transfer: 0.1376\t finalize: 0.0104\nAccumulated time: update_bounds func: 56.8465\t prepare: 0.9342\t bound: 52.2725\t transfer: 3.2582\t finalize: 0.2716\nbatch bounding time:  2.299140453338623\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 5853\nTotal time: 3.2073\t pickout: 0.0145\t decision: 0.7625\t get_bound: 2.2992\t add_domain: 0.1312\nAccumulated time:\t pickout: 0.3696\t decision: 18.8317\t get_bound: 56.8428\t add_domain: 4.6218\nCurrent (lb-rhs): -0.2693939208984375\n6954 domains visited\nCumulative time: 80.76916408538818\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 3417] [2, 3417] [2, 7786] [2, 1242] [2, 4883] [2, 2732] [2, 7786] [2, 4883] [2, 7786] [2, 2732] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 88 / 1024 = 0.0859375\npruning-in-iteration extra time: 0.00015592575073242188\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0410M\nThis batch time : update_bounds func: 2.3438\t prepare: 0.0346\t bound: 2.1313\t transfer: 0.1661\t finalize: 0.0109\nAccumulated time: update_bounds func: 59.1903\t prepare: 0.9688\t bound: 54.4038\t transfer: 3.4243\t finalize: 0.2825\nbatch bounding time:  2.3441271781921387\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 6277\nTotal time: 3.2838\t pickout: 0.0163\t decision: 0.7791\t get_bound: 2.3442\t add_domain: 0.1441\nAccumulated time:\t pickout: 0.3859\t decision: 19.6108\t get_bound: 59.1870\t add_domain: 4.7659\nCurrent (lb-rhs): -0.2693939208984375\n7042 domains visited\nCumulative time: 84.05664587020874\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1242] [2, 4883] [1, 2615] [2, 2732] [2, 1242] [2, 7054] [2, 1242] [2, 4883] [2, 2732] [2, 7780] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 97 / 1024 = 0.0947265625\npruning-in-iteration extra time: 0.00015306472778320312\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0420M\nThis batch time : update_bounds func: 2.3296\t prepare: 0.0346\t bound: 2.1204\t transfer: 0.1631\t finalize: 0.0107\nAccumulated time: update_bounds func: 61.5199\t prepare: 1.0034\t bound: 56.5242\t transfer: 3.5873\t finalize: 0.2932\nbatch bounding time:  2.32993221282959\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 6692\nTotal time: 3.3056\t pickout: 0.0148\t decision: 0.8159\t get_bound: 2.3300\t add_domain: 0.1449\nAccumulated time:\t pickout: 0.4007\t decision: 20.4267\t get_bound: 61.5170\t add_domain: 4.9108\nCurrent (lb-rhs): -0.2693939208984375\n7139 domains visited\nCumulative time: 87.36590313911438\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6033] [2, 6033] [1, 2615] [2, 1965] [2, 1965] [1, 2615] [2, 1965] [2, 6033] [2, 1965] [2, 1965] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 59 / 1024 = 0.0576171875\npruning-in-iteration extra time: 0.000179290771484375\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0430M\nThis batch time : update_bounds func: 2.3116\t prepare: 0.0354\t bound: 2.1174\t transfer: 0.1479\t finalize: 0.0102\nAccumulated time: update_bounds func: 63.8315\t prepare: 1.0387\t bound: 58.6416\t transfer: 3.7352\t finalize: 0.3034\nbatch bounding time:  2.3119895458221436\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 7145\nTotal time: 3.2392\t pickout: 0.0156\t decision: 0.7613\t get_bound: 2.3121\t add_domain: 0.1502\nAccumulated time:\t pickout: 0.4163\t decision: 21.1880\t get_bound: 63.8290\t add_domain: 5.0610\nCurrent (lb-rhs): -0.2693939208984375\n7198 domains visited\nCumulative time: 90.60881304740906\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 2732] [2, 6033] [2, 1965] [1, 2615] [2, 7786] [2, 1965] [2, 7786] [2, 7786] [2, 5965] [2, 2300] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 64 / 1024 = 0.0625\npruning-in-iteration extra time: 0.00015282630920410156\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0430M\nThis batch time : update_bounds func: 2.3400\t prepare: 0.0361\t bound: 2.1221\t transfer: 0.1706\t finalize: 0.0103\nAccumulated time: update_bounds func: 66.1715\t prepare: 1.0748\t bound: 60.7638\t transfer: 3.9058\t finalize: 0.3137\nbatch bounding time:  2.34043025970459\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 7593\nTotal time: 3.3366\t pickout: 0.0750\t decision: 0.7641\t get_bound: 2.3405\t add_domain: 0.1570\nAccumulated time:\t pickout: 0.4913\t decision: 21.9521\t get_bound: 66.1695\t add_domain: 5.2180\nCurrent (lb-rhs): -0.2693939208984375\n7262 domains visited\nCumulative time: 93.94913816452026\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 2300] [2, 6329] [2, 2300] [2, 1965] [1, 2615] [1, 2615] [2, 1204] [2, 1965] [1, 2615] [2, 2300] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 87 / 1024 = 0.0849609375\npruning-in-iteration extra time: 0.00019288063049316406\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0449M\nThis batch time : update_bounds func: 2.3301\t prepare: 0.0359\t bound: 2.1248\t transfer: 0.1572\t finalize: 0.0114\nAccumulated time: update_bounds func: 68.5016\t prepare: 1.1107\t bound: 62.8886\t transfer: 4.0630\t finalize: 0.3251\nbatch bounding time:  2.3305158615112305\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26400 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25495 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 8018\nTotal time: 3.2613\t pickout: 0.0145\t decision: 0.7626\t get_bound: 2.3306\t add_domain: 0.1535\nAccumulated time:\t pickout: 0.5058\t decision: 22.7148\t get_bound: 68.5001\t add_domain: 5.3715\nCurrent (lb-rhs): -0.2693939208984375\n7349 domains visited\nCumulative time: 97.21508502960205\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 183] [3, 183] [2, 1376] [1, 2615] [1, 2615] [2, 1965] [1, 2615] [2, 424] [2, 424] [2, 7780] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 283 / 1024 = 0.2763671875\npruning-in-iteration extra time: 0.027971267700195312\nTensors transferred: pre=96.4883M lA=34.9110M alpha=5.3145M beta=0.0459M\nThis batch time : update_bounds func: 2.1914\t prepare: 0.0379\t bound: 1.9812\t transfer: 0.1609\t finalize: 0.0106\nAccumulated time: update_bounds func: 70.6931\t prepare: 1.1486\t bound: 64.8697\t transfer: 4.2239\t finalize: 0.3357\nbatch bounding time:  2.1916627883911133\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_36_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "SAT", "took": "7.959681034088135", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpz5btjtvz.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:56:28 2024 on Cerberus\nInternal results will be saved to /tmp/tmpz5btjtvz.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-1.96812832, -6.49893856,  1.24639082,  1.69520974,  4.02628088,\n          2.75464153,  2.39966416,  3.83796740, -4.41950607, -2.72571588]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.01335669, -6.61650658,  1.13246620,  1.72124791,  3.89798760,\n           2.86626458,  2.36144567,  4.41441536, -4.41264629, -2.79052591],\n         [-2.01335669, -6.61650658,  1.13246620,  1.72124791,  3.89798760,\n           2.86626458,  2.36144567,  4.41441536, -4.41264629, -2.79052591]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.91134453, 10.51449394,  2.76552153,  2.17673969,  1.03172302,\n           1.53654194, -0.51642776,  8.31063366,  6.68851376]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.9483 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 5.4083778858184814\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_44_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.2937912940979", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpqkc9rkya.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:56:36 2024 on Cerberus\nInternal results will be saved to /tmp/tmpqkc9rkya.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 8.53005219,  2.65516281,  3.62637854, -1.70141554,  3.90650630,\n         -4.50923967, -6.23203850, -2.73792219,  3.00681901,  4.35398483]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 8.30019855,  3.03155708,  3.46851993, -1.78878641,  3.78387809,\n          -4.47537851, -6.29589128, -2.85683608,  2.87485886,  4.81936932],\n         [ 8.30019855,  3.03155708,  3.46851993, -1.78878641,  3.78387809,\n          -4.47537851, -6.29589128, -2.85683608,  2.87485886,  4.81936932]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.26864147,  4.83167839, 10.08898544,  4.51632023, 12.77557755,\n          14.59609032, 11.15703487,  5.42533970,  3.48082924]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5132 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 8.53005219,  2.65516281,  3.62637854, -1.70141554,  3.90650630,\n         -4.50923967, -6.23203850, -2.73792219,  3.00681901,  4.35398483]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [986]; unstable size 986; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 433 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 406 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [433]; unstable size 433; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 406 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [406]; unstable size 406; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [28]; unstable size 28; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 4.16252232,  3.01627779,  8.77518272,  3.12779689, 11.22480965,\n         13.38881111,  9.78200722,  3.72122145,  2.91803312]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.802578449249268\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_55_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.192093133926392", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpt530wp4x.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:56:44 2024 on Cerberus\nInternal results will be saved to /tmp/tmpt530wp4x.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 4.24215508, -0.69684935,  0.55492795, -0.75052458,  0.43542138,\n         -1.24407911, -1.12097180, -2.80693984,  7.00109816,  0.78772008]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 4.52412033, -0.46530545,  0.71960282, -0.91084248,  0.43153140,\n          -1.32478404, -1.19332063, -2.72541595,  6.33062458,  0.94560248],\n         [ 4.52412033, -0.46530545,  0.71960282, -0.91084248,  0.43153140,\n          -1.32478404, -1.19332063, -2.72541595,  6.33062458,  0.94560248]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.80650425, 6.79592991, 5.61102200, 7.24146700, 5.89909315,\n          7.65540886, 7.52394533, 9.05604076, 5.38502216]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.4683 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 4.24215508, -0.69684935,  0.55492795, -0.75052458,  0.43542138,\n         -1.24407911, -1.12097180, -2.80693984,  7.00109816,  0.78772008]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [958]; unstable size 958; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 489 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 415 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [489]; unstable size 489; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 415 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [415]; unstable size 415; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[0.48258770, 5.41662645, 3.66289711, 5.48767662, 3.82613802, 5.83310986,\n         5.75515890, 7.10578251, 3.81086254]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.7489941120147705\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_71_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "15.325042963027954", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp74ou9f2t.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:56:51 2024 on Cerberus\nInternal results will be saved to /tmp/tmp74ou9f2t.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n          1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-1.91160369, -1.75271857,  0.69264895,  2.51886392,  1.29905379,\n           1.52446699,  3.16162252,  0.31408176, -3.63012004, -0.84955752],\n         [-1.91160369, -1.75271857,  0.69264895,  2.51886392,  1.29905379,\n           1.52446699,  3.16162252,  0.31408176, -3.63012004, -0.84955752]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.07322598, 4.91434097, 2.46897364, 0.64275861, 1.86256874,\n          1.63715553, 2.84754086, 6.79174232, 4.01117992]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5042 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n          1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1915]; unstable size 1915; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 111 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [772]; unstable size 772; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 111 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [630]; unstable size 630; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [67]; unstable size 67; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.80871987,  2.45961547,  0.37849379, -0.61176801,  0.52175808,\n          0.03984666,  0.67118931,  4.57306957,  1.52069569]], device='cuda:0') None\nbest_l after optimization: 14.235380172729492 with beta sum per layer: []\nalpha/beta optimization time: 5.300621747970581\ninitial alpha-CROWN bounds: tensor([[ 3.03655910,  2.69559646,  0.59893465, -0.46514177,  0.67544174,\n          0.21066761,  0.89655685,  4.81512547,  1.77163982]], device='cuda:0')\nWorst class: (+ rhs) -0.46514177322387695\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 289.73686385154724\n##### Instance 0 first 10 spec matrices: [[[-1.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.0365591049194336.\n\nProperties batch 1, size 1\nRemaining timeout: 289.69981932640076\n##### Instance 0 first 10 spec matrices: [[[ 0. -1.  0.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.69559645652771.\n\nProperties batch 2, size 1\nRemaining timeout: 289.6724030971527\n##### Instance 0 first 10 spec matrices: [[[ 0.  0. -1.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.5989346504211426.\n\nProperties batch 3, size 1\nRemaining timeout: 289.64780855178833\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0. -1.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.46514177]], device='cuda:0') need to verify.\nModel prediction is: tensor([-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n         1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.46514177]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1915\nlayer 1 size torch.Size([8192]) unstable 749\nlayer 2 size torch.Size([8192]) unstable 614\nlayer 3 size torch.Size([250]) unstable 63\n-----------------\n# of unstable neurons: 3341\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 189] \nsplit level 1: [3, 236] \nsplit level 2: [3, 76] \nsplit level 3: [3, 238] \nsplit level 4: [3, 44] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 7 / 32 = 0.21875\npruning-in-iteration extra time: 0.015497207641601562\nTensors transferred: pre=3.0153M lA=1.1778M alpha=0.2065M beta=0.0002M\nThis batch time : update_bounds func: 0.6833\t prepare: 0.0031\t bound: 0.6726\t transfer: 0.0070\t finalize: 0.0006\nAccumulated time: update_bounds func: 0.6833\t prepare: 0.0031\t bound: 0.6726\t transfer: 0.0070\t finalize: 0.0006\nbatch bounding time:  0.6833648681640625\nCurrent worst splitting domains lb-rhs (depth):\n-0.14887 (5), -0.14311 (5), -0.13851 (5), -0.13621 (5), -0.10090 (5), -0.09864 (5), -0.09558 (5), -0.09537 (5), -0.09237 (5), -0.09080 (5), -0.08998 (5), -0.08598 (5), -0.06068 (5), -0.05457 (5), -0.05413 (5), -0.05390 (5), -0.05302 (5), -0.04999 (5), -0.04704 (5), -0.04551 (5), \nlength of domains: 25\nTotal time: 0.9097\t pickout: 0.0009\t decision: 0.2167\t get_bound: 0.6867\t add_domain: 0.0054\nAccumulated time:\t pickout: 0.0009\t decision: 0.2167\t get_bound: 0.6867\t add_domain: 0.0054\nCurrent (lb-rhs): -0.14886951446533203\n7 domains visited\nCumulative time: 1.073601245880127\n\nbatch:  torch.Size([25, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([25, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] \n(50, 3, 32, 32) torch.Size([50, 1, 10]) torch.Size([50, 1])\npruning_in_iteration open status: False\nratio of positive domain = 10 / 50 = 0.19999999999999996\npruning-in-iteration extra time: 0.00015306472778320312\nTensors transferred: pre=4.7113M lA=2.3557M alpha=0.3227M beta=0.0003M\nThis batch time : update_bounds func: 0.4331\t prepare: 0.0024\t bound: 0.4219\t transfer: 0.0082\t finalize: 0.0006\nAccumulated time: update_bounds func: 1.1164\t prepare: 0.0055\t bound: 1.0945\t transfer: 0.0152\t finalize: 0.0011\nbatch bounding time:  0.4331662654876709\nCurrent worst splitting domains lb-rhs (depth):\n-0.11342 (6), -0.10721 (6), -0.10490 (6), -0.10249 (6), -0.10028 (6), -0.09935 (6), -0.09675 (6), -0.09580 (6), -0.06432 (6), -0.06324 (6), -0.06069 (6), -0.05826 (6), -0.05811 (6), -0.05549 (6), -0.05466 (6), -0.05458 (6), -0.05427 (6), -0.05199 (6), -0.05098 (6), -0.05044 (6), \nlength of domains: 40\nTotal time: 0.4673\t pickout: 0.0013\t decision: 0.0279\t get_bound: 0.4332\t add_domain: 0.0048\nAccumulated time:\t pickout: 0.0022\t decision: 0.2446\t get_bound: 1.1199\t add_domain: 0.0102\nCurrent (lb-rhs): -0.11342048645019531\n17 domains visited\nCumulative time: 1.541212558746338\n\nbatch:  torch.Size([40, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([40, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 86] [3, 119] [3, 119] [3, 119] [3, 86] [3, 86] [3, 119] [3, 119] [3, 86] [3, 119] \n(80, 3, 32, 32) torch.Size([80, 1, 10]) torch.Size([80, 1])\npruning_in_iteration open status: True\nratio of positive domain = 40 / 80 = 0.5\npruning-in-iteration extra time: 0.02480602264404297\nTensors transferred: pre=7.5381M lA=1.8845M alpha=0.5164M beta=0.0005M\nThis batch time : update_bounds func: 0.5176\t prepare: 0.0036\t bound: 0.5048\t transfer: 0.0082\t finalize: 0.0008\nAccumulated time: update_bounds func: 1.6340\t prepare: 0.0091\t bound: 1.5993\t transfer: 0.0234\t finalize: 0.0020\nbatch bounding time:  0.5176095962524414\nCurrent worst splitting domains lb-rhs (depth):\n-0.07957 (7), -0.07407 (7), -0.07299 (7), -0.07111 (7), -0.06886 (7), -0.06856 (7), -0.06591 (7), -0.06535 (7), -0.06301 (7), -0.06225 (7), -0.06194 (7), -0.03045 (7), -0.02705 (7), -0.02641 (7), -0.02465 (7), -0.02439 (7), -0.02411 (7), -0.02389 (7), -0.02070 (7), -0.01987 (7), \nlength of domains: 40\nTotal time: 0.5549\t pickout: 0.0016\t decision: 0.0308\t get_bound: 0.5177\t add_domain: 0.0048\nAccumulated time:\t pickout: 0.0038\t decision: 0.2755\t get_bound: 1.6376\t add_domain: 0.0150\nCurrent (lb-rhs): -0.07957124710083008\n57 domains visited\nCumulative time: 2.096505880355835\n\nbatch:  torch.Size([40, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([40, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 119] [3, 86] [3, 86] [3, 86] [3, 119] [3, 86] [3, 86] [3, 86] [3, 86] [3, 119] \n(80, 3, 32, 32) torch.Size([80, 1, 10]) torch.Size([80, 1])\npruning_in_iteration open status: True\nratio of positive domain = 64 / 80 = 0.8\npruning-in-iteration extra time: 0.0254669189453125\nTensors transferred: pre=7.5381M lA=0.7538M alpha=0.5164M beta=0.0006M\nThis batch time : update_bounds func: 0.5135\t prepare: 0.0033\t bound: 0.5040\t transfer: 0.0053\t finalize: 0.0009\nAccumulated time: update_bounds func: 2.1475\t prepare: 0.0124\t bound: 2.1033\t transfer: 0.0287\t finalize: 0.0029\nbatch bounding time:  0.5136070251464844\nCurrent worst splitting domains lb-rhs (depth):\n-0.04667 (8), -0.04101 (8), -0.03977 (8), -0.03796 (8), -0.03642 (8), -0.03548 (8), -0.03272 (8), -0.03257 (8), -0.03241 (8), -0.03021 (8), -0.02907 (8), -0.02898 (8), -0.02764 (8), -0.02535 (8), -0.02335 (8), -0.02225 (8), \nlength of domains: 16\nTotal time: 0.5473\t pickout: 0.0016\t decision: 0.0288\t get_bound: 0.5137\t add_domain: 0.0032\nAccumulated time:\t pickout: 0.0055\t decision: 0.3042\t get_bound: 2.1512\t add_domain: 0.0182\nCurrent (lb-rhs): -0.04667401313781738\n121 domains visited\nCumulative time: 2.6441304683685303\n\nbatch:  torch.Size([16, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([16, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 21 / 32 = 0.65625\npruning-in-iteration extra time: 0.02553415298461914\nTensors transferred: pre=3.0153M lA=0.5182M alpha=0.2065M beta=0.0003M\nThis batch time : update_bounds func: 0.5099\t prepare: 0.0026\t bound: 0.5034\t transfer: 0.0033\t finalize: 0.0005\nAccumulated time: update_bounds func: 2.6574\t prepare: 0.0150\t bound: 2.6067\t transfer: 0.0321\t finalize: 0.0033\nbatch bounding time:  0.5099911689758301\nCurrent worst splitting domains lb-rhs (depth):\n-0.01575 (9), -0.01069 (9), -0.00833 (9), -0.00693 (9), -0.00548 (9), -0.00480 (9), -0.00447 (9), -0.00223 (9), -0.00112 (9), -0.00060 (9), -0.00032 (9), \nlength of domains: 11\nTotal time: 0.5346\t pickout: 0.0011\t decision: 0.0208\t get_bound: 0.5100\t add_domain: 0.0027\nAccumulated time:\t pickout: 0.0066\t decision: 0.3251\t get_bound: 2.6613\t add_domain: 0.0208\nCurrent (lb-rhs): -0.015746116638183594\n142 domains visited\nCumulative time: 3.1791088581085205\n\nbatch:  torch.Size([11, 32, 32, 32]) pre split depth:  2\nbatch:  torch.Size([11, 32, 32, 32]) post split depth:  2\nsplitting decisions: \nsplit level 0: [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] \nsplit level 1: [3, 207] [3, 207] [3, 207] [3, 207] [3, 179] [3, 207] [3, 207] [3, 207] [3, 207] [3, 207] \n(44, 3, 32, 32) torch.Size([44, 1, 10]) torch.Size([44, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 44 / 44 = 1.0\npruning-in-iteration extra time: 0.00015926361083984375\nTensors transferred: pre=4.1460M lA=2.0730M alpha=0.2840M beta=0.0005M\nThis batch time : update_bounds func: 0.0142\t prepare: 0.0029\t bound: 0.0065\t transfer: 0.0042\t finalize: 0.0005\nAccumulated time: update_bounds func: 2.6717\t prepare: 0.0179\t bound: 2.6133\t transfer: 0.0363\t finalize: 0.0039\nbatch bounding time:  0.014278888702392578\nlength of domains: 0\nTotal time: 0.0431\t pickout: 0.0011\t decision: 0.0229\t get_bound: 0.0177\t add_domain: 0.0014\nAccumulated time:\t pickout: 0.0077\t decision: 0.3480\t get_bound: 2.6789\t add_domain: 0.0223\nNo domains left, verification finished!\n186 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 3.2230935096740723\n\n\nProperties batch 4, size 1\nRemaining timeout: 286.35024189949036\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0. -1.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.6754417419433594.\n\nProperties batch 5, size 1\nRemaining timeout: 286.3115015029907\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0. -1.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.21066761016845703.\n\nProperties batch 6, size 1\nRemaining timeout: 286.28526878356934\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1. -1.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.8965568542480469.\n\nProperties batch 7, size 1\nRemaining timeout: 286.25884890556335\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1.  0. -1.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.815125465393066.\n\nProperties batch 8, size 1\nRemaining timeout: 286.23369240760803\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1.  0.  0. -1.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 1.7716398239135742.\nResult: unsat\nTime: 13.795894145965576\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_79_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "5.9940879344940186", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpl0cgtib5.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:57:08 2024 on Cerberus\nInternal results will be saved to /tmp/tmpl0cgtib5.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 5.74447012,  4.42454815, -2.42684126,  1.39197063, -0.32172263,\n         -1.55043101, -3.09571075, -1.65020883,  9.32281113,  3.43952250]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 5.94260406,  4.11544514, -2.25063801,  1.45511723, -0.20617530,\n          -1.56343246, -3.12074471, -1.37484074,  8.60002995,  3.29764056],\n         [ 5.94260406,  4.11544514, -2.25063801,  1.45511723, -0.20617530,\n          -1.56343246, -3.12074471, -1.37484074,  8.60002995,  3.29764056]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 2.65742588,  4.48458481, 10.85066795,  7.14491272,  8.80620480,\n          10.16346264, 11.72077465,  9.97487068,  5.30238914]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.4414 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 5.74447012,  4.42454815, -2.42684126,  1.39197063, -0.32172263,\n         -1.55043101, -3.09571075, -1.65020883,  9.32281113,  3.43952250]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [651]; unstable size 651; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 409 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 315 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [409]; unstable size 409; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 315 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [315]; unstable size 315; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [29]; unstable size 29; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.02741337,  3.13008642,  9.57361889,  6.11081266,  7.53568316,\n          9.06027126, 10.50288391,  8.74797821,  4.31495619]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.562102317810059\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_89_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.124053716659546", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpi4157oam.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:57:15 2024 on Cerberus\nInternal results will be saved to /tmp/tmpi4157oam.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 4.45820999,  7.45845509, -3.23980594, -2.27957010, -2.48054481,\n         -2.96445680, -1.49695182,  0.53347808,  2.00121641, 12.13869095]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 4.48702860,  8.27062798, -3.10395789, -2.26830912, -2.43770671,\n          -3.00064445, -1.41095793,  0.37030119,  2.13667798, 11.57998276],\n         [ 4.48702860,  8.27062798, -3.10395789, -2.26830912, -2.43770671,\n          -3.00064445, -1.41095793,  0.37030119,  2.13667798, 11.57998276]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 7.09295416,  3.30935478, 14.68394089, 13.84829140, 14.01768970,\n          14.58062744, 12.99094105, 11.20968151,  9.44330502]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.4567 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 4.45820999,  7.45845509, -3.23980594, -2.27957010, -2.48054481,\n         -2.96445680, -1.49695182,  0.53347808,  2.00121641, 12.13869095]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1424]; unstable size 1424; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [779]; unstable size 779; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [711]; unstable size 711; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [72]; unstable size 72; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 3.64582014,  1.50040674, 10.60078907, 10.46342754,  9.78866482,\n         10.80335045,  9.39674187,  6.79292107,  5.55253506]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.690493106842041\n"}, {"network": "cifar10_2_255_simplified", "property": "cifar10_spec_idx_98_eps_0.00784_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.08052659034729", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmppylhf7cu.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:57:23 2024 on Cerberus\nInternal results will be saved to /tmp/tmppylhf7cu.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[11.13540268, -2.12778544,  5.17189455,  3.04508901,  0.87349188,\n          1.41967106,  3.88908839, -2.58672547, -2.09313250,  1.06364989]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[10.81753254, -2.22862196,  5.32995367,  3.01262641,  0.91334116,\n           1.51196551,  3.93635917, -2.66108012, -2.23742366,  0.85475343],\n         [10.81753254, -2.22862196,  5.32995367,  3.01262641,  0.91334116,\n           1.51196551,  3.93635917, -2.66108012, -2.23742366,  0.85475343]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[13.04615402,  5.48757887,  7.80490589,  9.90419102,  9.30556679,\n           6.88117313, 13.47861290, 13.05495644,  9.96277905]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5104 seconds.\nPGD attack failed\nModel prediction is: tensor([[11.13540268, -2.12778544,  5.17189455,  3.04508901,  0.87349188,\n          1.41967106,  3.88908839, -2.58672547, -2.09313250,  1.06364989]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [453]; unstable size 453; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 255 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 185 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [255]; unstable size 255; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 185 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [185]; unstable size 185; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [18]; unstable size 18; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[11.61046410,  5.03864193,  7.01849794,  8.97960472,  8.52255058,\n          6.12826920, 12.15253353, 12.06939507,  8.96183014]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.698111295700073\n"}, {"network": "cifar10_8_255_simplified", "property": "cifar10_spec_idx_11_eps_0.03137_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "7.336820602416992", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmprcn53360.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:57:30 2024 on Cerberus\nInternal results will be saved to /tmp/tmprcn53360.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333730936050415, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.21427953,  1.82282424, -0.95329773, -0.56605446, -0.84413540,\n         -1.23256934, -1.73097754, -0.13519278,  1.34799814,  2.77015758]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.29628807,  2.17531061, -1.04846025, -0.64505720, -0.91715002,\n          -1.33519125, -1.74386621, -0.27812806,  1.57999504,  2.65876412],\n         [ 0.29628807,  2.17531061, -1.04846025, -0.64505720, -0.91715002,\n          -1.33519125, -1.74386621, -0.27812806,  1.57999504,  2.65876412]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[2.36247611, 0.48345351, 3.70722437, 3.30382133, 3.57591414,\n          3.99395537, 4.40263033, 2.93689227, 1.07876909]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.1411 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.21427953,  1.82282424, -0.95329773, -0.56605446, -0.84413540,\n         -1.23256934, -1.73097754, -0.13519278,  1.34799814,  2.77015758]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [271]; unstable size 271; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 72 total_size 128 output_shape 128\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 104 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [764]; unstable size 764; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 104 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [104]; unstable size 104; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 1.12246084, -0.09214251,  2.15115285,  1.82449377,  2.04218793,\n          2.28744054,  2.44691110,  0.59095794, -0.05210567]], device='cuda:0') None\n\nall verified at 15th iter\nbest_l after optimization: 13.648920059204102 with beta sum per layer: []\nalpha/beta optimization time: 1.6283111572265625\ninitial alpha-CROWN bounds: tensor([[1.25593853e+00, 6.02006912e-05, 2.30788565e+00, 1.95581293e+00,\n         2.18180203e+00, 2.43680453e+00, 2.61702108e+00, 8.00339997e-01,\n         9.32549834e-02]], device='cuda:0')\nWorst class: (+ rhs) 6.020069122314453e-05\nverified with init bound!\nResult: unsat\nTime: 5.958808183670044\n"}, {"network": "cifar10_8_255_simplified", "property": "cifar10_spec_idx_23_eps_0.03137_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "SAT", "took": "5.270559310913086", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpx80n9mil.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:57:39 2024 on Cerberus\nInternal results will be saved to /tmp/tmpx80n9mil.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-1.15086710,  3.25605297, -1.23933947, -0.37451699, -0.90260547,\n         -0.36854154, -0.54609567, -0.02293947, -1.39983487,  3.49594378]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-1.04514432,  3.73228550, -1.37737966, -0.42511427, -1.09885812,\n          -0.39768636, -0.55368650, -0.32959464, -1.25875306,  3.67082763],\n         [-1.04514432,  3.73228550, -1.37737966, -0.42511427, -1.09885812,\n          -0.39768636, -0.55368650, -0.32959464, -1.25875306,  3.67082763]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 4.71597195, -0.06145787,  5.04820728,  4.09594202,  4.76968575,\n           4.06851387,  4.22451401,  4.00042248,  4.92958069]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.1870 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 3.809772253036499\n"}, {"network": "cifar10_8_255_simplified", "property": "cifar10_spec_idx_39_eps_0.03137_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "SAT", "took": "5.1725897789001465", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp43c1l2h1.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:57:45 2024 on Cerberus\nInternal results will be saved to /tmp/tmp43c1l2h1.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.85258293,  0.00403488, -0.37607461,  0.47144255, -0.00784957,\n          1.03636110, -0.47195727, -0.07978809,  0.87156272, -0.89035553]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.85305285,  0.83073950, -0.57770157,  0.24319662, -0.20337465,\n           0.67435169, -0.66313702, -0.24651816,  0.99708223, -0.31799167],\n         [-0.85305285,  0.83073950, -0.57770157,  0.24319662, -0.20337465,\n           0.67435169, -0.66313702, -0.24651816,  0.99708223, -0.31799167]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 1.52740455, -0.15638781,  1.25205326,  0.43115509,  0.87772632,\n           1.33748865,  0.92086983, -0.32273054,  0.99234337]]],\n       device='cuda:0')\nnumber of violation:  2\nAttack finished in 1.1812 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 3.715224504470825\n"}, {"network": "cifar10_8_255_simplified", "property": "cifar10_spec_idx_55_eps_0.03137_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "7.214738607406616", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp62l_vb5l.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:57:52 2024 on Cerberus\nInternal results will be saved to /tmp/tmp62l_vb5l.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333704859018326, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.42585230,  0.08431870, -0.01928085, -0.90985799,  0.06956941,\n         -1.05303371, -0.86464828, -0.79066396,  2.44238663,  0.17502323]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.44630742,  0.24157470, -0.05671391, -0.90471673,  0.04474342,\n          -0.98103559, -0.92129189, -0.63299358,  2.13165689,  0.16863680],\n         [ 1.44630742,  0.24157470, -0.05671391, -0.90471673,  0.04474342,\n          -0.98103559, -0.92129189, -0.63299358,  2.13165689,  0.16863680]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[0.68534946, 1.89008212, 2.18837070, 3.03637362, 2.08691359,\n          3.11269236, 3.05294871, 2.76465034, 1.96302009]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.2054 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.42585230,  0.08431870, -0.01928085, -0.90985799,  0.06956941,\n         -1.05303371, -0.86464828, -0.79066396,  2.44238663,  0.17502323]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [239]; unstable size 239; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 65 total_size 128 output_shape 128\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 109 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [1126]; unstable size 1126; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 109 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [109]; unstable size 109; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[-0.02372177,  0.26599938,  0.59504104,  1.23817945,  0.60576856,\n          1.16680133,  1.19513106,  0.77421379,  0.18742943]], device='cuda:0') None\n\nall verified at 2th iter\nbest_l after optimization: 6.494163513183594 with beta sum per layer: []\nalpha/beta optimization time: 1.4537060260772705\ninitial alpha-CROWN bounds: tensor([[0.01864116, 0.32097566, 0.64471412, 1.29487813, 0.65704823, 1.22855043,\n         1.24993324, 0.84288228, 0.23654008]], device='cuda:0')\nWorst class: (+ rhs) 0.018641158938407898\nverified with init bound!\nResult: unsat\nTime: 5.784929275512695\n"}, {"network": "cifar10_8_255_simplified", "property": "cifar10_spec_idx_74_eps_0.03137_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "SAT", "took": "5.082950592041016", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp1__jicvb.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:00 2024 on Cerberus\nInternal results will be saved to /tmp/tmp1__jicvb.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.99456906,  0.92221719, -0.18530147, -1.26380289,  0.52791083,\n         -2.13896990, -2.99637532,  0.54190046,  1.94590044,  1.81956339]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.79191017,  0.95040131, -0.33032101, -1.12356019,  0.31390333,\n          -1.99346793, -2.62409258,  0.25408816,  2.13896036,  1.78995204],\n         [ 1.79191017,  0.95040131, -0.33032101, -1.12356019,  0.31390333,\n          -1.99346793, -2.62409258,  0.25408816,  2.13896036,  1.78995204]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 8.41508865e-01,  2.12223125e+00,  2.91547036e+00,  1.47800684e+00,\n           3.78537798e+00,  4.41600275e+00,  1.53782201e+00, -3.47050190e-01,\n           1.95813179e-03]]], device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.1370 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 3.6598572731018066\n"}, {"network": "cifar10_8_255_simplified", "property": "cifar10_spec_idx_86_eps_0.03137_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "SAT", "took": "5.2255699634552", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmphc4r5_dq.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:07 2024 on Cerberus\nInternal results will be saved to /tmp/tmphc4r5_dq.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.14482355,  0.42702734,  1.17975521,  0.42308298, -0.50366199,\n          0.39120865, -1.85243201, -0.93027544,  0.79907751,  0.54692328]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.13597828,  0.66834402,  0.47835025,  0.33799794, -0.37628934,\n           0.34226054, -1.88110042, -0.92241907,  0.90814900,  0.81491107],\n         [ 0.13597828,  0.66834402,  0.47835025,  0.33799794, -0.37628934,\n           0.34226054, -1.88110042, -0.92241907,  0.90814900,  0.81491107]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 0.34237197, -0.18999377,  0.14035231,  0.85463959,  0.13608971,\n           2.35945058,  1.40076935, -0.42979875, -0.33656082]]],\n       device='cuda:0')\nnumber of violation:  3\nAttack finished in 1.1987 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 3.7878596782684326\n"}, {"network": "cifar10_8_255_simplified", "property": "cifar10_spec_idx_98_eps_0.03137_n1", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "5.743451833724976", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpdkn8ox6l.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:13 2024 on Cerberus\nInternal results will be saved to /tmp/tmpdkn8ox6l.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333730936050415, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 3.94495416, -2.07960415,  1.75831628,  0.85984302, -0.04009467,\n         -0.15203522, -0.26500970, -0.92295241, -1.24032235, -1.26781774]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 3.64041114, -2.15951562,  1.84199905,  0.90193647, -0.02822447,\n          -0.08333384, -0.13002752, -0.84000218, -1.27070475, -1.38328218],\n         [ 3.64041114, -2.15951562,  1.84199905,  0.90193647, -0.02822447,\n          -0.08333384, -0.13002752, -0.84000218, -1.27070475, -1.38328218]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.79992676, 1.79841208, 2.73847461, 3.66863561, 3.72374487,\n          3.77043867, 4.48041344, 4.91111565, 5.02369308]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.1558 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 3.94495416, -2.07960415,  1.75831628,  0.85984302, -0.04009467,\n         -0.15203522, -0.26500970, -0.92295241, -1.24032235, -1.26781774]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [103]; unstable size 103; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 219 total_size 8192 output_shape (128, 8, 8)\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 41 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [219]; unstable size 219; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 41 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [41]; unstable size 41; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[5.12637615, 1.59213471, 2.38341928, 3.30685139, 3.30003548, 3.32878542,\n         3.79563451, 4.43923283, 4.36108780]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.333691596984863\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_10_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.412179946899414", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp6j8_n_co.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:20 2024 on Cerberus\nInternal results will be saved to /tmp/tmp6j8_n_co.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.43827474, -5.26821756, -0.24924204, -1.00314927, -1.42481220,\n         -1.00641870, -2.44114733, -2.40455437,  0.35379457, -4.80686331]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.35088444, -5.26208687, -0.27259591, -0.93527448, -1.39557958,\n          -0.94545192, -2.31221080, -2.32778692,  0.55532157, -4.74117327],\n         [ 1.35088444, -5.26208687, -0.27259591, -0.93527448, -1.39557958,\n          -0.94545192, -2.31221080, -2.32778692,  0.55532157, -4.74117327]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[6.61297131, 1.62348032, 2.28615904, 2.74646401, 2.29633641,\n          3.66309524, 3.67867136, 0.79556286, 6.09205770]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5709 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.43827474, -5.26821756, -0.24924204, -1.00314927, -1.42481220,\n         -1.00641870, -2.44114733, -2.40455437,  0.35379457, -4.80686331]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [3119]; unstable size 3119; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 153 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 49 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [153]; unstable size 153; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 49 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1252]; unstable size 1252; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [79]; unstable size 79; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [41]; unstable size 41; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [67]; unstable size 67; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[5.06930304, 0.73012900, 1.20832038, 1.51309800, 0.97737789, 2.18857384,\n         2.55259299, 0.01476887, 4.82836056]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.996585130691528\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_21_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.302163124084473", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpuu3jvgml.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:28 2024 on Cerberus\nInternal results will be saved to /tmp/tmpuu3jvgml.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 2.14061451, -7.58005762,  1.55118299, -0.69311768, -1.58620059,\n         -3.66751027, -4.57932281, -5.63344622, -4.01265717, -7.37496614]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 2.07994413, -7.58967447,  1.58834994, -0.70260131, -1.58201480,\n          -3.65404916, -4.56366301, -5.60641527, -4.04987907, -7.38536358],\n         [ 2.07994413, -7.58967447,  1.58834994, -0.70260131, -1.58201480,\n          -3.65404916, -4.56366301, -5.60641527, -4.04987907, -7.38536358]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[9.66961861, 0.49159420, 2.78254557, 3.66195893, 5.73399353,\n          6.64360714, 7.68635941, 6.12982321, 9.46530724]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5249 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 2.14061451, -7.58005762,  1.55118299, -0.69311768, -1.58620059,\n         -3.66751027, -4.57932281, -5.63344622, -4.01265717, -7.37496614]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [3565]; unstable size 3565; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 12 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 282 total_size 16384 output_shape (64, 16, 16)\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [12]; unstable size 12; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 282 total_size 16384 output_shape (64, 16, 16)\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [282]; unstable size 282; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [14]; unstable size 14; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [7]; unstable size 7; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [9]; unstable size 9; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[9.27485943, 0.43070382, 2.65081549, 3.50906706, 5.58280373, 6.43016958,\n         7.46400595, 5.93557119, 9.23428917]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.823421239852905\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_37_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.327549934387207", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp8a6w4t_6.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:36 2024 on Cerberus\nInternal results will be saved to /tmp/tmp8a6w4t_6.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.25571486,  3.77851868, -4.00916004, -3.22680521, -4.53054523,\n         -4.37565517, -5.53191614, -4.20376968,  0.47601944,  3.14984918]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.21722656,  3.55940342, -3.96903133, -3.15021801, -4.50026608,\n          -4.26555204, -5.46244669, -4.09355307,  0.32603645,  3.24944854],\n         [ 0.21722656,  3.55940342, -3.96903133, -3.15021801, -4.50026608,\n          -4.26555204, -5.46244669, -4.09355307,  0.32603645,  3.24944854]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.34217691, 7.52843475, 6.70962143, 8.05966949, 7.82495546,\n          9.02185059, 7.65295649, 3.23336697, 0.30995488]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5620 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.25571486,  3.77851868, -4.00916004, -3.22680521, -4.53054523,\n         -4.37565517, -5.53191614, -4.20376968,  0.47601944,  3.14984918]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2147]; unstable size 2147; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 70 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [70]; unstable size 70; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [731]; unstable size 731; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [60]; unstable size 60; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [24]; unstable size 24; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [42]; unstable size 42; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[2.43055296, 6.70373487, 6.04302597, 7.19202280, 7.07307434, 8.29016685,\n         6.64738560, 2.35014272, 0.03637791]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.929975509643555\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_50_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.304852247238159", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpw9t5p8xr.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:44 2024 on Cerberus\nInternal results will be saved to /tmp/tmpw9t5p8xr.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.88575494, -2.46948004, -0.98218197, -1.86838031, -2.75453448,\n         -3.04634857, -3.35564113, -0.76873565, -1.40446281,  0.81824809]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.84236693, -2.81302500, -0.87701511, -1.80246806, -2.65725541,\n          -2.97314787, -3.35251307, -0.61052889, -1.43700242,  0.62772131],\n         [-0.84236693, -2.81302500, -0.87701511, -1.80246806, -2.65725541,\n          -2.97314787, -3.35251307, -0.61052889, -1.43700242,  0.62772131]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.47008824, 3.44074631, 1.50473642, 2.43018937, 3.28497672,\n          3.60086918, 3.98023438, 1.23825026, 2.06472373]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5726 seconds.\nPGD attack failed\nModel prediction is: tensor([[-0.88575494, -2.46948004, -0.98218197, -1.86838031, -2.75453448,\n         -3.04634857, -3.35564113, -0.76873565, -1.40446281,  0.81824809]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2520]; unstable size 2520; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 126 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [126]; unstable size 126; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [956]; unstable size 956; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [68]; unstable size 68; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [27]; unstable size 27; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [27]; unstable size 27; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[1.05932558, 2.63591695, 1.12628901, 2.04380608, 2.82444143, 3.17685628,\n         3.54555535, 0.90777040, 1.59514725]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.923092603683472\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_64_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "9.18410611152649", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpb7u5nff8.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:58:51 2024 on Cerberus\nInternal results will be saved to /tmp/tmpb7u5nff8.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.04189730, -1.39616895,  0.58027250,  0.29884806,  0.46679962,\n          0.18952516,  1.31852853, -0.25026628, -4.26790476, -1.34158278]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.01591706, -1.55922532,  0.63024652,  0.30251116,  0.53322899,\n           0.23309767,  1.22137964, -0.18820685, -4.19713736, -1.40943885],\n         [-2.01591706, -1.55922532,  0.63024652,  0.30251116,  0.53322899,\n           0.23309767,  1.22137964, -0.18820685, -4.19713736, -1.40943885]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.23729658, 2.78060484, 0.59113312, 0.91886848, 0.68815064,\n          0.98828197, 1.40958643, 5.41851711, 2.63081837]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5847 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.04189730, -1.39616895,  0.58027250,  0.29884806,  0.46679962,\n          0.18952516,  1.31852853, -0.25026628, -4.26790476, -1.34158278]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2044]; unstable size 2044; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 138 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 50 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [138]; unstable size 138; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 50 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1413]; unstable size 1413; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [124]; unstable size 124; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [49]; unstable size 49; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [90]; unstable size 90; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 1.81039190,  0.72419631, -0.10799131,  0.31532454,  0.10968924,\n          0.27898511,  0.39123270,  3.65472889,  0.96664143]], device='cuda:0') None\n\nall verified at 11th iter\nbest_l after optimization: 9.575700759887695 with beta sum per layer: []\nalpha/beta optimization time: 2.7866861820220947\ninitial alpha-CROWN bounds: tensor([[1.99569798e+00, 1.03388357e+00, 2.20686197e-03, 4.26993877e-01,\n         1.97594345e-01, 3.70851099e-01, 5.36541998e-01, 3.82398534e+00,\n         1.18794537e+00]], device='cuda:0')\nWorst class: (+ rhs) 0.002206861972808838\nverified with init bound!\nResult: unsat\nTime: 7.7875001430511475\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_78_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "ERR", "result": "ERR", "took": "152.12487196922302", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpmpkuvw18.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 01:59:02 2024 on Cerberus\nInternal results will be saved to /tmp/tmpmpkuvw18.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n          1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.12218428, -3.37873507,  0.83384383,  1.15502465,  0.81430137,\n           1.14255941,  0.72699016, -0.51800579, -3.24479532, -3.37587047],\n         [-2.12218428, -3.37873507,  0.83384383,  1.15502465,  0.81430137,\n           1.14255941,  0.72699016, -0.51800579, -3.24479532, -3.37587047]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.27720881, 4.53375959, 0.32118082, 0.34072328, 0.01246524,\n          0.42803448, 1.67303038, 4.39981985, 4.53089523]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5298 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n          1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [1942]; unstable size 1942; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 171 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [171]; unstable size 171; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1141]; unstable size 1141; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [81]; unstable size 81; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [26]; unstable size 26; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [50]; unstable size 50; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.46627355,  3.58290243,  0.00764847,  0.03963864, -0.11114836,\n         -0.01987219,  1.07829666,  3.52050567,  3.87006474]], device='cuda:0') None\nbest_l after optimization: 14.863643646240234 with beta sum per layer: []\nalpha/beta optimization time: 9.308796882629395\ninitial alpha-CROWN bounds: tensor([[ 2.54750180e+00,  3.65459037e+00,  4.54753637e-02,  7.04848766e-02,\n         -9.75860357e-02,  2.63571739e-03,  1.12346399e+00,  3.59175634e+00,\n          3.92532063e+00]], device='cuda:0')\nWorst class: (+ rhs) -0.09758603572845459\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 64, 16, 16]), torch.Size([1, 9, 64, 8, 8]), torch.Size([1, 9, 512]), torch.Size([1, 9, 512])]\n\nProperties batch 0, size 1\nRemaining timeout: 285.8163959980011\n##### Instance 0 first 10 spec matrices: [[[-1.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.547501802444458.\n\nProperties batch 1, size 1\nRemaining timeout: 285.7774703502655\n##### Instance 0 first 10 spec matrices: [[[ 0. -1.  0.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.654590368270874.\n\nProperties batch 2, size 1\nRemaining timeout: 285.75338554382324\n##### Instance 0 first 10 spec matrices: [[[ 0.  0. -1.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.04547536373138428.\n\nProperties batch 3, size 1\nRemaining timeout: 285.7316176891327\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  1. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.07048487663269043.\n\nProperties batch 4, size 1\nRemaining timeout: 285.71031165122986\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  1.  0. -1.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.09758604]], device='cuda:0') need to verify.\nModel prediction is: tensor([-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n         1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /34 start_node /46 with alignment adjustment\nsetting alpha for layer /36 start_node /46 with alignment adjustment\nsetting alpha for layer /38 start_node /46 with alignment adjustment\nsetting alpha for layer /40 start_node /46 with alignment adjustment\nsetting alpha for layer /43 start_node /46 with alignment adjustment\nsetting alpha for layer /45 start_node /46 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 64, 16, 16]), torch.Size([1, 1, 64, 8, 8]), torch.Size([1, 1, 512]), torch.Size([1, 1, 512])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.09758604]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/46']\nKeeping slopes for these layers: ['/46']\nlayer 0 size torch.Size([32768]) unstable 1942\nlayer 1 size torch.Size([8192]) unstable 166\nlayer 2 size torch.Size([16384]) unstable 1113\nlayer 3 size torch.Size([4096]) unstable 77\nlayer 4 size torch.Size([512]) unstable 25\nlayer 5 size torch.Size([512]) unstable 49\n-----------------\n# of unstable neurons: 3372\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [5, 112] \nsplit level 1: [4, 444] \nsplit level 2: [5, 419] \nsplit level 3: [5, 444] \nsplit level 4: [5, 202] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 14 / 32 = 0.4375\npruning-in-iteration extra time: 0.027996540069580078\nTensors transferred: pre=3.8125M lA=1.0723M alpha=0.2082M beta=0.0002M\nThis batch time : update_bounds func: 0.8921\t prepare: 0.0028\t bound: 0.8804\t transfer: 0.0082\t finalize: 0.0006\nAccumulated time: update_bounds func: 0.8921\t prepare: 0.0028\t bound: 0.8804\t transfer: 0.0082\t finalize: 0.0006\nbatch bounding time:  0.892195463180542\nCurrent worst splitting domains lb-rhs (depth):\n-0.06755 (5), -0.05574 (5), -0.05415 (5), -0.04667 (5), -0.04550 (5), -0.03474 (5), -0.02976 (5), -0.02654 (5), -0.02516 (5), -0.02263 (5), -0.01366 (5), -0.01081 (5), -0.00789 (5), -0.00785 (5), -0.00582 (5), -0.00557 (5), -0.00478 (5), -0.00445 (5), \nlength of domains: 18\nTotal time: 1.1232\t pickout: 0.0010\t decision: 0.2217\t get_bound: 0.8967\t add_domain: 0.0037\nAccumulated time:\t pickout: 0.0010\t decision: 0.2217\t get_bound: 0.8967\t add_domain: 0.0037\nCurrent (lb-rhs): -0.06755101680755615\n14 domains visited\nCumulative time: 1.336472511291504\n\nbatch:  torch.Size([18, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([18, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 36] [4, 166] [4, 36] [4, 166] [4, 36] [4, 395] [4, 36] [4, 395] [4, 36] [4, 395] \n(36, 3, 32, 32) torch.Size([36, 1, 10]) torch.Size([36, 1])\npruning_in_iteration open status: True\nratio of positive domain = 12 / 36 = 0.33333333333333337\npruning-in-iteration extra time: 0.02967047691345215\nTensors transferred: pre=4.2891M lA=1.4297M alpha=0.2342M beta=0.0002M\nThis batch time : update_bounds func: 0.7101\t prepare: 0.0036\t bound: 0.6997\t transfer: 0.0061\t finalize: 0.0006\nAccumulated time: update_bounds func: 1.6022\t prepare: 0.0064\t bound: 1.5802\t transfer: 0.0143\t finalize: 0.0012\nbatch bounding time:  0.7101686000823975\nCurrent worst splitting domains lb-rhs (depth):\n-0.05872 (6), -0.05512 (6), -0.04728 (6), -0.04578 (6), -0.04182 (6), -0.04008 (6), -0.03942 (6), -0.03906 (6), -0.03698 (6), -0.03045 (6), -0.02911 (6), -0.02630 (6), -0.02538 (6), -0.02119 (6), -0.02098 (6), -0.01910 (6), -0.01895 (6), -0.01839 (6), -0.01357 (6), -0.00458 (6), \nlength of domains: 24\nTotal time: 0.7465\t pickout: 0.0015\t decision: 0.0304\t get_bound: 0.7102\t add_domain: 0.0044\nAccumulated time:\t pickout: 0.0024\t decision: 0.2521\t get_bound: 1.6070\t add_domain: 0.0081\nCurrent (lb-rhs): -0.058716416358947754\n26 domains visited\nCumulative time: 2.0832743644714355\n\nbatch:  torch.Size([24, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([24, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 316] [4, 395] [4, 316] [4, 395] [4, 316] [4, 166] [4, 359] [4, 166] [4, 359] [4, 166] \n(48, 3, 32, 32) torch.Size([48, 1, 10]) torch.Size([48, 1])\npruning_in_iteration open status: True\nratio of positive domain = 10 / 48 = 0.20833333333333337\npruning-in-iteration extra time: 0.00701451301574707\nTensors transferred: pre=5.7188M lA=2.2637M alpha=0.3123M beta=0.0004M\nThis batch time : update_bounds func: 0.6478\t prepare: 0.0036\t bound: 0.6353\t transfer: 0.0080\t finalize: 0.0008\nAccumulated time: update_bounds func: 2.2500\t prepare: 0.0100\t bound: 2.2155\t transfer: 0.0224\t finalize: 0.0020\nbatch bounding time:  0.6478745937347412\nCurrent worst splitting domains lb-rhs (depth):\n-0.05447 (7), -0.05081 (7), -0.04514 (7), -0.04290 (7), -0.04260 (7), -0.04246 (7), -0.03844 (7), -0.03811 (7), -0.03644 (7), -0.03600 (7), -0.03532 (7), -0.03352 (7), -0.03320 (7), -0.03210 (7), -0.03058 (7), -0.02986 (7), -0.02700 (7), -0.02597 (7), -0.02535 (7), -0.02505 (7), \nlength of domains: 38\nTotal time: 0.6890\t pickout: 0.0014\t decision: 0.0343\t get_bound: 0.6479\t add_domain: 0.0054\nAccumulated time:\t pickout: 0.0039\t decision: 0.2865\t get_bound: 2.2549\t add_domain: 0.0134\nCurrent (lb-rhs): -0.05446624755859375\n36 domains visited\nCumulative time: 2.772677421569824\n\nbatch:  torch.Size([38, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([38, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 413] [5, 413] [5, 413] [4, 316] [5, 413] [4, 316] [5, 413] [4, 316] [4, 166] [5, 413] \n(76, 3, 32, 32) torch.Size([76, 1, 10]) torch.Size([76, 1])\npruning_in_iteration open status: False\nratio of positive domain = 6 / 76 = 0.07894736842105265\npruning-in-iteration extra time: 0.0001513957977294922\nTensors transferred: pre=9.0547M lA=4.5273M alpha=0.4945M beta=0.0007M\nThis batch time : update_bounds func: 0.6412\t prepare: 0.0051\t bound: 0.6214\t transfer: 0.0131\t finalize: 0.0015\nAccumulated time: update_bounds func: 2.8913\t prepare: 0.0151\t bound: 2.8369\t transfer: 0.0355\t finalize: 0.0035\nbatch bounding time:  0.6412897109985352\nCurrent worst splitting domains lb-rhs (depth):\n-0.05067 (8), -0.04709 (8), -0.04703 (8), -0.04333 (8), -0.04163 (8), -0.03980 (8), -0.03930 (8), -0.03879 (8), -0.03871 (8), -0.03560 (8), -0.03538 (8), -0.03508 (8), -0.03388 (8), -0.03363 (8), -0.03363 (8), -0.03348 (8), -0.03304 (8), -0.03292 (8), -0.03187 (8), -0.03141 (8), \nlength of domains: 70\nTotal time: 0.6911\t pickout: 0.0013\t decision: 0.0401\t get_bound: 0.6413\t add_domain: 0.0083\nAccumulated time:\t pickout: 0.0052\t decision: 0.3266\t get_bound: 2.8962\t add_domain: 0.0217\nCurrent (lb-rhs): -0.05066633224487305\n42 domains visited\nCumulative time: 3.464188814163208\n\nbatch:  torch.Size([70, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([70, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 166] [4, 166] [4, 89] [5, 413] [4, 359] [5, 413] [4, 359] [5, 413] [4, 166] [4, 89] \n(140, 3, 32, 32) torch.Size([140, 1, 10]) torch.Size([140, 1])\npruning_in_iteration open status: False\nratio of positive domain = 24 / 140 = 0.17142857142857137\npruning-in-iteration extra time: 0.0001647472381591797\nTensors transferred: pre=16.6797M lA=8.3398M alpha=0.9108M beta=0.0013M\nThis batch time : update_bounds func: 0.7077\t prepare: 0.0078\t bound: 0.6764\t transfer: 0.0212\t finalize: 0.0021\nAccumulated time: update_bounds func: 3.5990\t prepare: 0.0230\t bound: 3.5134\t transfer: 0.0567\t finalize: 0.0055\nbatch bounding time:  0.7077884674072266\nCurrent worst splitting domains lb-rhs (depth):\n-0.04770 (9), -0.04637 (9), -0.04419 (9), -0.04410 (9), -0.04219 (9), -0.04212 (9), -0.04041 (9), -0.03856 (9), -0.03784 (9), -0.03745 (9), -0.03717 (9), -0.03642 (9), -0.03585 (9), -0.03578 (9), -0.03458 (9), -0.03320 (9), -0.03319 (9), -0.03233 (9), -0.03229 (9), -0.03104 (9), \nlength of domains: 116\nTotal time: 0.7837\t pickout: 0.0032\t decision: 0.0595\t get_bound: 0.7078\t add_domain: 0.0132\nAccumulated time:\t pickout: 0.0083\t decision: 0.3861\t get_bound: 3.6041\t add_domain: 0.0349\nCurrent (lb-rhs): -0.0476992130279541\n66 domains visited\nCumulative time: 4.2487781047821045\n\nbatch:  torch.Size([116, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([116, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 89] [5, 281] [4, 395] [4, 395] [4, 89] [5, 281] [4, 89] [5, 281] [4, 166] [4, 395] \n(232, 3, 32, 32) torch.Size([232, 1, 10]) torch.Size([232, 1])\npruning_in_iteration open status: True\nratio of positive domain = 53 / 232 = 0.22844827586206895\npruning-in-iteration extra time: 0.026392221450805664\nTensors transferred: pre=27.6406M lA=10.6631M alpha=1.5094M beta=0.0027M\nThis batch time : update_bounds func: 0.8886\t prepare: 0.0115\t bound: 0.8377\t transfer: 0.0359\t finalize: 0.0033\nAccumulated time: update_bounds func: 4.4876\t prepare: 0.0345\t bound: 4.3510\t transfer: 0.0926\t finalize: 0.0088\nbatch bounding time:  0.8887217044830322\nCurrent worst splitting domains lb-rhs (depth):\n-0.04478 (10), -0.04331 (10), -0.04140 (10), -0.04135 (10), -0.04002 (10), -0.03924 (10), -0.03905 (10), -0.03863 (10), -0.03750 (10), -0.03653 (10), -0.03563 (10), -0.03554 (10), -0.03529 (10), -0.03463 (10), -0.03442 (10), -0.03376 (10), -0.03374 (10), -0.03307 (10), -0.03290 (10), -0.03279 (10), \nlength of domains: 179\nTotal time: 0.9977\t pickout: 0.0046\t decision: 0.0851\t get_bound: 0.8888\t add_domain: 0.0192\nAccumulated time:\t pickout: 0.0129\t decision: 0.4713\t get_bound: 4.4928\t add_domain: 0.0541\nCurrent (lb-rhs): -0.04478418827056885\n119 domains visited\nCumulative time: 5.247700929641724\n\nbatch:  torch.Size([179, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([179, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 395] [4, 166] [4, 166] [5, 281] [4, 348] [4, 359] [5, 413] [5, 281] [4, 395] [4, 166] \n(358, 3, 32, 32) torch.Size([358, 1, 10]) torch.Size([358, 1])\npruning_in_iteration open status: True\nratio of positive domain = 109 / 358 = 0.3044692737430168\npruning-in-iteration extra time: 0.0303342342376709\nTensors transferred: pre=42.6523M lA=14.8330M alpha=2.3291M beta=0.0044M\nThis batch time : update_bounds func: 1.0711\t prepare: 0.0201\t bound: 0.9936\t transfer: 0.0517\t finalize: 0.0054\nAccumulated time: update_bounds func: 5.5587\t prepare: 0.0546\t bound: 5.3446\t transfer: 0.1444\t finalize: 0.0142\nbatch bounding time:  1.071246862411499\nCurrent worst splitting domains lb-rhs (depth):\n-0.04209 (11), -0.04013 (11), -0.03864 (11), -0.03861 (11), -0.03725 (11), -0.03589 (11), -0.03584 (11), -0.03572 (11), -0.03477 (11), -0.03361 (11), -0.03359 (11), -0.03281 (11), -0.03281 (11), -0.03208 (11), -0.03164 (11), -0.03092 (11), -0.03074 (11), -0.03073 (11), -0.03063 (11), -0.03037 (11), \nlength of domains: 249\nTotal time: 1.2301\t pickout: 0.0071\t decision: 0.1251\t get_bound: 1.0713\t add_domain: 0.0266\nAccumulated time:\t pickout: 0.0200\t decision: 0.5963\t get_bound: 5.5641\t add_domain: 0.0806\nCurrent (lb-rhs): -0.04209423065185547\n228 domains visited\nCumulative time: 6.479094505310059\n\nbatch:  torch.Size([249, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([249, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [4, 89] [4, 348] [4, 348] [4, 89] [4, 348] [4, 348] [5, 281] [4, 89] [5, 281] \n(498, 3, 32, 32) torch.Size([498, 1, 10]) torch.Size([498, 1])\npruning_in_iteration open status: True\nratio of positive domain = 165 / 498 = 0.3313253012048193\npruning-in-iteration extra time: 0.03202986717224121\nTensors transferred: pre=59.3320M lA=19.8369M alpha=3.2400M beta=0.0071M\nThis batch time : update_bounds func: 1.3449\t prepare: 0.0241\t bound: 1.2452\t transfer: 0.0683\t finalize: 0.0069\nAccumulated time: update_bounds func: 6.9036\t prepare: 0.0787\t bound: 6.5899\t transfer: 0.2126\t finalize: 0.0211\nbatch bounding time:  1.3450655937194824\nCurrent worst splitting domains lb-rhs (depth):\n-0.04000 (12), -0.03833 (12), -0.03647 (12), -0.03643 (12), -0.03630 (12), -0.03516 (12), -0.03432 (12), -0.03401 (12), -0.03391 (12), -0.03386 (12), -0.03378 (12), -0.03340 (12), -0.03244 (12), -0.03138 (12), -0.03111 (12), -0.03104 (12), -0.03094 (12), -0.03082 (12), -0.03076 (12), -0.03047 (12), \nlength of domains: 333\nTotal time: 1.5564\t pickout: 0.0091\t decision: 0.1629\t get_bound: 1.3451\t add_domain: 0.0392\nAccumulated time:\t pickout: 0.0291\t decision: 0.7592\t get_bound: 6.9093\t add_domain: 0.1199\nCurrent (lb-rhs): -0.039996981620788574\n393 domains visited\nCumulative time: 8.037494659423828\n\nbatch:  torch.Size([333, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([333, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] \n(666, 3, 32, 32) torch.Size([666, 1, 10]) torch.Size([666, 1])\npruning_in_iteration open status: True\nratio of positive domain = 320 / 666 = 0.48048048048048053\npruning-in-iteration extra time: 0.06425142288208008\nTensors transferred: pre=79.3477M lA=20.6709M alpha=4.3330M beta=0.0108M\nThis batch time : update_bounds func: 1.4928\t prepare: 0.0337\t bound: 1.3809\t transfer: 0.0682\t finalize: 0.0093\nAccumulated time: update_bounds func: 8.3963\t prepare: 0.1124\t bound: 7.9708\t transfer: 0.2808\t finalize: 0.0304\nbatch bounding time:  1.492964744567871\nCurrent worst splitting domains lb-rhs (depth):\n-0.03834 (13), -0.03612 (13), -0.03471 (13), -0.03469 (13), -0.03453 (13), -0.03336 (13), -0.03205 (13), -0.03180 (13), -0.03179 (13), -0.03159 (13), -0.03142 (13), -0.03136 (13), -0.03074 (13), -0.02953 (13), -0.02950 (13), -0.02910 (13), -0.02905 (13), -0.02877 (13), -0.02873 (13), -0.02862 (13), \nlength of domains: 346\nTotal time: 1.7740\t pickout: 0.0122\t decision: 0.2236\t get_bound: 1.4930\t add_domain: 0.0451\nAccumulated time:\t pickout: 0.0413\t decision: 0.9828\t get_bound: 8.4023\t add_domain: 0.1649\nCurrent (lb-rhs): -0.03834176063537598\n713 domains visited\nCumulative time: 9.814426183700562\n\nbatch:  torch.Size([346, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([346, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [4, 348] [5, 45] [5, 45] [5, 45] [5, 45] [5, 45] [4, 348] [5, 281] [5, 45] \n(692, 3, 32, 32) torch.Size([692, 1, 10]) torch.Size([692, 1])\npruning_in_iteration open status: True\nratio of positive domain = 364 / 692 = 0.5260115606936416\npruning-in-iteration extra time: 0.0345306396484375\nTensors transferred: pre=82.4453M lA=19.5391M alpha=4.5021M beta=0.0119M\nThis batch time : update_bounds func: 1.4683\t prepare: 0.0342\t bound: 1.3452\t transfer: 0.0788\t finalize: 0.0095\nAccumulated time: update_bounds func: 9.8647\t prepare: 0.1466\t bound: 9.3160\t transfer: 0.3596\t finalize: 0.0399\nbatch bounding time:  1.4685094356536865\nCurrent worst splitting domains lb-rhs (depth):\n-0.03685 (14), -0.03455 (14), -0.03322 (14), -0.03321 (14), -0.03289 (14), -0.03191 (14), -0.03046 (14), -0.03038 (14), -0.03017 (14), -0.02998 (14), -0.02988 (14), -0.02975 (14), -0.02924 (14), -0.02843 (14), -0.02806 (14), -0.02780 (14), -0.02778 (14), -0.02721 (14), -0.02711 (14), -0.02704 (14), \nlength of domains: 328\nTotal time: 2.1745\t pickout: 0.0126\t decision: 0.6554\t get_bound: 1.4686\t add_domain: 0.0379\nAccumulated time:\t pickout: 0.0540\t decision: 1.6382\t get_bound: 9.8709\t add_domain: 0.2029\nCurrent (lb-rhs): -0.03684568405151367\n1077 domains visited\nCumulative time: 11.991591453552246\n\nbatch:  torch.Size([328, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([328, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] \n(656, 3, 32, 32) torch.Size([656, 1, 10]) torch.Size([656, 1])\npruning_in_iteration open status: True\nratio of positive domain = 343 / 656 = 0.5228658536585367\npruning-in-iteration extra time: 0.03274655342102051\nTensors transferred: pre=78.1562M lA=18.6455M alpha=4.2679M beta=0.0125M\nThis batch time : update_bounds func: 1.3754\t prepare: 0.0311\t bound: 1.2719\t transfer: 0.0619\t finalize: 0.0100\nAccumulated time: update_bounds func: 11.2401\t prepare: 0.1777\t bound: 10.5879\t transfer: 0.4216\t finalize: 0.0499\nbatch bounding time:  1.3755784034729004\nCurrent worst splitting domains lb-rhs (depth):\n-0.03540 (15), -0.03313 (15), -0.03182 (15), -0.03176 (15), -0.03135 (15), -0.03046 (15), -0.02892 (15), -0.02885 (15), -0.02852 (15), -0.02851 (15), -0.02840 (15), -0.02831 (15), -0.02783 (15), -0.02707 (15), -0.02668 (15), -0.02628 (15), -0.02600 (15), -0.02578 (15), -0.02564 (15), -0.02542 (15), \nlength of domains: 313\nTotal time: 1.6198\t pickout: 0.0115\t decision: 0.1856\t get_bound: 1.3756\t add_domain: 0.0470\nAccumulated time:\t pickout: 0.0655\t decision: 1.8237\t get_bound: 11.2465\t add_domain: 0.2499\nCurrent (lb-rhs): -0.035396575927734375\n1420 domains visited\nCumulative time: 13.614446640014648\n\nbatch:  torch.Size([313, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([313, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 354] [5, 454] [4, 251] [4, 251] [5, 354] [4, 348] [4, 348] [5, 354] [3, 1876] [4, 329] \n(626, 3, 32, 32) torch.Size([626, 1, 10]) torch.Size([626, 1])\npruning_in_iteration open status: True\nratio of positive domain = 272 / 626 = 0.43450479233226835\npruning-in-iteration extra time: 0.032067298889160156\nTensors transferred: pre=74.5820M lA=21.0879M alpha=4.0727M beta=0.0125M\nThis batch time : update_bounds func: 1.4732\t prepare: 0.0300\t bound: 1.3678\t transfer: 0.0656\t finalize: 0.0093\nAccumulated time: update_bounds func: 12.7133\t prepare: 0.2077\t bound: 11.9558\t transfer: 0.4871\t finalize: 0.0592\nbatch bounding time:  1.4733376502990723\nCurrent worst splitting domains lb-rhs (depth):\n-0.03438 (16), -0.03222 (16), -0.03080 (16), -0.03072 (16), -0.03030 (16), -0.02945 (16), -0.02792 (16), -0.02784 (16), -0.02758 (16), -0.02749 (16), -0.02735 (16), -0.02725 (16), -0.02681 (16), -0.02634 (16), -0.02595 (16), -0.02564 (16), -0.02526 (16), -0.02494 (16), -0.02473 (16), -0.02457 (16), \nlength of domains: 354\nTotal time: 1.7040\t pickout: 0.0113\t decision: 0.1786\t get_bound: 1.4734\t add_domain: 0.0407\nAccumulated time:\t pickout: 0.0768\t decision: 2.0023\t get_bound: 12.7199\t add_domain: 0.2906\nCurrent (lb-rhs): -0.03438115119934082\n1692 domains visited\nCumulative time: 15.321098804473877\n\nbatch:  torch.Size([354, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([354, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 55] [3, 1876] [4, 329] [4, 329] [4, 329] [5, 40] [5, 40] [5, 40] [4, 251] [4, 251] \n(708, 3, 32, 32) torch.Size([708, 1, 10]) torch.Size([708, 1])\npruning_in_iteration open status: True\nratio of positive domain = 330 / 708 = 0.4661016949152542\npruning-in-iteration extra time: 0.03260397911071777\nTensors transferred: pre=84.3516M lA=22.5176M alpha=4.6062M beta=0.0149M\nThis batch time : update_bounds func: 1.5567\t prepare: 0.0336\t bound: 1.4270\t transfer: 0.0851\t finalize: 0.0105\nAccumulated time: update_bounds func: 14.2700\t prepare: 0.2413\t bound: 13.3827\t transfer: 0.5722\t finalize: 0.0697\nbatch bounding time:  1.5568830966949463\nCurrent worst splitting domains lb-rhs (depth):\n-0.03352 (17), -0.03126 (17), -0.02996 (17), -0.02949 (17), -0.02940 (17), -0.02893 (17), -0.02857 (17), -0.02701 (17), -0.02696 (17), -0.02655 (17), -0.02652 (17), -0.02643 (17), -0.02638 (17), -0.02599 (17), -0.02541 (17), -0.02507 (17), -0.02480 (17), -0.02435 (17), -0.02399 (17), -0.02387 (17), \nlength of domains: 378\nTotal time: 5.9865\t pickout: 0.0131\t decision: 4.3694\t get_bound: 1.5570\t add_domain: 0.0471\nAccumulated time:\t pickout: 0.0899\t decision: 6.3717\t get_bound: 14.2769\t add_domain: 0.3377\nCurrent (lb-rhs): -0.03351736068725586\n2022 domains visited\nCumulative time: 21.310470581054688\n\nbatch:  torch.Size([378, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([378, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 454] [5, 313] [4, 348] [5, 454] [5, 40] [3, 3316] [3, 3316] [5, 454] [5, 201] [5, 201] \n(756, 3, 32, 32) torch.Size([756, 1, 10]) torch.Size([756, 1])\npruning_in_iteration open status: True\nratio of positive domain = 333 / 756 = 0.44047619047619047\npruning-in-iteration extra time: 0.03198647499084473\nTensors transferred: pre=90.0703M lA=25.1982M alpha=4.9185M beta=0.0173M\nThis batch time : update_bounds func: 1.7037\t prepare: 0.0375\t bound: 1.5588\t transfer: 0.0953\t finalize: 0.0114\nAccumulated time: update_bounds func: 15.9737\t prepare: 0.2788\t bound: 14.9415\t transfer: 0.6675\t finalize: 0.0811\nbatch bounding time:  1.7039670944213867\nCurrent worst splitting domains lb-rhs (depth):\n-0.03262 (18), -0.03037 (18), -0.02908 (18), -0.02860 (18), -0.02845 (18), -0.02803 (18), -0.02768 (18), -0.02606 (18), -0.02603 (18), -0.02568 (18), -0.02562 (18), -0.02549 (18), -0.02546 (18), -0.02513 (18), -0.02448 (18), -0.02417 (18), -0.02393 (18), -0.02345 (18), -0.02310 (18), -0.02309 (18), \nlength of domains: 423\nTotal time: 6.6882\t pickout: 0.0132\t decision: 4.8785\t get_bound: 1.7041\t add_domain: 0.0924\nAccumulated time:\t pickout: 0.1031\t decision: 11.2502\t get_bound: 15.9810\t add_domain: 0.4301\nCurrent (lb-rhs): -0.032624125480651855\n2355 domains visited\nCumulative time: 28.002639532089233\n\nbatch:  torch.Size([423, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([423, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 313] [4, 89] [5, 40] [3, 3316] [5, 454] [5, 454] [3, 1876] [3, 3316] [3, 3316] [4, 329] \n(846, 3, 32, 32) torch.Size([846, 1, 10]) torch.Size([846, 1])\npruning_in_iteration open status: True\nratio of positive domain = 384 / 846 = 0.45390070921985815\npruning-in-iteration extra time: 0.03248238563537598\nTensors transferred: pre=100.7930M lA=27.5811M alpha=5.5040M beta=0.0210M\nThis batch time : update_bounds func: 1.8229\t prepare: 0.0416\t bound: 1.6750\t transfer: 0.0935\t finalize: 0.0121\nAccumulated time: update_bounds func: 17.7966\t prepare: 0.3204\t bound: 16.6165\t transfer: 0.7610\t finalize: 0.0932\nbatch bounding time:  1.823136568069458\nCurrent worst splitting domains lb-rhs (depth):\n-0.03182 (19), -0.02933 (19), -0.02840 (19), -0.02830 (19), -0.02774 (19), -0.02761 (19), -0.02717 (19), -0.02689 (19), -0.02522 (19), -0.02515 (19), -0.02472 (19), -0.02463 (19), -0.02462 (19), -0.02461 (19), -0.02428 (19), -0.02392 (19), -0.02389 (19), -0.02370 (19), -0.02360 (19), -0.02323 (19), \nlength of domains: 462\nTotal time: 7.4438\t pickout: 0.0177\t decision: 5.5417\t get_bound: 1.8232\t add_domain: 0.0611\nAccumulated time:\t pickout: 0.1209\t decision: 16.7919\t get_bound: 17.8042\t add_domain: 0.4912\nCurrent (lb-rhs): -0.0318218469619751\n2739 domains visited\nCumulative time: 35.4504759311676\n\nbatch:  torch.Size([462, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([462, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 40] [5, 454] [3, 1876] [3, 1876] [4, 329] [4, 329] [3, 1876] [5, 40] [5, 40] [5, 124] \n(924, 3, 32, 32) torch.Size([924, 1, 10]) torch.Size([924, 1])\npruning_in_iteration open status: True\nratio of positive domain = 375 / 924 = 0.4058441558441559\npruning-in-iteration extra time: 0.03377079963684082\nTensors transferred: pre=110.0859M lA=32.7041M alpha=6.0115M beta=0.0229M\nThis batch time : update_bounds func: 2.0716\t prepare: 0.0440\t bound: 1.9047\t transfer: 0.1085\t finalize: 0.0136\nAccumulated time: update_bounds func: 19.8682\t prepare: 0.3644\t bound: 18.5213\t transfer: 0.8694\t finalize: 0.1069\nbatch bounding time:  2.0719034671783447\nCurrent worst splitting domains lb-rhs (depth):\n-0.03073 (20), -0.03011 (20), -0.02852 (20), -0.02758 (20), -0.02726 (20), -0.02685 (20), -0.02685 (20), -0.02654 (20), -0.02630 (20), -0.02599 (20), -0.02598 (20), -0.02500 (20), -0.02438 (20), -0.02394 (20), -0.02393 (20), -0.02375 (20), -0.02366 (20), -0.02356 (20), -0.02354 (20), -0.02327 (20), \nlength of domains: 549\nTotal time: 8.1779\t pickout: 0.0164\t decision: 5.9998\t get_bound: 2.0720\t add_domain: 0.0897\nAccumulated time:\t pickout: 0.1373\t decision: 22.7917\t get_bound: 19.8762\t add_domain: 0.5810\nCurrent (lb-rhs): -0.030732393264770508\n3114 domains visited\nCumulative time: 43.6317343711853\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1876] [3, 1876] [3, 3627] [4, 329] [5, 96] [5, 96] [4, 329] [4, 329] [5, 96] [3, 1876] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 443 / 1024 = 0.4326171875\npruning-in-iteration extra time: 0.03319597244262695\nTensors transferred: pre=122.0000M lA=34.6104M alpha=6.6621M beta=0.0273M\nThis batch time : update_bounds func: 2.2833\t prepare: 0.0501\t bound: 2.0191\t transfer: 0.1981\t finalize: 0.0152\nAccumulated time: update_bounds func: 22.1515\t prepare: 0.4144\t bound: 20.5404\t transfer: 1.0675\t finalize: 0.1221\nbatch bounding time:  2.283658504486084\nCurrent worst splitting domains lb-rhs (depth):\n-0.02996 (21), -0.02933 (21), -0.02777 (21), -0.02681 (21), -0.02654 (21), -0.02613 (21), -0.02600 (21), -0.02574 (21), -0.02541 (21), -0.02520 (21), -0.02516 (21), -0.02418 (21), -0.02382 (21), -0.02315 (21), -0.02306 (21), -0.02292 (21), -0.02283 (21), -0.02276 (21), -0.02266 (21), -0.02249 (21), \nlength of domains: 618\nTotal time: 9.4617\t pickout: 0.0176\t decision: 7.0672\t get_bound: 2.2837\t add_domain: 0.0932\nAccumulated time:\t pickout: 0.1549\t decision: 29.8589\t get_bound: 22.1599\t add_domain: 0.6741\nCurrent (lb-rhs): -0.029959678649902344\n3557 domains visited\nCumulative time: 53.09743690490723\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 55] [3, 3294] [3, 1876] [3, 1876] [3, 1876] [4, 55] [4, 55] [5, 96] [5, 96] [3, 1876] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 382 / 1024 = 0.373046875\npruning-in-iteration extra time: 0.032460689544677734\nTensors transferred: pre=122.0000M lA=38.2441M alpha=6.6621M beta=0.0273M\nThis batch time : update_bounds func: 2.5932\t prepare: 0.0482\t bound: 2.1573\t transfer: 0.3721\t finalize: 0.0146\nAccumulated time: update_bounds func: 24.7447\t prepare: 0.4627\t bound: 22.6977\t transfer: 1.4396\t finalize: 0.1367\nbatch bounding time:  2.593518018722534\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02860 (22), -0.02777 (21), -0.02614 (22), -0.02582 (22), -0.02539 (22), -0.02521 (22), -0.02498 (22), -0.02472 (22), -0.02461 (22), -0.02451 (22), -0.02439 (22), -0.02414 (22), -0.02355 (22), -0.02347 (22), -0.02315 (21), -0.02311 (22), -0.02225 (22), -0.02221 (22), -0.02207 (22), \nlength of domains: 748\nTotal time: 9.0162\t pickout: 0.0180\t decision: 6.2668\t get_bound: 2.5936\t add_domain: 0.1378\nAccumulated time:\t pickout: 0.1729\t decision: 36.1257\t get_bound: 24.7535\t add_domain: 0.8119\nCurrent (lb-rhs): -0.029248595237731934\n3939 domains visited\nCumulative time: 62.118648052215576\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 3316] [5, 96] [5, 454] [3, 3316] [3, 1883] [3, 1883] [3, 1883] [3, 1883] [5, 454] [3, 670] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 359 / 1024 = 0.3505859375\npruning-in-iteration extra time: 0.03272557258605957\nTensors transferred: pre=122.0000M lA=39.6143M alpha=6.6621M beta=0.0283M\nThis batch time : update_bounds func: 2.4778\t prepare: 0.0484\t bound: 2.2122\t transfer: 0.2008\t finalize: 0.0156\nAccumulated time: update_bounds func: 27.2225\t prepare: 0.5111\t bound: 24.9099\t transfer: 1.6404\t finalize: 0.1523\nbatch bounding time:  2.4781365394592285\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02792 (23), -0.02777 (21), -0.02660 (23), -0.02582 (22), -0.02531 (23), -0.02498 (22), -0.02467 (23), -0.02451 (22), -0.02451 (23), -0.02447 (23), -0.02414 (22), -0.02389 (23), -0.02386 (23), -0.02370 (23), -0.02358 (23), -0.02355 (22), -0.02315 (21), -0.02311 (22), -0.02309 (23), \nlength of domains: 901\nTotal time: 8.8829\t pickout: 0.0185\t decision: 6.2787\t get_bound: 2.4782\t add_domain: 0.1074\nAccumulated time:\t pickout: 0.1914\t decision: 42.4044\t get_bound: 27.2317\t add_domain: 0.9194\nCurrent (lb-rhs): -0.029248595237731934\n4298 domains visited\nCumulative time: 71.00564932823181\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 654] [3, 1883] [5, 40] [5, 40] [3, 1883] [5, 96] [3, 1883] [3, 1883] [5, 454] [5, 454] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 284 / 1024 = 0.27734375\npruning-in-iteration extra time: 0.035649776458740234\nTensors transferred: pre=122.0000M lA=44.0820M alpha=6.6621M beta=0.0283M\nThis batch time : update_bounds func: 2.5960\t prepare: 0.0491\t bound: 2.3949\t transfer: 0.1366\t finalize: 0.0146\nAccumulated time: update_bounds func: 29.8185\t prepare: 0.5602\t bound: 27.3048\t transfer: 1.7770\t finalize: 0.1669\nbatch bounding time:  2.59629225730896\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02711 (24), -0.02660 (23), -0.02616 (24), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02397 (24), -0.02389 (23), -0.02386 (24), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02315 (21), -0.02311 (22), -0.02309 (24), -0.02307 (24), \nlength of domains: 1129\nTotal time: 9.4514\t pickout: 0.0182\t decision: 6.2667\t get_bound: 2.5964\t add_domain: 0.5700\nAccumulated time:\t pickout: 0.2097\t decision: 48.6711\t get_bound: 29.8281\t add_domain: 1.4894\nCurrent (lb-rhs): -0.029248595237731934\n4582 domains visited\nCumulative time: 80.46191143989563\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 40] [5, 40] [5, 40] [3, 1258] [3, 1868] [3, 3051] [5, 454] [5, 40] [5, 40] [4, 191] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 347 / 1024 = 0.3388671875\npruning-in-iteration extra time: 0.032232046127319336\nTensors transferred: pre=122.0000M lA=40.3291M alpha=6.6621M beta=0.0303M\nThis batch time : update_bounds func: 2.5232\t prepare: 0.0494\t bound: 2.2520\t transfer: 0.2051\t finalize: 0.0159\nAccumulated time: update_bounds func: 32.3416\t prepare: 0.6096\t bound: 29.5567\t transfer: 1.9820\t finalize: 0.1828\nbatch bounding time:  2.523470163345337\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02562 (25), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02349 (25), -0.02327 (25), -0.02315 (21), -0.02311 (22), -0.02309 (24), -0.02258 (25), \nlength of domains: 1294\nTotal time: 8.9024\t pickout: 0.0180\t decision: 6.2482\t get_bound: 2.5236\t add_domain: 0.1127\nAccumulated time:\t pickout: 0.2276\t decision: 54.9193\t get_bound: 32.3516\t add_domain: 1.6020\nCurrent (lb-rhs): -0.029248595237731934\n4929 domains visited\nCumulative time: 89.3686318397522\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 454] [3, 3627] [5, 313] [3, 3316] [3, 3294] [3, 1667] [4, 191] [3, 3051] [3, 3294] [3, 3627] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 374 / 1024 = 0.365234375\npruning-in-iteration extra time: 0.03293251991271973\nTensors transferred: pre=122.0000M lA=38.7207M alpha=6.6621M beta=0.0293M\nThis batch time : update_bounds func: 2.4423\t prepare: 0.0483\t bound: 2.1847\t transfer: 0.1923\t finalize: 0.0161\nAccumulated time: update_bounds func: 34.7840\t prepare: 0.6580\t bound: 31.7414\t transfer: 2.1743\t finalize: 0.1988\nbatch bounding time:  2.442678451538086\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02513 (26), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02315 (21), -0.02311 (22), -0.02309 (24), -0.02302 (26), -0.02275 (26), -0.02230 (23), \nlength of domains: 1432\nTotal time: 8.8802\t pickout: 0.0179\t decision: 6.3059\t get_bound: 2.4428\t add_domain: 0.1137\nAccumulated time:\t pickout: 0.2455\t decision: 61.2252\t get_bound: 34.7944\t add_domain: 1.7157\nCurrent (lb-rhs): -0.029248595237731934\n5303 domains visited\nCumulative time: 98.25381016731262\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1667] [3, 670] [3, 1868] [3, 1868] [5, 313] [5, 40] [5, 124] [5, 454] [5, 124] [3, 670] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 342 / 1024 = 0.333984375\npruning-in-iteration extra time: 0.03240156173706055\nTensors transferred: pre=122.0000M lA=40.6270M alpha=6.6621M beta=0.0312M\nThis batch time : update_bounds func: 2.5026\t prepare: 0.0493\t bound: 2.2617\t transfer: 0.1720\t finalize: 0.0188\nAccumulated time: update_bounds func: 37.2866\t prepare: 0.7073\t bound: 34.0031\t transfer: 2.3463\t finalize: 0.2176\nbatch bounding time:  2.5029773712158203\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02463 (27), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02315 (21), -0.02311 (22), -0.02309 (24), -0.02275 (27), -0.02255 (27), -0.02230 (23), \nlength of domains: 1602\nTotal time: 9.0290\t pickout: 0.0181\t decision: 6.2912\t get_bound: 2.5031\t add_domain: 0.2166\nAccumulated time:\t pickout: 0.2636\t decision: 67.5164\t get_bound: 37.2975\t add_domain: 1.9323\nCurrent (lb-rhs): -0.029248595237731934\n5645 domains visited\nCumulative time: 107.28950452804565\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1667] [3, 1883] [3, 1883] [5, 313] [3, 1868] [3, 1883] [3, 3051] [3, 681] [3, 1883] [3, 1883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 294 / 1024 = 0.287109375\npruning-in-iteration extra time: 0.03169369697570801\nTensors transferred: pre=122.0000M lA=43.4863M alpha=6.6621M beta=0.0322M\nThis batch time : update_bounds func: 2.6501\t prepare: 0.0496\t bound: 2.3783\t transfer: 0.2051\t finalize: 0.0161\nAccumulated time: update_bounds func: 39.9367\t prepare: 0.7569\t bound: 36.3814\t transfer: 2.5514\t finalize: 0.2338\nbatch bounding time:  2.6504576206207275\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02415 (28), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02315 (21), -0.02311 (22), -0.02309 (24), -0.02288 (28), -0.02230 (23), -0.02224 (28), \nlength of domains: 1820\nTotal time: 9.0900\t pickout: 0.0189\t decision: 6.2937\t get_bound: 2.6505\t add_domain: 0.1268\nAccumulated time:\t pickout: 0.2826\t decision: 73.8101\t get_bound: 39.9480\t add_domain: 2.0591\nCurrent (lb-rhs): -0.029248595237731934\n5939 domains visited\nCumulative time: 116.38372468948364\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1883] [5, 124] [3, 670] [5, 313] [3, 654] [5, 124] [3, 1876] [4, 191] [3, 3051] [5, 124] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 343 / 1024 = 0.3349609375\npruning-in-iteration extra time: 0.03176283836364746\nTensors transferred: pre=122.0000M lA=40.5674M alpha=6.6621M beta=0.0352M\nThis batch time : update_bounds func: 2.5489\t prepare: 0.0516\t bound: 2.2743\t transfer: 0.2062\t finalize: 0.0160\nAccumulated time: update_bounds func: 42.4856\t prepare: 0.8085\t bound: 38.6557\t transfer: 2.7576\t finalize: 0.2497\nbatch bounding time:  2.5491952896118164\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02370 (29), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02315 (21), -0.02311 (22), -0.02309 (24), -0.02241 (29), -0.02230 (23), -0.02199 (22), \nlength of domains: 1989\nTotal time: 8.9869\t pickout: 0.0180\t decision: 6.2990\t get_bound: 2.5493\t add_domain: 0.1206\nAccumulated time:\t pickout: 0.3006\t decision: 80.1091\t get_bound: 42.4973\t add_domain: 2.1797\nCurrent (lb-rhs): -0.029248595237731934\n6282 domains visited\nCumulative time: 125.37485957145691\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 681] [3, 3324] [3, 1258] [3, 3051] [3, 3286] [2, 4216] [3, 3286] [5, 124] [4, 191] [3, 1883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 251 / 1024 = 0.2451171875\npruning-in-iteration extra time: 0.02964472770690918\nTensors transferred: pre=122.0000M lA=46.1670M alpha=6.6621M beta=0.0381M\nThis batch time : update_bounds func: 2.7305\t prepare: 0.0501\t bound: 2.4888\t transfer: 0.1716\t finalize: 0.0191\nAccumulated time: update_bounds func: 45.2161\t prepare: 0.8585\t bound: 41.1445\t transfer: 2.9292\t finalize: 0.2688\nbatch bounding time:  2.7309353351593018\nTraceback (most recent call last):\n  File \"abcrown.py\", line 647, in <module>\n    main()\n  File \"abcrown.py\", line 570, in main\n    refined_betas=refined_betas, attack_images=all_adv_candidates, attack_margins=attack_margins)\n  File \"abcrown.py\", line 392, in complete_verifier\n    attack_images=this_spec_attack_images)\n  File \"abcrown.py\", line 206, in bab\n    timeout=timeout, refined_betas=refined_betas, rhs=rhs)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py\", line 561, in relu_bab_parallel\n    stop_func=stop_criterion, multi_spec_keep_func=multi_spec_keep_func)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py\", line 283, in batch_verification\n    branching_decision, rhs, intermediate_betas, check_infeasibility, dom_cs, (2*num_copy)*batch)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/branching_domains.py\", line 536, in add\n    [lb.append(new_lb[right_indexer + batch]) if new_lb is not None else None for lb, new_lb in zip(self.all_lb_alls, lb_alls)]\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/branching_domains.py\", line 536, in <listcomp>\n    [lb.append(new_lb[right_indexer + batch]) if new_lb is not None else None for lb, new_lb in zip(self.all_lb_alls, lb_alls)]\n  File \"/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/tensor_storage.py\", line 70, in append\n    new_tensor = self._allocate(new_size)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/tensor_storage.py\", line 51, in _allocate\n    return torch.empty(allocate_shape, dtype=self.dtype, device=self.device, pin_memory=True)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_88_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "UNSAT", "took": "6.998533010482788", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp2die5ars.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 02:01:35 2024 on Cerberus\nInternal results will be saved to /tmp/tmp2die5ars.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.39393386, -0.71249902, -1.62325943, -1.73206186, -0.67567652,\n         -2.47843599, -3.07558155, -1.90466976,  2.34261847, -0.12327250]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.49835101, -0.59551758, -1.62404847, -1.84851956, -0.67073911,\n          -2.59058094, -3.11322832, -1.92009211,  2.25205374, -0.12188021],\n         [ 0.49835101, -0.59551758, -1.62404847, -1.84851956, -0.67073911,\n          -2.59058094, -3.11322832, -1.92009211,  2.25205374, -0.12188021]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.75370276, 2.84757137, 3.87610221, 4.10057354, 2.92279291,\n          4.84263468, 5.36528206, 4.17214584, 2.37393403]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6797 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.39393386, -0.71249902, -1.62325943, -1.73206186, -0.67567652,\n         -2.47843599, -3.07558155, -1.90466976,  2.34261847, -0.12327250]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2270]; unstable size 2270; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 82 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [82]; unstable size 82; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1004]; unstable size 1004; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [71]; unstable size 71; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [35]; unstable size 35; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [68]; unstable size 68; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[1.14224076, 1.87831330, 2.95792055, 3.11696959, 1.87635398, 3.81624889,\n         4.26846266, 2.82570076, 1.48256898]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.504642963409424\n"}, {"network": "convBigRELU__PGD", "property": "cifar10_spec_idx_97_eps_0.00784", "timeout": "300", "verifier": "abcrown", "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})", "success": "OK", "result": "SAT", "took": "4.97473669052124", "stderr": "", "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpcksf9vot.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Tue May 14 02:01:44 2024 on Cerberus\nInternal results will be saved to /tmp/tmpcksf9vot.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.96105886, -4.99818325,  0.49796951, -1.34900188,  0.09806482,\n         -1.69785976, -2.05051923, -2.63068223,  0.86458349, -4.01851368]],\n       device='cuda:0')\npgd early stop\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.90111589, -5.00151634,  0.45460641, -1.36289418,  0.06061684,\n          -1.70949721, -2.10207391, -2.64322639,  0.98923618, -3.94865346],\n         [ 0.90111589, -5.00151634,  0.45460641, -1.36289418,  0.06061684,\n          -1.70949721, -2.10207391, -2.64322639,  0.98923618, -3.94865346]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.90263224,  0.44650948,  2.26400995,  0.84049904,  2.61061311,\n           3.00318980,  3.54434228, -0.08812028,  4.84976959]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 0.9193 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 3.522308826446533\n"}]}