{
    "instances": [
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_0_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "6.060031414031982",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp12arjp5j.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:10:40 2024 on Cerberus\nInternal results will be saved to /tmp/tmp12arjp5j.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.67527163, -1.52271450,  0.63711810,  4.41396284,  0.79158354,\n          3.94307709,  1.38516212, -1.23928893, -1.21486461, -1.90701199]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.59891230, -1.67729628,  0.56920362,  4.30161333,  0.84799862,\n           4.37968922,  1.18421865, -1.15600204, -1.47534275, -1.94664717],\n         [-0.59891230, -1.67729628,  0.56920362,  4.30161333,  0.84799862,\n           4.37968922,  1.18421865, -1.15600204, -1.47534275, -1.94664717]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 4.90052557,  5.97890949,  3.73240972,  3.45361471, -0.07807589,\n           3.11739469,  5.45761538,  5.77695608,  6.24826050]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.5236 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.52702522277832\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_9_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "14.119056940078735",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpzhs5xe06.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:10:48 2024 on Cerberus\nInternal results will be saved to /tmp/tmpzhs5xe06.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n         -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.38077497,  7.11155891, -0.81550092,  0.36877590, -1.63488841,\n          -0.19252896,  0.41075173, -2.10415339,  2.38559103,  6.26932716],\n         [ 1.38077497,  7.11155891, -0.81550092,  0.36877590, -1.63488841,\n          -0.19252896,  0.41075173, -2.10415339,  2.38559103,  6.26932716]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.73078394, 7.92705965, 6.74278307, 8.74644756, 7.30408764,\n          6.70080709, 9.21571255, 4.72596788, 0.84223175]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.8194 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n         -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1386]; unstable size 1386; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [725]; unstable size 725; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [675]; unstable size 675; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.91427422,  4.55402946,  4.00052071,  5.54376316,  4.45263672,\n          3.79798222,  5.70426083,  1.83534622, -0.87034988]], device='cuda:0') None\nbest_l after optimization: 35.21476745605469 with beta sum per layer: []\nalpha/beta optimization time: 5.698338270187378\ninitial alpha-CROWN bounds: tensor([[ 3.25364685,  4.91604710,  4.39052010,  5.89016533,  4.82441044,\n          4.12997150,  6.11451769,  2.23822498, -0.54273725]], device='cuda:0')\nWorst class: (+ rhs) -0.5427372455596924\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 289.0324954986572\n##### Instance 0 first 10 spec matrices: [[[-1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.2536468505859375.\n\nProperties batch 1, size 1\nRemaining timeout: 288.9888460636139\n##### Instance 0 first 10 spec matrices: [[[ 0.  1. -1.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.916047096252441.\n\nProperties batch 2, size 1\nRemaining timeout: 288.9603235721588\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0. -1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.390520095825195.\n\nProperties batch 3, size 1\nRemaining timeout: 288.9332683086395\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 5.890165328979492.\n\nProperties batch 4, size 1\nRemaining timeout: 288.90814876556396\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0. -1.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.824410438537598.\n\nProperties batch 5, size 1\nRemaining timeout: 288.88251304626465\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0. -1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.129971504211426.\n\nProperties batch 6, size 1\nRemaining timeout: 288.8586411476135\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0. -1.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 6.114517688751221.\n\nProperties batch 7, size 1\nRemaining timeout: 288.83444714546204\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0.  0. -1.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.238224983215332.\n\nProperties batch 8, size 1\nRemaining timeout: 288.81063747406006\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0.  0.  0. -1.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.54273725]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n        -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.54273725]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1386\nlayer 1 size torch.Size([8192]) unstable 713\nlayer 2 size torch.Size([8192]) unstable 658\nlayer 3 size torch.Size([250]) unstable 51\n-----------------\n# of unstable neurons: 2808\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 56] \nsplit level 1: [3, 27] \nsplit level 2: [3, 169] \nsplit level 3: [3, 203] \nsplit level 4: [3, 43] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 30 / 32 = 0.9375\npruning-in-iteration extra time: 0.028387069702148438\nTensors transferred: pre=3.0153M lA=0.0942M alpha=0.1733M beta=0.0002M\nThis batch time : update_bounds func: 0.7247\t prepare: 0.0033\t bound: 0.7134\t transfer: 0.0075\t finalize: 0.0005\nAccumulated time: update_bounds func: 0.7247\t prepare: 0.0033\t bound: 0.7134\t transfer: 0.0075\t finalize: 0.0005\nbatch bounding time:  0.7248015403747559\nCurrent worst splitting domains lb-rhs (depth):\n-0.04987 (5), -0.02502 (5), \nlength of domains: 2\nTotal time: 0.9501\t pickout: 0.0008\t decision: 0.2195\t get_bound: 0.7285\t add_domain: 0.0012\nAccumulated time:\t pickout: 0.0008\t decision: 0.2195\t get_bound: 0.7285\t add_domain: 0.0012\nCurrent (lb-rhs): -0.04987192153930664\n30 domains visited\nCumulative time: 1.2485990524291992\n\nbatch:  torch.Size([2, 32, 32, 32]) pre split depth:  4\nbatch:  torch.Size([2, 32, 32, 32]) post split depth:  4\nsplitting decisions: \nsplit level 0: [3, 40] [3, 40] \nsplit level 1: [3, 112] [3, 112] \nsplit level 2: [3, 207] [3, 207] \nsplit level 3: [3, 116] [3, 116] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 32 / 32 = 1.0\npruning-in-iteration extra time: 0.00011420249938964844\nTensors transferred: pre=3.0153M lA=1.5076M alpha=0.1733M beta=0.0003M\nThis batch time : update_bounds func: 0.0125\t prepare: 0.0019\t bound: 0.0056\t transfer: 0.0045\t finalize: 0.0004\nAccumulated time: update_bounds func: 0.7372\t prepare: 0.0051\t bound: 0.7191\t transfer: 0.0120\t finalize: 0.0009\nbatch bounding time:  0.012546539306640625\nlength of domains: 0\nTotal time: 0.0406\t pickout: 0.0007\t decision: 0.0236\t get_bound: 0.0156\t add_domain: 0.0007\nAccumulated time:\t pickout: 0.0015\t decision: 0.2431\t get_bound: 0.7441\t add_domain: 0.0019\nNo domains left, verification finished!\n62 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 1.2896955013275146\n\nResult: unsat\nTime: 12.56140398979187\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_17_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.298092603683472",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpzqt4lvjv.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:11:03 2024 on Cerberus\nInternal results will be saved to /tmp/tmpzqt4lvjv.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.71794301, -2.73549056,  0.46490240,  2.03653789,  2.18022227,\n          1.30874860, -0.23802117,  5.58075476, -1.34880483, -1.73270965]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.48575890, -2.88859630,  0.55571985,  2.02314472,  2.39839435,\n           1.23517108, -0.19022425,  4.85715294, -1.25886095, -1.80295110],\n         [-0.48575890, -2.88859630,  0.55571985,  2.02314472,  2.39839435,\n           1.23517108, -0.19022425,  4.85715294, -1.25886095, -1.80295110]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.34291172, 7.74574947, 4.30143309, 2.83400822, 2.45875859,\n          3.62198186, 5.04737711, 6.11601400, 6.66010380]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5544 seconds.\nPGD attack failed\nModel prediction is: tensor([[-0.71794301, -2.73549056,  0.46490240,  2.03653789,  2.18022227,\n          1.30874860, -0.23802117,  5.58075476, -1.34880483, -1.73270965]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1626]; unstable size 1626; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [939]; unstable size 939; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [715]; unstable size 715; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [67]; unstable size 67; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[2.33360100, 3.98111582, 2.04257393, 1.23024130, 1.41948509, 1.87243629,\n         3.18333197, 3.42234945, 2.19061136]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.8445258140563965\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_27_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "TIMEOUT",
            "took": "300",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp8lmol7g6.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:11:11 2024 on Cerberus\nInternal results will be saved to /tmp/tmp8lmol7g6.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n          0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 2.89373541, -4.66593504,  1.82701683,  0.85894787,  2.86403251,\n           0.92236614, -0.28302976,  1.52670109, -3.56230187, -0.78883207],\n         [ 2.89373541, -4.66593504,  1.82701683,  0.85894787,  2.86403251,\n           0.92236614, -0.28302976,  1.52670109, -3.56230187, -0.78883207]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[7.55967045, 1.06671858, 2.03478765, 0.02970290, 1.97136927,\n          3.17676520, 1.36703432, 6.45603752, 3.68256760]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5601 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n          0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1437]; unstable size 1437; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [675]; unstable size 675; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [555]; unstable size 555; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 5.91307020, -0.15005422,  0.85581970, -0.97932029,  0.71283913,\n          1.95830917, -0.64448357,  4.80564117,  1.95628214]], device='cuda:0') None\nbest_l after optimization: 15.584517478942871 with beta sum per layer: []\nalpha/beta optimization time: 5.204577922821045\ninitial alpha-CROWN bounds: tensor([[ 6.05828190, -0.03196836,  0.96516585, -0.86231709,  0.83091259,\n          2.05264521, -0.49636269,  4.94511271,  2.12304664]], device='cuda:0')\nWorst class: (+ rhs) -0.8623170852661133\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 289.9489235877991\n##### Instance 0 first 10 spec matrices: [[[ 1. -1.  0.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 6.058281898498535.\n\nProperties batch 1, size 1\nRemaining timeout: 289.91292214393616\n##### Instance 0 first 10 spec matrices: [[[ 1.  0. -1.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.03196836]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n         0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.03196836]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1437\nlayer 1 size torch.Size([8192]) unstable 665\nlayer 2 size torch.Size([8192]) unstable 541\nlayer 3 size torch.Size([250]) unstable 53\n-----------------\n# of unstable neurons: 2696\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 123] \nsplit level 1: [3, 247] \nsplit level 2: [3, 248] \nsplit level 3: [3, 126] \nsplit level 4: [3, 55] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 32 / 32 = 1.0\npruning-in-iteration extra time: 0.00016164779663085938\nTensors transferred: pre=3.0153M lA=1.5076M alpha=0.1661M beta=0.0002M\nThis batch time : update_bounds func: 0.0198\t prepare: 0.0027\t bound: 0.0094\t transfer: 0.0072\t finalize: 0.0004\nAccumulated time: update_bounds func: 0.0198\t prepare: 0.0027\t bound: 0.0094\t transfer: 0.0072\t finalize: 0.0004\nbatch bounding time:  0.019815683364868164\nlength of domains: 0\nTotal time: 0.2449\t pickout: 0.0009\t decision: 0.2202\t get_bound: 0.0231\t add_domain: 0.0007\nAccumulated time:\t pickout: 0.0009\t decision: 0.2202\t get_bound: 0.0231\t add_domain: 0.0007\nNo domains left, verification finished!\n32 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 0.42005252838134766\n\n\nProperties batch 2, size 1\nRemaining timeout: 289.4157249927521\n##### Instance 0 first 10 spec matrices: [[[ 1.  0.  0. -1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.9651658535003662.\n\nProperties batch 3, size 1\nRemaining timeout: 289.38272047042847\n##### Instance 0 first 10 spec matrices: [[[ 1.  0.  0.  0. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.86231709]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n         0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.86231709]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1437\nlayer 1 size torch.Size([8192]) unstable 665\nlayer 2 size torch.Size([8192]) unstable 541\nlayer 3 size torch.Size([250]) unstable 53\n-----------------\n# of unstable neurons: 2696\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 134] \nsplit level 1: [3, 247] \nsplit level 2: [3, 123] \nsplit level 3: [3, 71] \nsplit level 4: [3, 52] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 11 / 32 = 0.34375\npruning-in-iteration extra time: 0.021520614624023438\nTensors transferred: pre=3.0153M lA=0.9894M alpha=0.1661M beta=0.0002M\nThis batch time : update_bounds func: 0.7023\t prepare: 0.0029\t bound: 0.6931\t transfer: 0.0057\t finalize: 0.0004\nAccumulated time: update_bounds func: 0.7220\t prepare: 0.0056\t bound: 0.7025\t transfer: 0.0129\t finalize: 0.0008\nbatch bounding time:  0.7023367881774902\nCurrent worst splitting domains lb-rhs (depth):\n-0.51872 (5), -0.50867 (5), -0.47914 (5), -0.47639 (5), -0.46053 (5), -0.44768 (5), -0.40704 (5), -0.39984 (5), -0.26980 (5), -0.25875 (5), -0.25381 (5), -0.24612 (5), -0.24009 (5), -0.22822 (5), -0.22390 (5), -0.21487 (5), -0.19789 (5), -0.18600 (5), -0.16737 (5), -0.14893 (5), \nlength of domains: 21\nTotal time: 0.7405\t pickout: 0.0007\t decision: 0.0308\t get_bound: 0.7062\t add_domain: 0.0029\nAccumulated time:\t pickout: 0.0007\t decision: 0.0308\t get_bound: 0.7062\t add_domain: 0.0029\nCurrent (lb-rhs): -0.5187225341796875\n11 domains visited\nCumulative time: 0.747816801071167\n\nbatch:  torch.Size([21, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([21, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] \n(42, 3, 32, 32) torch.Size([42, 1, 10]) torch.Size([42, 1])\npruning_in_iteration open status: True\nratio of positive domain = 14 / 42 = 0.33333333333333337\npruning-in-iteration extra time: 0.01766061782836914\nTensors transferred: pre=3.9575M lA=1.3192M alpha=0.2180M beta=0.0002M\nThis batch time : update_bounds func: 0.4922\t prepare: 0.0022\t bound: 0.4843\t transfer: 0.0052\t finalize: 0.0005\nAccumulated time: update_bounds func: 1.2142\t prepare: 0.0078\t bound: 1.1868\t transfer: 0.0181\t finalize: 0.0014\nbatch bounding time:  0.49225902557373047\nCurrent worst splitting domains lb-rhs (depth):\n-0.47139 (6), -0.46284 (6), -0.43350 (6), -0.43137 (6), -0.41344 (6), -0.40248 (6), -0.36403 (6), -0.36149 (6), -0.35578 (6), -0.33286 (6), -0.29661 (6), -0.28733 (6), -0.27547 (6), -0.24270 (6), -0.22407 (6), -0.21333 (6), -0.20804 (6), -0.19957 (6), -0.19530 (6), -0.19490 (6), \nlength of domains: 28\nTotal time: 0.5202\t pickout: 0.0011\t decision: 0.0232\t get_bound: 0.4923\t add_domain: 0.0035\nAccumulated time:\t pickout: 0.0018\t decision: 0.0539\t get_bound: 1.1985\t add_domain: 0.0064\nCurrent (lb-rhs): -0.471386194229126\n25 domains visited\nCumulative time: 1.2682368755340576\n\nbatch:  torch.Size([28, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([28, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] \n(56, 3, 32, 32) torch.Size([56, 1, 10]) torch.Size([56, 1])\npruning_in_iteration open status: True\nratio of positive domain = 12 / 56 = 0.2142857142857143\npruning-in-iteration extra time: 0.0001761913299560547\nTensors transferred: pre=5.2767M lA=2.6384M alpha=0.2906M beta=0.0004M\nThis batch time : update_bounds func: 0.4524\t prepare: 0.0026\t bound: 0.4415\t transfer: 0.0076\t finalize: 0.0006\nAccumulated time: update_bounds func: 1.6666\t prepare: 0.0104\t bound: 1.6283\t transfer: 0.0257\t finalize: 0.0020\nbatch bounding time:  0.45243167877197266\nCurrent worst splitting domains lb-rhs (depth):\n-0.43693 (7), -0.42708 (7), -0.39961 (7), -0.39623 (7), -0.37680 (7), -0.36551 (7), -0.34704 (7), -0.33570 (7), -0.32636 (7), -0.32499 (7), -0.31908 (7), -0.29468 (7), -0.28918 (7), -0.28457 (7), -0.27956 (7), -0.27038 (7), -0.25666 (7), -0.24449 (7), -0.23416 (7), -0.22628 (7), \nlength of domains: 44\nTotal time: 0.4856\t pickout: 0.0012\t decision: 0.0267\t get_bound: 0.4525\t add_domain: 0.0052\nAccumulated time:\t pickout: 0.0031\t decision: 0.0807\t get_bound: 1.6510\t add_domain: 0.0116\nCurrent (lb-rhs): -0.4369344711303711\n37 domains visited\nCumulative time: 1.7548158168792725\n\nbatch:  torch.Size([44, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([44, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] \n(88, 3, 32, 32) torch.Size([88, 1, 10]) torch.Size([88, 1])\npruning_in_iteration open status: True\nratio of positive domain = 25 / 88 = 0.28409090909090906\npruning-in-iteration extra time: 0.017689228057861328\nTensors transferred: pre=8.2920M lA=2.9681M alpha=0.4567M beta=0.0007M\nThis batch time : update_bounds func: 0.5302\t prepare: 0.0035\t bound: 0.5115\t transfer: 0.0142\t finalize: 0.0009\nAccumulated time: update_bounds func: 2.1968\t prepare: 0.0139\t bound: 2.1398\t transfer: 0.0399\t finalize: 0.0029\nbatch bounding time:  0.5302729606628418\nCurrent worst splitting domains lb-rhs (depth):\n-0.40897 (8), -0.39871 (8), -0.37132 (8), -0.36745 (8), -0.34901 (8), -0.33765 (8), -0.31770 (8), -0.30631 (8), -0.29811 (8), -0.29695 (8), -0.29110 (8), -0.29056 (8), -0.28809 (8), -0.26677 (8), -0.26006 (8), -0.25461 (8), -0.25300 (8), -0.24933 (8), -0.24835 (8), -0.24094 (8), \nlength of domains: 63\nTotal time: 0.5744\t pickout: 0.0016\t decision: 0.0349\t get_bound: 0.5303\t add_domain: 0.0076\nAccumulated time:\t pickout: 0.0047\t decision: 0.1156\t get_bound: 2.1813\t add_domain: 0.0191\nCurrent (lb-rhs): -0.4089663028717041\n62 domains visited\nCumulative time: 2.330122232437134\n\nbatch:  torch.Size([63, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([63, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] \n(126, 3, 32, 32) torch.Size([126, 1, 10]) torch.Size([126, 1])\npruning_in_iteration open status: False\nratio of positive domain = 13 / 126 = 0.10317460317460314\npruning-in-iteration extra time: 0.00015878677368164062\nTensors transferred: pre=11.8726M lA=5.9363M alpha=0.6539M beta=0.0011M\nThis batch time : update_bounds func: 0.5276\t prepare: 0.0048\t bound: 0.5019\t transfer: 0.0189\t finalize: 0.0019\nAccumulated time: update_bounds func: 2.7244\t prepare: 0.0187\t bound: 2.6417\t transfer: 0.0588\t finalize: 0.0048\nbatch bounding time:  0.5276813507080078\nCurrent worst splitting domains lb-rhs (depth):\n-0.38090 (9), -0.37239 (9), -0.37092 (9), -0.36096 (9), -0.34137 (9), -0.33827 (9), -0.33674 (9), -0.33229 (9), -0.32070 (9), -0.31300 (9), -0.30942 (9), -0.30053 (9), -0.28946 (9), -0.27811 (9), -0.27461 (9), -0.26700 (9), -0.26686 (9), -0.26371 (9), -0.26330 (9), -0.26261 (9), \nlength of domains: 113\nTotal time: 0.5856\t pickout: 0.0021\t decision: 0.0455\t get_bound: 0.5277\t add_domain: 0.0103\nAccumulated time:\t pickout: 0.0068\t decision: 0.1611\t get_bound: 2.7090\t add_domain: 0.0294\nCurrent (lb-rhs): -0.38089609146118164\n75 domains visited\nCumulative time: 2.9166388511657715\n\nbatch:  torch.Size([113, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([113, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] \n(226, 3, 32, 32) torch.Size([226, 1, 10]) torch.Size([226, 1])\npruning_in_iteration open status: True\nratio of positive domain = 110 / 226 = 0.48672566371681414\npruning-in-iteration extra time: 0.027638912200927734\nTensors transferred: pre=21.2953M lA=5.4652M alpha=1.1729M beta=0.0022M\nThis batch time : update_bounds func: 0.6589\t prepare: 0.0106\t bound: 0.6178\t transfer: 0.0282\t finalize: 0.0021\nAccumulated time: update_bounds func: 3.3833\t prepare: 0.0293\t bound: 3.2595\t transfer: 0.0870\t finalize: 0.0070\nbatch bounding time:  0.6589548587799072\nCurrent worst splitting domains lb-rhs (depth):\n-0.35947 (10), -0.35085 (10), -0.34871 (10), -0.33884 (10), -0.31798 (10), -0.31492 (10), -0.31355 (10), -0.30912 (10), -0.29946 (10), -0.29199 (10), -0.28763 (10), -0.27890 (10), -0.26958 (10), -0.25749 (10), -0.25432 (10), -0.24742 (10), -0.24447 (10), -0.24396 (10), -0.23997 (10), -0.23954 (10), \nlength of domains: 116\nTotal time: 0.7478\t pickout: 0.0036\t decision: 0.0735\t get_bound: 0.6590\t add_domain: 0.0117\nAccumulated time:\t pickout: 0.0104\t decision: 0.2346\t get_bound: 3.3680\t add_domain: 0.0411\nCurrent (lb-rhs): -0.3594694137573242\n185 domains visited\nCumulative time: 3.6656510829925537\n\nbatch:  torch.Size([116, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([116, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] \n(232, 3, 32, 32) torch.Size([232, 1, 10]) torch.Size([232, 1])\npruning_in_iteration open status: False\nratio of positive domain = 39 / 232 = 0.1681034482758621\npruning-in-iteration extra time: 0.00016188621520996094\nTensors transferred: pre=21.8606M lA=10.9303M alpha=1.2041M beta=0.0024M\nThis batch time : update_bounds func: 0.6708\t prepare: 0.0082\t bound: 0.6225\t transfer: 0.0375\t finalize: 0.0024\nAccumulated time: update_bounds func: 4.0542\t prepare: 0.0376\t bound: 3.8820\t transfer: 0.1245\t finalize: 0.0093\nbatch bounding time:  0.6709649562835693\nCurrent worst splitting domains lb-rhs (depth):\n-0.33915 (11), -0.33049 (11), -0.32864 (11), -0.32760 (11), -0.32070 (11), -0.31964 (11), -0.31777 (11), -0.30984 (11), -0.29697 (11), -0.29359 (11), -0.29238 (11), -0.28872 (11), -0.28763 (11), -0.28690 (11), -0.28415 (11), -0.28057 (11), -0.27859 (11), -0.27083 (11), -0.27044 (11), -0.26671 (11), \nlength of domains: 193\nTotal time: 0.7622\t pickout: 0.0036\t decision: 0.0663\t get_bound: 0.6710\t add_domain: 0.0213\nAccumulated time:\t pickout: 0.0140\t decision: 0.3009\t get_bound: 4.0391\t add_domain: 0.0625\nCurrent (lb-rhs): -0.33915257453918457\n224 domains visited\nCumulative time: 4.429095268249512\n\nbatch:  torch.Size([193, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([193, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] \n(386, 3, 32, 32) torch.Size([386, 1, 10]) torch.Size([386, 1])\npruning_in_iteration open status: True\nratio of positive domain = 190 / 386 = 0.49222797927461137\npruning-in-iteration extra time: 0.027679920196533203\nTensors transferred: pre=36.3716M lA=9.2342M alpha=2.0033M beta=0.0044M\nThis batch time : update_bounds func: 0.8337\t prepare: 0.0139\t bound: 0.7543\t transfer: 0.0617\t finalize: 0.0036\nAccumulated time: update_bounds func: 4.8879\t prepare: 0.0515\t bound: 4.6363\t transfer: 0.1862\t finalize: 0.0129\nbatch bounding time:  0.8338184356689453\nCurrent worst splitting domains lb-rhs (depth):\n-0.31952 (12), -0.31131 (12), -0.31026 (12), -0.30937 (12), -0.30279 (12), -0.30224 (12), -0.29970 (12), -0.29263 (12), -0.27878 (12), -0.27646 (12), -0.27428 (12), -0.27156 (12), -0.27052 (12), -0.27051 (12), -0.26701 (12), -0.26425 (12), -0.25997 (12), -0.25235 (12), -0.25228 (12), -0.24906 (12), \nlength of domains: 196\nTotal time: 0.9763\t pickout: 0.0058\t decision: 0.1152\t get_bound: 0.8339\t add_domain: 0.0215\nAccumulated time:\t pickout: 0.0197\t decision: 0.4161\t get_bound: 4.8729\t add_domain: 0.0839\nCurrent (lb-rhs): -0.3195211887359619\n414 domains visited\nCumulative time: 5.406620502471924\n\nbatch:  torch.Size([196, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([196, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] \n(392, 3, 32, 32) torch.Size([392, 1, 10]) torch.Size([392, 1])\npruning_in_iteration open status: True\nratio of positive domain = 222 / 392 = 0.5663265306122449\npruning-in-iteration extra time: 0.02923274040222168\nTensors transferred: pre=36.9369M lA=8.0093M alpha=2.0344M beta=0.0049M\nThis batch time : update_bounds func: 0.7600\t prepare: 0.0135\t bound: 0.7015\t transfer: 0.0406\t finalize: 0.0041\nAccumulated time: update_bounds func: 5.6479\t prepare: 0.0649\t bound: 5.3378\t transfer: 0.2267\t finalize: 0.0170\nbatch bounding time:  0.7601184844970703\nCurrent worst splitting domains lb-rhs (depth):\n-0.30481 (13), -0.29672 (13), -0.29490 (13), -0.29435 (13), -0.28757 (13), -0.28673 (13), -0.28491 (13), -0.27730 (13), -0.26466 (13), -0.26204 (13), -0.26015 (13), -0.25702 (13), -0.25625 (13), -0.25574 (13), -0.25263 (13), -0.24975 (13), -0.24498 (13), -0.23744 (13), -0.23679 (13), -0.23403 (13), \nlength of domains: 170\nTotal time: 0.8876\t pickout: 0.0056\t decision: 0.1031\t get_bound: 0.7602\t add_domain: 0.0186\nAccumulated time:\t pickout: 0.0253\t decision: 0.5192\t get_bound: 5.6331\t add_domain: 0.1025\nCurrent (lb-rhs): -0.3048117160797119\n636 domains visited\nCumulative time: 6.295769691467285\n\nbatch:  torch.Size([170, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([170, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 241] [3, 1] \n(340, 3, 32, 32) torch.Size([340, 1, 10]) torch.Size([340, 1])\npruning_in_iteration open status: True\nratio of positive domain = 157 / 340 = 0.46176470588235297\npruning-in-iteration extra time: 0.030034303665161133\nTensors transferred: pre=32.0371M lA=8.6218M alpha=1.7646M beta=0.0049M\nThis batch time : update_bounds func: 0.8425\t prepare: 0.0118\t bound: 0.7548\t transfer: 0.0719\t finalize: 0.0037\nAccumulated time: update_bounds func: 6.4905\t prepare: 0.0767\t bound: 6.0926\t transfer: 0.2986\t finalize: 0.0207\nbatch bounding time:  0.8426311016082764\nCurrent worst splitting domains lb-rhs (depth):\n-0.29670 (14), -0.28881 (14), -0.28705 (14), -0.28641 (14), -0.27990 (14), -0.27885 (14), -0.27709 (14), -0.26964 (14), -0.25611 (14), -0.25369 (14), -0.25189 (14), -0.24857 (14), -0.24808 (14), -0.24733 (14), -0.24442 (14), -0.24156 (14), -0.23736 (14), -0.22968 (14), -0.22925 (14), -0.22633 (14), \nlength of domains: 183\nTotal time: 0.9611\t pickout: 0.0048\t decision: 0.0924\t get_bound: 0.8427\t add_domain: 0.0212\nAccumulated time:\t pickout: 0.0302\t decision: 0.6116\t get_bound: 6.4758\t add_domain: 0.1237\nCurrent (lb-rhs): -0.29670190811157227\n793 domains visited\nCumulative time: 7.258605241775513\n\nbatch:  torch.Size([183, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([183, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 241] [3, 241] [3, 241] [3, 241] [3, 241] [3, 241] [3, 1] [3, 241] [3, 1] [3, 1] \n(366, 3, 32, 32) torch.Size([366, 1, 10]) torch.Size([366, 1])\npruning_in_iteration open status: True\nratio of positive domain = 183 / 366 = 0.5\npruning-in-iteration extra time: 0.031081438064575195\nTensors transferred: pre=34.4870M lA=8.6689M alpha=1.8995M beta=0.0056M\nThis batch time : update_bounds func: 0.8291\t prepare: 0.0162\t bound: 0.7647\t transfer: 0.0441\t finalize: 0.0038\nAccumulated time: update_bounds func: 7.3195\t prepare: 0.0929\t bound: 6.8573\t transfer: 0.3427\t finalize: 0.0245\nbatch bounding time:  0.8291771411895752\nCurrent worst splitting domains lb-rhs (depth):\n-0.28924 (15), -0.28126 (15), -0.27959 (15), -0.27886 (15), -0.27232 (15), -0.27128 (15), -0.26941 (15), -0.26195 (15), -0.24885 (15), -0.24620 (15), -0.24452 (15), -0.24128 (15), -0.24049 (15), -0.23979 (15), -0.23700 (15), -0.23390 (15), -0.22978 (15), -0.22231 (15), -0.22158 (15), -0.21881 (15), \nlength of domains: 183\nTotal time: 0.9606\t pickout: 0.0054\t decision: 0.1041\t get_bound: 0.8292\t add_domain: 0.0218\nAccumulated time:\t pickout: 0.0356\t decision: 0.7157\t get_bound: 7.3051\t add_domain: 0.1455\nCurrent (lb-rhs): -0.2892448902130127\n976 domains visited\nCumulative time: 8.221460342407227\n\nbatch:  torch.Size([183, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([183, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [2, 1206] [3, 126] [3, 126] [3, 241] [3, 241] [3, 1] [3, 1] [3, 126] [3, 126] \n(366, 3, 32, 32) torch.Size([366, 1, 10]) torch.Size([366, 1])\npruning_in_iteration open status: True\nratio of positive domain = 100 / 366 = 0.27322404371584696\npruning-in-iteration extra time: 0.03276467323303223\nTensors transferred: pre=34.4870M lA=12.5322M alpha=1.8995M beta=0.0063M\nThis batch time : update_bounds func: 0.9497\t prepare: 0.0192\t bound: 0.8813\t transfer: 0.0447\t finalize: 0.0039\nAccumulated time: update_bounds func: 8.2693\t prepare: 0.1121\t bound: 7.7386\t transfer: 0.3875\t finalize: 0.0284\nbatch bounding time:  0.9498889446258545\nCurrent worst splitting domains lb-rhs (depth):\n-0.28220 (16), -0.28135 (16), -0.27413 (16), -0.27373 (16), -0.27290 (16), -0.27183 (16), -0.27069 (16), -0.26855 (16), -0.26549 (16), -0.26459 (16), -0.26268 (16), -0.26226 (16), -0.26180 (16), -0.26055 (16), -0.25507 (16), -0.25287 (16), -0.24245 (16), -0.24022 (16), -0.23833 (16), -0.23811 (16), \nlength of domains: 266\nTotal time: 1.1135\t pickout: 0.0054\t decision: 0.1204\t get_bound: 0.9500\t add_domain: 0.0377\nAccumulated time:\t pickout: 0.0410\t decision: 0.8361\t get_bound: 8.2551\t add_domain: 0.1832\nCurrent (lb-rhs): -0.2821979522705078\n1076 domains visited\nCumulative time: 9.336992979049683\n\nbatch:  torch.Size([266, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([266, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [2, 3243] [2, 7594] [2, 7054] [3, 1] [2, 2190] [2, 6307] [3, 56] [2, 2190] [2, 1206] \n(532, 3, 32, 32) torch.Size([532, 1, 10]) torch.Size([532, 1])\npruning_in_iteration open status: False\nratio of positive domain = 85 / 532 = 0.15977443609022557\npruning-in-iteration extra time: 0.00016021728515625\nTensors transferred: pre=50.1287M lA=25.0643M alpha=2.7610M beta=0.0096M\nThis batch time : update_bounds func: 1.3714\t prepare: 0.0198\t bound: 1.2426\t transfer: 0.1034\t finalize: 0.0051\nAccumulated time: update_bounds func: 9.6407\t prepare: 0.1319\t bound: 8.9812\t transfer: 0.4908\t finalize: 0.0335\nbatch bounding time:  1.3715717792510986\nCurrent worst splitting domains lb-rhs (depth):\n-0.27554 (17), -0.27521 (17), -0.27478 (17), -0.27388 (17), -0.26763 (17), -0.26734 (17), -0.26732 (17), -0.26679 (17), -0.26653 (17), -0.26589 (17), -0.26547 (17), -0.26499 (17), -0.26431 (17), -0.26397 (17), -0.26380 (17), -0.25926 (17), -0.25827 (17), -0.25810 (17), -0.25799 (17), -0.25676 (17), \nlength of domains: 447\nTotal time: 1.5947\t pickout: 0.0086\t decision: 0.1676\t get_bound: 1.3716\t add_domain: 0.0468\nAccumulated time:\t pickout: 0.0496\t decision: 1.0037\t get_bound: 9.6267\t add_domain: 0.2300\nCurrent (lb-rhs): -0.27553749084472656\n1161 domains visited\nCumulative time: 10.933241605758667\n\nbatch:  torch.Size([447, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([447, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 6307] [2, 2190] [2, 1206] [3, 126] [3, 56] [3, 230] [2, 6307] [3, 230] [2, 6307] \n(894, 3, 32, 32) torch.Size([894, 1, 10]) torch.Size([894, 1])\npruning_in_iteration open status: True\nratio of positive domain = 194 / 894 = 0.21700223713646527\npruning-in-iteration extra time: 0.021660566329956055\nTensors transferred: pre=84.2388M lA=32.9794M alpha=4.6398M beta=0.0179M\nThis batch time : update_bounds func: 2.4947\t prepare: 0.0567\t bound: 2.2083\t transfer: 0.2202\t finalize: 0.0088\nAccumulated time: update_bounds func: 12.1354\t prepare: 0.1886\t bound: 11.1895\t transfer: 0.7110\t finalize: 0.0424\nbatch bounding time:  2.494983196258545\nCurrent worst splitting domains lb-rhs (depth):\n-0.26964 (18), -0.26939 (18), -0.26930 (18), -0.26887 (18), -0.26860 (18), -0.26831 (18), -0.26787 (18), -0.26185 (18), -0.26156 (18), -0.26148 (18), -0.26128 (18), -0.26094 (18), -0.26049 (18), -0.26047 (18), -0.26028 (18), -0.26027 (18), -0.26019 (18), -0.25964 (18), -0.25964 (18), -0.25960 (18), \nlength of domains: 700\nTotal time: 3.0345\t pickout: 0.0124\t decision: 0.4258\t get_bound: 2.4951\t add_domain: 0.1012\nAccumulated time:\t pickout: 0.0620\t decision: 1.4296\t get_bound: 12.1218\t add_domain: 0.3312\nCurrent (lb-rhs): -0.2696399688720703\n1355 domains visited\nCumulative time: 13.969953060150146\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6307] [3, 56] [2, 6307] [3, 56] [2, 1206] [3, 56] [2, 1206] [3, 56] [3, 126] [3, 56] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 326 / 1024 = 0.318359375\npruning-in-iteration extra time: 0.030999422073364258\nTensors transferred: pre=96.4883M lA=32.8852M alpha=5.3145M beta=0.0215M\nThis batch time : update_bounds func: 2.6751\t prepare: 0.0365\t bound: 2.5028\t transfer: 0.1247\t finalize: 0.0101\nAccumulated time: update_bounds func: 14.8105\t prepare: 0.2251\t bound: 13.6924\t transfer: 0.8357\t finalize: 0.0525\nbatch bounding time:  2.6753017902374268\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26504 (19), -0.26475 (19), -0.26431 (19), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25834 (18), -0.25742 (19), -0.25731 (18), -0.25717 (19), -0.25709 (19), -0.25661 (19), -0.25660 (19), -0.25602 (19), -0.25579 (19), -0.25579 (19), \nlength of domains: 886\nTotal time: 4.8029\t pickout: 0.0144\t decision: 2.0103\t get_bound: 2.6754\t add_domain: 0.1027\nAccumulated time:\t pickout: 0.0764\t decision: 3.4399\t get_bound: 14.7972\t add_domain: 0.4339\nCurrent (lb-rhs): -0.2693941593170166\n1681 domains visited\nCumulative time: 18.77699375152588\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 230] [2, 1963] [2, 5139] [3, 230] [3, 230] [3, 56] [3, 126] [2, 2190] [3, 56] [2, 6307] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 363 / 1024 = 0.3544921875\npruning-in-iteration extra time: 0.037972450256347656\nTensors transferred: pre=96.4883M lA=31.1420M alpha=5.3145M beta=0.0234M\nThis batch time : update_bounds func: 2.7023\t prepare: 0.0364\t bound: 2.5326\t transfer: 0.1223\t finalize: 0.0102\nAccumulated time: update_bounds func: 17.5128\t prepare: 0.2614\t bound: 16.2250\t transfer: 0.9581\t finalize: 0.0626\nbatch bounding time:  2.702501058578491\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25999 (20), -0.25984 (20), -0.25964 (18), -0.25964 (20), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), \nlength of domains: 1035\nTotal time: 4.0540\t pickout: 0.0144\t decision: 0.9862\t get_bound: 2.7026\t add_domain: 0.3508\nAccumulated time:\t pickout: 0.0908\t decision: 4.4261\t get_bound: 17.4997\t add_domain: 0.7847\nCurrent (lb-rhs): -0.2693941593170166\n2044 domains visited\nCumulative time: 22.83461022377014\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 4715] [2, 5518] [3, 230] [2, 5139] [3, 56] [2, 2732] [3, 230] [3, 230] [2, 2190] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 367 / 1024 = 0.3583984375\npruning-in-iteration extra time: 0.03323507308959961\nTensors transferred: pre=96.4883M lA=31.0006M alpha=5.3145M beta=0.0244M\nThis batch time : update_bounds func: 2.5361\t prepare: 0.0359\t bound: 2.3546\t transfer: 0.1333\t finalize: 0.0114\nAccumulated time: update_bounds func: 20.0488\t prepare: 0.2974\t bound: 18.5796\t transfer: 1.0914\t finalize: 0.0740\nbatch bounding time:  2.536302328109741\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25546 (21), -0.25525 (21), -0.25507 (18), -0.25496 (19), -0.25486 (21), -0.25362 (19), -0.25348 (18), \nlength of domains: 1180\nTotal time: 3.6591\t pickout: 0.0146\t decision: 1.0222\t get_bound: 2.5364\t add_domain: 0.0860\nAccumulated time:\t pickout: 0.1054\t decision: 5.4483\t get_bound: 20.0361\t add_domain: 0.8707\nCurrent (lb-rhs): -0.2693941593170166\n2411 domains visited\nCumulative time: 26.49699568748474\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 227] [2, 5518] [3, 126] [2, 1204] [2, 7475] [3, 20] [2, 6379] [3, 230] [2, 5965] [3, 227] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 288 / 1024 = 0.28125\npruning-in-iteration extra time: 0.0321650505065918\nTensors transferred: pre=96.4883M lA=34.7226M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 2.8550\t prepare: 0.0371\t bound: 2.5756\t transfer: 0.2305\t finalize: 0.0110\nAccumulated time: update_bounds func: 22.9039\t prepare: 0.3345\t bound: 21.1552\t transfer: 1.3219\t finalize: 0.0850\nbatch bounding time:  2.855269193649292\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 1404\nTotal time: 3.9829\t pickout: 0.0141\t decision: 0.9738\t get_bound: 2.8553\t add_domain: 0.1396\nAccumulated time:\t pickout: 0.1194\t decision: 6.4222\t get_bound: 22.8915\t add_domain: 1.0103\nCurrent (lb-rhs): -0.2693941593170166\n2699 domains visited\nCumulative time: 30.4845552444458\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 20] [2, 5518] [2, 1963] [3, 126] [2, 5518] [2, 1206] [3, 84] [3, 20] [3, 227] [2, 1963] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 259 / 1024 = 0.2529296875\npruning-in-iteration extra time: 0.03173637390136719\nTensors transferred: pre=96.4883M lA=36.0418M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 2.8879\t prepare: 0.0381\t bound: 2.6023\t transfer: 0.2362\t finalize: 0.0104\nAccumulated time: update_bounds func: 25.7917\t prepare: 0.3726\t bound: 23.7575\t transfer: 1.5581\t finalize: 0.0954\nbatch bounding time:  2.888164520263672\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 1657\nTotal time: 4.0948\t pickout: 0.0146\t decision: 1.0931\t get_bound: 2.8882\t add_domain: 0.0989\nAccumulated time:\t pickout: 0.1340\t decision: 7.5152\t get_bound: 25.7797\t add_domain: 1.1091\nCurrent (lb-rhs): -0.2693941593170166\n2958 domains visited\nCumulative time: 34.582818031311035\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 2454] [2, 4883] [3, 84] [2, 6307] [3, 20] [2, 7140] [3, 20] [2, 2190] [2, 5518] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 244 / 1024 = 0.23828125\npruning-in-iteration extra time: 0.028255224227905273\nTensors transferred: pre=96.4883M lA=36.7956M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 2.9202\t prepare: 0.0347\t bound: 2.7275\t transfer: 0.1463\t finalize: 0.0108\nAccumulated time: update_bounds func: 28.7119\t prepare: 0.4073\t bound: 26.4850\t transfer: 1.7044\t finalize: 0.1062\nbatch bounding time:  2.9205148220062256\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 1925\nTotal time: 4.0836\t pickout: 0.0143\t decision: 0.9847\t get_bound: 2.9206\t add_domain: 0.1640\nAccumulated time:\t pickout: 0.1483\t decision: 8.4999\t get_bound: 28.7003\t add_domain: 1.2731\nCurrent (lb-rhs): -0.2693941593170166\n3202 domains visited\nCumulative time: 38.670318365097046\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 5518] [2, 3931] [2, 7064] [3, 84] [2, 6379] [2, 7140] [2, 7054] [2, 5518] [2, 7140] [3, 20] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 265 / 1024 = 0.2587890625\npruning-in-iteration extra time: 0.029265403747558594\nTensors transferred: pre=96.4883M lA=35.7591M alpha=5.3145M beta=0.0273M\nThis batch time : update_bounds func: 2.8167\t prepare: 0.0353\t bound: 2.6254\t transfer: 0.1439\t finalize: 0.0112\nAccumulated time: update_bounds func: 31.5286\t prepare: 0.4426\t bound: 29.1104\t transfer: 1.8482\t finalize: 0.1173\nbatch bounding time:  2.8170197010040283\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 2172\nTotal time: 4.5129\t pickout: 0.0152\t decision: 1.0178\t get_bound: 2.8171\t add_domain: 0.6627\nAccumulated time:\t pickout: 0.1635\t decision: 9.5178\t get_bound: 31.5174\t add_domain: 1.9359\nCurrent (lb-rhs): -0.2693941593170166\n3467 domains visited\nCumulative time: 43.18689584732056\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 2454] [3, 227] [2, 2454] [3, 20] [2, 7054] [2, 2732] [2, 7064] [2, 2732] [2, 2732] [2, 7064] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 253 / 1024 = 0.2470703125\npruning-in-iteration extra time: 0.02989935874938965\nTensors transferred: pre=96.4883M lA=36.3244M alpha=5.3145M beta=0.0283M\nThis batch time : update_bounds func: 2.8386\t prepare: 0.0357\t bound: 2.6369\t transfer: 0.1160\t finalize: 0.0114\nAccumulated time: update_bounds func: 34.3672\t prepare: 0.4783\t bound: 31.7473\t transfer: 1.9642\t finalize: 0.1287\nbatch bounding time:  2.8388512134552\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 2431\nTotal time: 3.9349\t pickout: 0.0143\t decision: 0.9736\t get_bound: 2.8389\t add_domain: 0.1081\nAccumulated time:\t pickout: 0.1778\t decision: 10.4914\t get_bound: 34.3564\t add_domain: 2.0440\nCurrent (lb-rhs): -0.2693941593170166\n3720 domains visited\nCumulative time: 47.125367403030396\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 7054] [2, 2454] [2, 6379] [3, 20] [2, 2454] [2, 2454] [2, 5654] [2, 7054] [2, 4883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 297 / 1024 = 0.2900390625\npruning-in-iteration extra time: 0.03149890899658203\nTensors transferred: pre=96.4883M lA=34.2515M alpha=5.3145M beta=0.0293M\nThis batch time : update_bounds func: 2.6565\t prepare: 0.0362\t bound: 2.4936\t transfer: 0.1147\t finalize: 0.0110\nAccumulated time: update_bounds func: 37.0237\t prepare: 0.5145\t bound: 34.2409\t transfer: 2.0789\t finalize: 0.1397\nbatch bounding time:  2.656770706176758\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 2646\nTotal time: 3.7527\t pickout: 0.0154\t decision: 0.9787\t get_bound: 2.6569\t add_domain: 0.1017\nAccumulated time:\t pickout: 0.1932\t decision: 11.4701\t get_bound: 37.0132\t add_domain: 2.1457\nCurrent (lb-rhs): -0.2693941593170166\n4017 domains visited\nCumulative time: 50.88145470619202\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 500] [2, 2454] [2, 1961] [2, 1961] [2, 5654] [3, 84] [2, 1961] [2, 1961] [2, 2454] [3, 84] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 316 / 1024 = 0.30859375\npruning-in-iteration extra time: 0.03414297103881836\nTensors transferred: pre=96.4883M lA=33.3563M alpha=5.3145M beta=0.0312M\nThis batch time : update_bounds func: 2.6699\t prepare: 0.0377\t bound: 2.4745\t transfer: 0.1463\t finalize: 0.0106\nAccumulated time: update_bounds func: 39.6937\t prepare: 0.5521\t bound: 36.7154\t transfer: 2.2252\t finalize: 0.1503\nbatch bounding time:  2.670149087905884\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 2842\nTotal time: 3.8265\t pickout: 0.0150\t decision: 1.0351\t get_bound: 2.6702\t add_domain: 0.1061\nAccumulated time:\t pickout: 0.2082\t decision: 12.5053\t get_bound: 39.6834\t add_domain: 2.2518\nCurrent (lb-rhs): -0.2693941593170166\n4333 domains visited\nCumulative time: 54.7113311290741\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 7140] [2, 4883] [2, 4883] [2, 5654] [2, 6307] [3, 227] [2, 5518] [2, 2190] [3, 84] [2, 2190] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 333 / 1024 = 0.3251953125\npruning-in-iteration extra time: 0.031356096267700195\nTensors transferred: pre=96.4883M lA=32.5554M alpha=5.3145M beta=0.0332M\nThis batch time : update_bounds func: 2.6646\t prepare: 0.0376\t bound: 2.4687\t transfer: 0.1460\t finalize: 0.0112\nAccumulated time: update_bounds func: 42.3582\t prepare: 0.5897\t bound: 39.1841\t transfer: 2.3712\t finalize: 0.1615\nbatch bounding time:  2.664797067642212\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 3021\nTotal time: 3.8050\t pickout: 0.0146\t decision: 1.0265\t get_bound: 2.6649\t add_domain: 0.0990\nAccumulated time:\t pickout: 0.2228\t decision: 13.5317\t get_bound: 42.3483\t add_domain: 2.3508\nCurrent (lb-rhs): -0.2693941593170166\n4666 domains visited\nCumulative time: 58.521220207214355\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 5139] [2, 2772] [2, 5139] [2, 2731] [3, 104] [2, 7054] [2, 5139] [2, 2731] [2, 2731] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 320 / 1024 = 0.3125\npruning-in-iteration extra time: 0.04499554634094238\nTensors transferred: pre=96.4883M lA=33.1678M alpha=5.3145M beta=0.0352M\nThis batch time : update_bounds func: 2.5746\t prepare: 0.0374\t bound: 2.3874\t transfer: 0.1376\t finalize: 0.0112\nAccumulated time: update_bounds func: 44.9328\t prepare: 0.6271\t bound: 41.5716\t transfer: 2.5088\t finalize: 0.1728\nbatch bounding time:  2.5748050212860107\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 3213\nTotal time: 3.6581\t pickout: 0.0144\t decision: 0.9680\t get_bound: 2.5749\t add_domain: 0.1007\nAccumulated time:\t pickout: 0.2372\t decision: 14.4998\t get_bound: 44.9232\t add_domain: 2.4515\nCurrent (lb-rhs): -0.2693941593170166\n4986 domains visited\nCumulative time: 62.182955741882324\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6330] [2, 6330] [2, 7054] [2, 7140] [3, 84] [2, 7054] [2, 5139] [3, 84] [2, 5139] [3, 227] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 313 / 1024 = 0.3056640625\npruning-in-iteration extra time: 0.035608530044555664\nTensors transferred: pre=96.4883M lA=33.4976M alpha=5.3145M beta=0.0361M\nThis batch time : update_bounds func: 2.7004\t prepare: 0.0365\t bound: 2.5196\t transfer: 0.1306\t finalize: 0.0128\nAccumulated time: update_bounds func: 47.6332\t prepare: 0.6637\t bound: 44.0912\t transfer: 2.6393\t finalize: 0.1856\nbatch bounding time:  2.700627565383911\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 3412\nTotal time: 3.7928\t pickout: 0.0142\t decision: 0.9642\t get_bound: 2.7007\t add_domain: 0.1137\nAccumulated time:\t pickout: 0.2515\t decision: 15.4639\t get_bound: 47.6239\t add_domain: 2.5652\nCurrent (lb-rhs): -0.2693941593170166\n5299 domains visited\nCumulative time: 65.97966408729553\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6330] [3, 104] [2, 2454] [2, 7054] [3, 84] [2, 6330] [2, 6330] [3, 84] [2, 6330] [2, 4883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 257 / 1024 = 0.2509765625\npruning-in-iteration extra time: 0.0332183837890625\nTensors transferred: pre=96.4883M lA=36.1360M alpha=5.3145M beta=0.0371M\nThis batch time : update_bounds func: 2.7731\t prepare: 0.0376\t bound: 2.5748\t transfer: 0.1478\t finalize: 0.0117\nAccumulated time: update_bounds func: 50.4063\t prepare: 0.7013\t bound: 46.6660\t transfer: 2.7871\t finalize: 0.1973\nbatch bounding time:  2.7733490467071533\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 3667\nTotal time: 3.9062\t pickout: 0.0157\t decision: 1.0061\t get_bound: 2.7734\t add_domain: 0.1109\nAccumulated time:\t pickout: 0.2672\t decision: 16.4701\t get_bound: 50.3973\t add_domain: 2.6761\nCurrent (lb-rhs): -0.2693941593170166\n5556 domains visited\nCumulative time: 69.88979291915894\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 3889] [2, 7064] [2, 1961] [2, 5518] [2, 7064] [2, 7534] [3, 219] [2, 7064] [2, 4715] [3, 227] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 175 / 1024 = 0.1708984375\npruning-in-iteration extra time: 0.0003247261047363281\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0381M\nThis batch time : update_bounds func: 3.0341\t prepare: 0.0364\t bound: 2.8083\t transfer: 0.1767\t finalize: 0.0118\nAccumulated time: update_bounds func: 53.4404\t prepare: 0.7377\t bound: 49.4743\t transfer: 2.9638\t finalize: 0.2091\nbatch bounding time:  3.0344860553741455\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 4004\nTotal time: 4.1603\t pickout: 0.0144\t decision: 0.9712\t get_bound: 3.0346\t add_domain: 0.1401\nAccumulated time:\t pickout: 0.2815\t decision: 17.4412\t get_bound: 53.4319\t add_domain: 2.8163\nCurrent (lb-rhs): -0.2693941593170166\n5731 domains visited\nCumulative time: 74.05399823188782\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 7064] [3, 84] [2, 7054] [3, 73] [2, 1250] [2, 4715] [2, 4715] [2, 5518] [2, 3931] [2, 7054] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 206 / 1024 = 0.201171875\npruning-in-iteration extra time: 0.015183687210083008\nTensors transferred: pre=96.4883M lA=38.5388M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.8389\t prepare: 0.0373\t bound: 2.6706\t transfer: 0.1167\t finalize: 0.0133\nAccumulated time: update_bounds func: 56.2793\t prepare: 0.7750\t bound: 52.1449\t transfer: 3.0805\t finalize: 0.2225\nbatch bounding time:  2.8391666412353516\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 4310\nTotal time: 5.5654\t pickout: 0.0143\t decision: 1.0881\t get_bound: 2.8392\t add_domain: 1.6237\nAccumulated time:\t pickout: 0.2959\t decision: 18.5294\t get_bound: 56.2711\t add_domain: 4.4400\nCurrent (lb-rhs): -0.2693941593170166\n5937 domains visited\nCumulative time: 79.62446165084839\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4715] [2, 4715] [2, 4715] [2, 4715] [3, 219] [3, 219] [3, 104] [2, 7054] [2, 6330] [3, 104] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 243 / 1024 = 0.2373046875\npruning-in-iteration extra time: 0.04151320457458496\nTensors transferred: pre=96.4883M lA=36.7956M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.8882\t prepare: 0.0469\t bound: 2.6928\t transfer: 0.1327\t finalize: 0.0145\nAccumulated time: update_bounds func: 59.1675\t prepare: 0.8219\t bound: 54.8377\t transfer: 3.2132\t finalize: 0.2370\nbatch bounding time:  2.888576030731201\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 4579\nTotal time: 4.0514\t pickout: 0.0155\t decision: 1.0180\t get_bound: 2.8887\t add_domain: 0.1292\nAccumulated time:\t pickout: 0.3114\t decision: 19.5473\t get_bound: 59.1598\t add_domain: 4.5692\nCurrent (lb-rhs): -0.2693941593170166\n6180 domains visited\nCumulative time: 83.68045711517334\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 3889] [2, 3870] [2, 5518] [2, 3889] [2, 3870] [3, 104] [2, 3870] [3, 84] [2, 7054] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 229 / 1024 = 0.2236328125\npruning-in-iteration extra time: 0.044011831283569336\nTensors transferred: pre=96.4883M lA=37.4552M alpha=5.3145M beta=0.0381M\nThis batch time : update_bounds func: 3.0775\t prepare: 0.0444\t bound: 2.8212\t transfer: 0.1951\t finalize: 0.0159\nAccumulated time: update_bounds func: 62.2450\t prepare: 0.8663\t bound: 57.6589\t transfer: 3.4083\t finalize: 0.2529\nbatch bounding time:  3.0778615474700928\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 4862\nTotal time: 4.2927\t pickout: 0.0153\t decision: 1.0396\t get_bound: 3.0780\t add_domain: 0.1598\nAccumulated time:\t pickout: 0.3266\t decision: 20.5870\t get_bound: 62.2378\t add_domain: 4.7290\nCurrent (lb-rhs): -0.2693941593170166\n6409 domains visited\nCumulative time: 87.97787189483643\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 5518] [3, 84] [2, 5518] [2, 3889] [2, 3889] [2, 7064] [2, 3931] [2, 3931] [3, 84] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 229 / 1024 = 0.2236328125\npruning-in-iteration extra time: 0.05590939521789551\nTensors transferred: pre=96.4883M lA=37.4552M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 3.2487\t prepare: 0.0647\t bound: 2.9199\t transfer: 0.2414\t finalize: 0.0215\nAccumulated time: update_bounds func: 65.4937\t prepare: 0.9310\t bound: 60.5788\t transfer: 3.6496\t finalize: 0.2744\nbatch bounding time:  3.249110221862793\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 5145\nTotal time: 4.5071\t pickout: 0.0169\t decision: 1.0718\t get_bound: 3.2492\t add_domain: 0.1692\nAccumulated time:\t pickout: 0.3435\t decision: 21.6588\t get_bound: 65.4870\t add_domain: 4.8982\nCurrent (lb-rhs): -0.2693941593170166\n6638 domains visited\nCumulative time: 92.4927167892456\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 3931] [3, 219] [2, 7064] [2, 3931] [2, 7064] [3, 73] [3, 84] [2, 5518] [2, 5518] [2, 6330] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 155 / 1024 = 0.1513671875\npruning-in-iteration extra time: 0.0001704692840576172\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.9311\t prepare: 0.0354\t bound: 2.7205\t transfer: 0.1640\t finalize: 0.0103\nAccumulated time: update_bounds func: 68.4248\t prepare: 0.9664\t bound: 63.2992\t transfer: 3.8137\t finalize: 0.2847\nbatch bounding time:  2.931530475616455\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 5502\nTotal time: 4.1392\t pickout: 0.0182\t decision: 1.0488\t get_bound: 2.9316\t add_domain: 0.1406\nAccumulated time:\t pickout: 0.3617\t decision: 22.7076\t get_bound: 68.4186\t add_domain: 5.0387\nCurrent (lb-rhs): -0.2693941593170166\n6793 domains visited\nCumulative time: 96.6359543800354\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 73] [2, 3889] [2, 3889] [2, 4702] [2, 1963] [2, 1963] [2, 4702] [2, 7064] [2, 1963] [2, 1963] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 172 / 1024 = 0.16796875\npruning-in-iteration extra time: 0.0001876354217529297\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 2.9991\t prepare: 0.0351\t bound: 2.7983\t transfer: 0.1543\t finalize: 0.0105\nAccumulated time: update_bounds func: 71.4239\t prepare: 1.0015\t bound: 66.0975\t transfer: 3.9680\t finalize: 0.2952\nbatch bounding time:  2.9995169639587402\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 5842\nTotal time: 4.1375\t pickout: 0.0153\t decision: 0.9866\t get_bound: 2.9996\t add_domain: 0.1360\nAccumulated time:\t pickout: 0.3770\t decision: 23.6942\t get_bound: 71.4182\t add_domain: 5.1747\nCurrent (lb-rhs): -0.2693941593170166\n6965 domains visited\nCumulative time: 100.77711176872253\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 7786] [2, 1242] [2, 3417] [2, 3417] [2, 7786] [2, 1242] [2, 2732] [2, 4883] [2, 7786] [2, 2732] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 90 / 1024 = 0.087890625\npruning-in-iteration extra time: 0.00036787986755371094\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0410M\nThis batch time : update_bounds func: 3.0363\t prepare: 0.0368\t bound: 2.8381\t transfer: 0.1493\t finalize: 0.0113\nAccumulated time: update_bounds func: 74.4602\t prepare: 1.0383\t bound: 68.9356\t transfer: 4.1173\t finalize: 0.3065\nbatch bounding time:  3.036698579788208\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 6264\nTotal time: 4.2599\t pickout: 0.0144\t decision: 0.9817\t get_bound: 3.0368\t add_domain: 0.2271\nAccumulated time:\t pickout: 0.3914\t decision: 24.6758\t get_bound: 74.4550\t add_domain: 5.4019\nCurrent (lb-rhs): -0.2693941593170166\n7055 domains visited\nCumulative time: 105.04089331626892\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 4883] [2, 2732] [2, 1242] [2, 4883] [1, 2615] [2, 2732] [2, 1242] [2, 7054] [2, 1242] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 104 / 1024 = 0.1015625\npruning-in-iteration extra time: 0.0001811981201171875\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0420M\nThis batch time : update_bounds func: 2.9191\t prepare: 0.0389\t bound: 2.7181\t transfer: 0.1490\t finalize: 0.0122\nAccumulated time: update_bounds func: 77.3793\t prepare: 1.0773\t bound: 71.6537\t transfer: 4.2663\t finalize: 0.3187\nbatch bounding time:  2.9194581508636475\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 6672\nTotal time: 4.1174\t pickout: 0.0151\t decision: 1.0324\t get_bound: 2.9195\t add_domain: 0.1503\nAccumulated time:\t pickout: 0.4065\t decision: 25.7083\t get_bound: 77.3745\t add_domain: 5.5521\nCurrent (lb-rhs): -0.2693941593170166\n7159 domains visited\nCumulative time: 109.16176080703735\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1965] [2, 6033] [2, 3425] [2, 500] [2, 6033] [2, 6033] [2, 6033] [2, 6033] [1, 2615] [2, 1965] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 57 / 1024 = 0.0556640625\npruning-in-iteration extra time: 0.0001709461212158203\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0430M\nThis batch time : update_bounds func: 2.9597\t prepare: 0.0359\t bound: 2.7525\t transfer: 0.1602\t finalize: 0.0103\nAccumulated time: update_bounds func: 80.3390\t prepare: 1.1131\t bound: 74.4062\t transfer: 4.4265\t finalize: 0.3290\nbatch bounding time:  2.96006441116333\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 7127\nTotal time: 4.1970\t pickout: 0.0153\t decision: 0.9862\t get_bound: 2.9601\t add_domain: 0.2354\nAccumulated time:\t pickout: 0.4218\t decision: 26.6945\t get_bound: 80.3347\t add_domain: 5.7875\nCurrent (lb-rhs): -0.2693941593170166\n7216 domains visited\nCumulative time: 113.36261248588562\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6033] [2, 1965] [2, 2732] [2, 6033] [2, 1965] [1, 2615] [2, 7786] [2, 1965] [2, 7786] [2, 7786] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 56 / 1024 = 0.0546875\npruning-in-iteration extra time: 0.00017762184143066406\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0430M\nThis batch time : update_bounds func: 2.9265\t prepare: 0.0363\t bound: 2.7245\t transfer: 0.1537\t finalize: 0.0109\nAccumulated time: update_bounds func: 83.2655\t prepare: 1.1494\t bound: 77.1307\t transfer: 4.5802\t finalize: 0.3398\nbatch bounding time:  2.9268789291381836\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 7583\nTotal time: 4.0777\t pickout: 0.0145\t decision: 0.9592\t get_bound: 2.9270\t add_domain: 0.1771\nAccumulated time:\t pickout: 0.4362\t decision: 27.6537\t get_bound: 83.2616\t add_domain: 5.9646\nCurrent (lb-rhs): -0.2693941593170166\n7272 domains visited\nCumulative time: 117.44399213790894\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6329] [2, 2300] [2, 1965] [1, 2615] [2, 1204] [2, 1965] [1, 2615] [2, 2300] [2, 2300] [2, 2300] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 82 / 1024 = 0.080078125\npruning-in-iteration extra time: 0.00017905235290527344\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0449M\nThis batch time : update_bounds func: 2.9874\t prepare: 0.0367\t bound: 2.7838\t transfer: 0.1553\t finalize: 0.0108\nAccumulated time: update_bounds func: 86.2528\t prepare: 1.1862\t bound: 79.9145\t transfer: 4.7356\t finalize: 0.3506\nbatch bounding time:  2.987820625305176\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26831 (18), -0.26399 (19), -0.26327 (19), -0.26019 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25731 (18), -0.25602 (19), -0.25579 (19), -0.25507 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25218 (19), \nlength of domains: 8013\nTotal time: 4.1561\t pickout: 0.0145\t decision: 0.9653\t get_bound: 2.9879\t add_domain: 0.1884\nAccumulated time:\t pickout: 0.4507\t decision: 28.6190\t get_bound: 86.2495\t add_domain: 6.1529\nCurrent (lb-rhs): -0.2693941593170166\n7354 domains visited\nCumulative time: 121.60456395149231\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1965] [1, 2615] [2, 424] [2, 7780] [3, 183] [2, 1376] [3, 183] [2, 1041] [2, 424] [2, 7780] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 277 / 1024 = 0.2705078125\npruning-in-iteration extra time: 0.03429913520812988\nTensors transferred: pre=96.4883M lA=35.1937M alpha=5.3145M beta=0.0459M\nThis batch time : update_bounds func: 3.1619\t prepare: 0.0546\t bound: 2.9238\t transfer: 0.1700\t finalize: 0.0120\nAccumulated time: update_bounds func: 89.4148\t prepare: 1.2407\t bound: 82.8383\t transfer: 4.9056\t finalize: 0.3625\nbatch bounding time:  3.1622202396392822\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_36_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "8.51320505142212",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpzl4624cf.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:13:40 2024 on Cerberus\nInternal results will be saved to /tmp/tmpzl4624cf.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-1.96812832, -6.49893856,  1.24639082,  1.69520974,  4.02628088,\n          2.75464153,  2.39966416,  3.83796740, -4.41950607, -2.72571588]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.01335669, -6.61650658,  1.13246620,  1.72124791,  3.89798760,\n           2.86626458,  2.36144567,  4.41441536, -4.41264629, -2.79052591],\n         [-2.01335669, -6.61650658,  1.13246620,  1.72124791,  3.89798760,\n           2.86626458,  2.36144567,  4.41441536, -4.41264629, -2.79052591]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.91134453, 10.51449394,  2.76552153,  2.17673969,  1.03172302,\n           1.53654194, -0.51642776,  8.31063366,  6.68851376]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 2.0133 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 5.829620122909546\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_44_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.896880865097046",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpeab_fg_n.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:13:49 2024 on Cerberus\nInternal results will be saved to /tmp/tmpeab_fg_n.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 8.53005219,  2.65516281,  3.62637854, -1.70141554,  3.90650630,\n         -4.50923967, -6.23203850, -2.73792219,  3.00681901,  4.35398483]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 8.30019855,  3.03155708,  3.46851993, -1.78878641,  3.78387809,\n          -4.47537851, -6.29589128, -2.85683608,  2.87485886,  4.81936932],\n         [ 8.30019855,  3.03155708,  3.46851993, -1.78878641,  3.78387809,\n          -4.47537851, -6.29589128, -2.85683608,  2.87485886,  4.81936932]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.26864147,  4.83167839, 10.08898544,  4.51632023, 12.77557755,\n          14.59609032, 11.15703487,  5.42533970,  3.48082924]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6417 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 8.53005219,  2.65516281,  3.62637854, -1.70141554,  3.90650630,\n         -4.50923967, -6.23203850, -2.73792219,  3.00681901,  4.35398483]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [986]; unstable size 986; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 433 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 406 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [433]; unstable size 433; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 406 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [406]; unstable size 406; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [28]; unstable size 28; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 4.16252184,  3.01627731,  8.77518272,  3.12779641, 11.22480965,\n         13.38881111,  9.78200912,  3.72122002,  2.91803265]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.258423566818237\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_55_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.726408958435059",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpb_dhpp5_.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:13:57 2024 on Cerberus\nInternal results will be saved to /tmp/tmpb_dhpp5_.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 4.24215508, -0.69684935,  0.55492795, -0.75052458,  0.43542138,\n         -1.24407911, -1.12097180, -2.80693984,  7.00109816,  0.78772008]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 4.52412033, -0.46530545,  0.71960282, -0.91084248,  0.43153140,\n          -1.32478404, -1.19332063, -2.72541595,  6.33062458,  0.94560248],\n         [ 4.52412033, -0.46530545,  0.71960282, -0.91084248,  0.43153140,\n          -1.32478404, -1.19332063, -2.72541595,  6.33062458,  0.94560248]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.80650425, 6.79592991, 5.61102200, 7.24146700, 5.89909315,\n          7.65540886, 7.52394533, 9.05604076, 5.38502216]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6045 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 4.24215508, -0.69684935,  0.55492795, -0.75052458,  0.43542138,\n         -1.24407911, -1.12097180, -2.80693984,  7.00109816,  0.78772008]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [958]; unstable size 958; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 489 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 415 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [489]; unstable size 489; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 415 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [415]; unstable size 415; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[0.48258674, 5.41662693, 3.66289854, 5.48767662, 3.82613802, 5.83310986,\n         5.75515890, 7.10577965, 3.81086421]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.168102502822876\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_71_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "16.17529559135437",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp7x3wriqu.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:14:06 2024 on Cerberus\nInternal results will be saved to /tmp/tmp7x3wriqu.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n          1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-1.91160369, -1.75271857,  0.69264895,  2.51886392,  1.29905379,\n           1.52446699,  3.16162252,  0.31408176, -3.63012004, -0.84955752],\n         [-1.91160369, -1.75271857,  0.69264895,  2.51886392,  1.29905379,\n           1.52446699,  3.16162252,  0.31408176, -3.63012004, -0.84955752]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.07322598, 4.91434097, 2.46897364, 0.64275861, 1.86256874,\n          1.63715553, 2.84754086, 6.79174232, 4.01117992]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5672 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n          1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1915]; unstable size 1915; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 111 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [772]; unstable size 772; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 111 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [630]; unstable size 630; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [67]; unstable size 67; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.80871940,  2.45961452,  0.37849426, -0.61176658,  0.52175808,\n          0.03984690,  0.67118979,  4.57306957,  1.52069569]], device='cuda:0') None\nbest_l after optimization: 14.235376358032227 with beta sum per layer: []\nalpha/beta optimization time: 5.7829461097717285\ninitial alpha-CROWN bounds: tensor([[ 3.03656006,  2.69559622,  0.59893227, -0.46514201,  0.67543983,\n          0.21066689,  0.89655733,  4.81512594,  1.77164030]], device='cuda:0')\nWorst class: (+ rhs) -0.46514201164245605\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 289.04699087142944\n##### Instance 0 first 10 spec matrices: [[[-1.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.03656005859375.\n\nProperties batch 1, size 1\nRemaining timeout: 289.00973105430603\n##### Instance 0 first 10 spec matrices: [[[ 0. -1.  0.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.695596218109131.\n\nProperties batch 2, size 1\nRemaining timeout: 288.981662273407\n##### Instance 0 first 10 spec matrices: [[[ 0.  0. -1.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.5989322662353516.\n\nProperties batch 3, size 1\nRemaining timeout: 288.95491123199463\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0. -1.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.46514201]], device='cuda:0') need to verify.\nModel prediction is: tensor([-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n         1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.46514201]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1915\nlayer 1 size torch.Size([8192]) unstable 749\nlayer 2 size torch.Size([8192]) unstable 614\nlayer 3 size torch.Size([250]) unstable 63\n-----------------\n# of unstable neurons: 3341\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 189] \nsplit level 1: [3, 236] \nsplit level 2: [3, 76] \nsplit level 3: [3, 238] \nsplit level 4: [3, 44] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 7 / 32 = 0.21875\npruning-in-iteration extra time: 0.01580071449279785\nTensors transferred: pre=3.0153M lA=1.1778M alpha=0.2065M beta=0.0002M\nThis batch time : update_bounds func: 0.6973\t prepare: 0.0026\t bound: 0.6877\t transfer: 0.0064\t finalize: 0.0006\nAccumulated time: update_bounds func: 0.6973\t prepare: 0.0026\t bound: 0.6877\t transfer: 0.0064\t finalize: 0.0006\nbatch bounding time:  0.6974303722381592\nCurrent worst splitting domains lb-rhs (depth):\n-0.14887 (5), -0.14311 (5), -0.13851 (5), -0.13621 (5), -0.10090 (5), -0.09864 (5), -0.09558 (5), -0.09537 (5), -0.09237 (5), -0.09080 (5), -0.08998 (5), -0.08598 (5), -0.06068 (5), -0.05457 (5), -0.05413 (5), -0.05390 (5), -0.05302 (5), -0.04999 (5), -0.04704 (5), -0.04551 (5), \nlength of domains: 25\nTotal time: 0.9324\t pickout: 0.0008\t decision: 0.2245\t get_bound: 0.7014\t add_domain: 0.0056\nAccumulated time:\t pickout: 0.0008\t decision: 0.2245\t get_bound: 0.7014\t add_domain: 0.0056\nCurrent (lb-rhs): -0.14886975288391113\n7 domains visited\nCumulative time: 1.0876622200012207\n\nbatch:  torch.Size([25, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([25, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] \n(50, 3, 32, 32) torch.Size([50, 1, 10]) torch.Size([50, 1])\npruning_in_iteration open status: False\nratio of positive domain = 10 / 50 = 0.19999999999999996\npruning-in-iteration extra time: 0.0001430511474609375\nTensors transferred: pre=4.7113M lA=2.3557M alpha=0.3227M beta=0.0003M\nThis batch time : update_bounds func: 0.4550\t prepare: 0.0032\t bound: 0.4427\t transfer: 0.0085\t finalize: 0.0006\nAccumulated time: update_bounds func: 1.1523\t prepare: 0.0058\t bound: 1.1303\t transfer: 0.0149\t finalize: 0.0011\nbatch bounding time:  0.45508265495300293\nCurrent worst splitting domains lb-rhs (depth):\n-0.11342 (6), -0.10721 (6), -0.10490 (6), -0.10249 (6), -0.10028 (6), -0.09935 (6), -0.09675 (6), -0.09581 (6), -0.06432 (6), -0.06324 (6), -0.06069 (6), -0.05826 (6), -0.05811 (6), -0.05549 (6), -0.05466 (6), -0.05458 (6), -0.05427 (6), -0.05200 (6), -0.05098 (6), -0.05045 (6), \nlength of domains: 40\nTotal time: 0.4874\t pickout: 0.0012\t decision: 0.0266\t get_bound: 0.4551\t add_domain: 0.0044\nAccumulated time:\t pickout: 0.0021\t decision: 0.2511\t get_bound: 1.1565\t add_domain: 0.0100\nCurrent (lb-rhs): -0.11342024803161621\n17 domains visited\nCumulative time: 1.575333595275879\n\nbatch:  torch.Size([40, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([40, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 86] [3, 119] [3, 119] [3, 119] [3, 86] [3, 86] [3, 119] [3, 119] [3, 86] [3, 119] \n(80, 3, 32, 32) torch.Size([80, 1, 10]) torch.Size([80, 1])\npruning_in_iteration open status: True\nratio of positive domain = 40 / 80 = 0.5\npruning-in-iteration extra time: 0.026683330535888672\nTensors transferred: pre=7.5381M lA=1.8845M alpha=0.5164M beta=0.0005M\nThis batch time : update_bounds func: 0.5292\t prepare: 0.0033\t bound: 0.5166\t transfer: 0.0084\t finalize: 0.0008\nAccumulated time: update_bounds func: 1.6815\t prepare: 0.0091\t bound: 1.6469\t transfer: 0.0233\t finalize: 0.0020\nbatch bounding time:  0.5292305946350098\nCurrent worst splitting domains lb-rhs (depth):\n-0.07957 (7), -0.07407 (7), -0.07299 (7), -0.07111 (7), -0.06887 (7), -0.06856 (7), -0.06591 (7), -0.06535 (7), -0.06301 (7), -0.06225 (7), -0.06195 (7), -0.03045 (7), -0.02706 (7), -0.02641 (7), -0.02465 (7), -0.02439 (7), -0.02411 (7), -0.02390 (7), -0.02070 (7), -0.01987 (7), \nlength of domains: 40\nTotal time: 0.5671\t pickout: 0.0015\t decision: 0.0312\t get_bound: 0.5293\t add_domain: 0.0051\nAccumulated time:\t pickout: 0.0036\t decision: 0.2824\t get_bound: 1.6858\t add_domain: 0.0152\nCurrent (lb-rhs): -0.07957053184509277\n57 domains visited\nCumulative time: 2.142810106277466\n\nbatch:  torch.Size([40, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([40, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 119] [3, 86] [3, 86] [3, 86] [3, 119] [3, 86] [3, 86] [3, 86] [3, 86] [3, 119] \n(80, 3, 32, 32) torch.Size([80, 1, 10]) torch.Size([80, 1])\npruning_in_iteration open status: True\nratio of positive domain = 64 / 80 = 0.8\npruning-in-iteration extra time: 0.02738809585571289\nTensors transferred: pre=7.5381M lA=0.7538M alpha=0.5164M beta=0.0006M\nThis batch time : update_bounds func: 0.5337\t prepare: 0.0034\t bound: 0.5238\t transfer: 0.0053\t finalize: 0.0011\nAccumulated time: update_bounds func: 2.2152\t prepare: 0.0125\t bound: 2.1707\t transfer: 0.0287\t finalize: 0.0031\nbatch bounding time:  0.5337581634521484\nCurrent worst splitting domains lb-rhs (depth):\n-0.04667 (8), -0.04101 (8), -0.03977 (8), -0.03795 (8), -0.03642 (8), -0.03548 (8), -0.03272 (8), -0.03257 (8), -0.03241 (8), -0.03021 (8), -0.02906 (8), -0.02898 (8), -0.02764 (8), -0.02535 (8), -0.02335 (8), -0.02225 (8), \nlength of domains: 16\nTotal time: 0.5682\t pickout: 0.0015\t decision: 0.0303\t get_bound: 0.5338\t add_domain: 0.0026\nAccumulated time:\t pickout: 0.0051\t decision: 0.3127\t get_bound: 2.2196\t add_domain: 0.0178\nCurrent (lb-rhs): -0.04667258262634277\n121 domains visited\nCumulative time: 2.7114052772521973\n\nbatch:  torch.Size([16, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([16, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 21 / 32 = 0.65625\npruning-in-iteration extra time: 0.026453495025634766\nTensors transferred: pre=3.0153M lA=0.5182M alpha=0.2065M beta=0.0003M\nThis batch time : update_bounds func: 0.5141\t prepare: 0.0020\t bound: 0.5084\t transfer: 0.0032\t finalize: 0.0004\nAccumulated time: update_bounds func: 2.7293\t prepare: 0.0145\t bound: 2.6791\t transfer: 0.0319\t finalize: 0.0035\nbatch bounding time:  0.5141181945800781\nCurrent worst splitting domains lb-rhs (depth):\n-0.01575 (9), -0.01069 (9), -0.00833 (9), -0.00693 (9), -0.00548 (9), -0.00480 (9), -0.00447 (9), -0.00223 (9), -0.00112 (9), -0.00060 (9), -0.00032 (9), \nlength of domains: 11\nTotal time: 0.5373\t pickout: 0.0009\t decision: 0.0196\t get_bound: 0.5142\t add_domain: 0.0026\nAccumulated time:\t pickout: 0.0060\t decision: 0.3323\t get_bound: 2.7338\t add_domain: 0.0204\nCurrent (lb-rhs): -0.015745878219604492\n142 domains visited\nCumulative time: 3.249002695083618\n\nbatch:  torch.Size([11, 32, 32, 32]) pre split depth:  2\nbatch:  torch.Size([11, 32, 32, 32]) post split depth:  2\nsplitting decisions: \nsplit level 0: [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] \nsplit level 1: [3, 207] [3, 207] [3, 207] [3, 207] [3, 179] [3, 207] [3, 207] [3, 207] [3, 207] [3, 207] \n(44, 3, 32, 32) torch.Size([44, 1, 10]) torch.Size([44, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 44 / 44 = 1.0\npruning-in-iteration extra time: 0.0001461505889892578\nTensors transferred: pre=4.1460M lA=2.0730M alpha=0.2840M beta=0.0005M\nThis batch time : update_bounds func: 0.0131\t prepare: 0.0022\t bound: 0.0059\t transfer: 0.0044\t finalize: 0.0005\nAccumulated time: update_bounds func: 2.7423\t prepare: 0.0167\t bound: 2.6850\t transfer: 0.0362\t finalize: 0.0040\nbatch bounding time:  0.013106584548950195\nlength of domains: 0\nTotal time: 0.0429\t pickout: 0.0010\t decision: 0.0200\t get_bound: 0.0158\t add_domain: 0.0060\nAccumulated time:\t pickout: 0.0070\t decision: 0.3522\t get_bound: 2.7496\t add_domain: 0.0264\nNo domains left, verification finished!\n186 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 3.292811155319214\n\n\nProperties batch 4, size 1\nRemaining timeout: 285.5825400352478\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0. -1.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.6754398345947266.\n\nProperties batch 5, size 1\nRemaining timeout: 285.544095993042\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0. -1.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.21066689491271973.\n\nProperties batch 6, size 1\nRemaining timeout: 285.51739168167114\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1. -1.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.8965573310852051.\n\nProperties batch 7, size 1\nRemaining timeout: 285.4918098449707\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1.  0. -1.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.815125942230225.\n\nProperties batch 8, size 1\nRemaining timeout: 285.4661774635315\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1.  0.  0. -1.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 1.7716403007507324.\nResult: unsat\nTime: 14.559200286865234\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_79_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.392583131790161",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpmm8syk1k.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:14:23 2024 on Cerberus\nInternal results will be saved to /tmp/tmpmm8syk1k.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 5.74447012,  4.42454815, -2.42684126,  1.39197063, -0.32172263,\n         -1.55043101, -3.09571075, -1.65020883,  9.32281113,  3.43952250]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 5.94260406,  4.11544514, -2.25063801,  1.45511723, -0.20617530,\n          -1.56343246, -3.12074471, -1.37484074,  8.60002995,  3.29764056],\n         [ 5.94260406,  4.11544514, -2.25063801,  1.45511723, -0.20617530,\n          -1.56343246, -3.12074471, -1.37484074,  8.60002995,  3.29764056]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 2.65742588,  4.48458481, 10.85066795,  7.14491272,  8.80620480,\n          10.16346264, 11.72077465,  9.97487068,  5.30238914]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5066 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 5.74447012,  4.42454815, -2.42684126,  1.39197063, -0.32172263,\n         -1.55043101, -3.09571075, -1.65020883,  9.32281113,  3.43952250]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [651]; unstable size 651; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 409 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 315 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [409]; unstable size 409; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 315 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [315]; unstable size 315; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [29]; unstable size 29; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.02741385,  3.13008499,  9.57361889,  6.11081362,  7.53568220,\n          9.06027031, 10.50288391,  8.74798012,  4.31495667]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.81732702255249\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_89_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.536479473114014",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpjnwij5kf.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:14:31 2024 on Cerberus\nInternal results will be saved to /tmp/tmpjnwij5kf.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 4.45820999,  7.45845509, -3.23980594, -2.27957010, -2.48054481,\n         -2.96445680, -1.49695182,  0.53347808,  2.00121641, 12.13869095]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 4.48702860,  8.27062798, -3.10395789, -2.26830912, -2.43770671,\n          -3.00064445, -1.41095793,  0.37030119,  2.13667798, 11.57998276],\n         [ 4.48702860,  8.27062798, -3.10395789, -2.26830912, -2.43770671,\n          -3.00064445, -1.41095793,  0.37030119,  2.13667798, 11.57998276]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 7.09295416,  3.30935478, 14.68394089, 13.84829140, 14.01768970,\n          14.58062744, 12.99094105, 11.20968151,  9.44330502]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5619 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 4.45820999,  7.45845509, -3.23980594, -2.27957010, -2.48054481,\n         -2.96445680, -1.49695182,  0.53347808,  2.00121641, 12.13869095]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1424]; unstable size 1424; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [779]; unstable size 779; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [711]; unstable size 711; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [72]; unstable size 72; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 3.64582014,  1.50040650, 10.60079098, 10.46342564,  9.78866577,\n         10.80334854,  9.39673996,  6.79292107,  5.55253315]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.0496978759765625\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_98_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.4610350131988525",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpk27u0b3r.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:14:39 2024 on Cerberus\nInternal results will be saved to /tmp/tmpk27u0b3r.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[11.13540268, -2.12778544,  5.17189455,  3.04508901,  0.87349188,\n          1.41967106,  3.88908839, -2.58672547, -2.09313250,  1.06364989]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[10.81753254, -2.22862196,  5.32995367,  3.01262641,  0.91334116,\n           1.51196551,  3.93635917, -2.66108012, -2.23742366,  0.85475343],\n         [10.81753254, -2.22862196,  5.32995367,  3.01262641,  0.91334116,\n           1.51196551,  3.93635917, -2.66108012, -2.23742366,  0.85475343]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[13.04615402,  5.48757887,  7.80490589,  9.90419102,  9.30556679,\n           6.88117313, 13.47861290, 13.05495644,  9.96277905]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5671 seconds.\nPGD attack failed\nModel prediction is: tensor([[11.13540268, -2.12778544,  5.17189455,  3.04508901,  0.87349188,\n          1.41967106,  3.88908839, -2.58672547, -2.09313250,  1.06364989]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [453]; unstable size 453; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 255 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 185 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [255]; unstable size 255; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 185 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [185]; unstable size 185; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [18]; unstable size 18; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[11.61046410,  5.03864193,  7.01849794,  8.97960472,  8.52254963,\n          6.12827015, 12.15253353, 12.06939507,  8.96183014]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.995360851287842\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_11_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "7.571279764175415",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpfpo9pljd.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:14:47 2024 on Cerberus\nInternal results will be saved to /tmp/tmpfpo9pljd.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333730936050415, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.21427953,  1.82282424, -0.95329773, -0.56605446, -0.84413540,\n         -1.23256934, -1.73097754, -0.13519278,  1.34799814,  2.77015758]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.29628807,  2.17531061, -1.04846025, -0.64505720, -0.91715002,\n          -1.33519125, -1.74386621, -0.27812806,  1.57999504,  2.65876412],\n         [ 0.29628807,  2.17531061, -1.04846025, -0.64505720, -0.91715002,\n          -1.33519125, -1.74386621, -0.27812806,  1.57999504,  2.65876412]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[2.36247611, 0.48345351, 3.70722437, 3.30382133, 3.57591414,\n          3.99395537, 4.40263033, 2.93689227, 1.07876909]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.1954 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.21427953,  1.82282424, -0.95329773, -0.56605446, -0.84413540,\n         -1.23256934, -1.73097754, -0.13519278,  1.34799814,  2.77015758]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [271]; unstable size 271; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 72 total_size 128 output_shape 128\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 104 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [764]; unstable size 764; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 104 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [104]; unstable size 104; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 1.12246096, -0.09214263,  2.15115285,  1.82449377,  2.04218793,\n          2.28744030,  2.44691086,  0.59095800, -0.05210555]], device='cuda:0') None\n\nall verified at 15th iter\nbest_l after optimization: 13.648921966552734 with beta sum per layer: []\nalpha/beta optimization time: 1.6604773998260498\ninitial alpha-CROWN bounds: tensor([[1.25593841e+00, 6.01112843e-05, 2.30788612e+00, 1.95581341e+00,\n         2.18180227e+00, 2.43680477e+00, 2.61702156e+00, 8.00340235e-01,\n         9.32550728e-02]], device='cuda:0')\nWorst class: (+ rhs) 6.0111284255981445e-05\nverified with init bound!\nResult: unsat\nTime: 6.131303548812866\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_23_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.559837818145752",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpubljhlco.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:14:56 2024 on Cerberus\nInternal results will be saved to /tmp/tmpubljhlco.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-1.15086710,  3.25605297, -1.23933947, -0.37451699, -0.90260547,\n         -0.36854154, -0.54609567, -0.02293947, -1.39983487,  3.49594378]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-1.04514432,  3.73228550, -1.37737966, -0.42511427, -1.09885812,\n          -0.39768636, -0.55368650, -0.32959464, -1.25875306,  3.67082763],\n         [-1.04514432,  3.73228550, -1.37737966, -0.42511427, -1.09885812,\n          -0.39768636, -0.55368650, -0.32959464, -1.25875306,  3.67082763]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 4.71597195, -0.06145787,  5.04820728,  4.09594202,  4.76968575,\n           4.06851387,  4.22451401,  4.00042248,  4.92958069]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.2522 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.041693449020386\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_39_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.262538194656372",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpvkj7q5gq.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:03 2024 on Cerberus\nInternal results will be saved to /tmp/tmpvkj7q5gq.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.85258293,  0.00403488, -0.37607461,  0.47144255, -0.00784957,\n          1.03636110, -0.47195727, -0.07978809,  0.87156272, -0.89035553]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.85305285,  0.83073950, -0.57770157,  0.24319662, -0.20337465,\n           0.67435169, -0.66313702, -0.24651816,  0.99708223, -0.31799167],\n         [-0.85305285,  0.83073950, -0.57770157,  0.24319662, -0.20337465,\n           0.67435169, -0.66313702, -0.24651816,  0.99708223, -0.31799167]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 1.52740455, -0.15638781,  1.25205326,  0.43115509,  0.87772632,\n           1.33748865,  0.92086983, -0.32273054,  0.99234337]]],\n       device='cuda:0')\nnumber of violation:  2\nAttack finished in 1.2002 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 3.8041536808013916\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_55_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "7.931611061096191",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpmverpka0.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:09 2024 on Cerberus\nInternal results will be saved to /tmp/tmpmverpka0.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333704859018326, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.42585230,  0.08431870, -0.01928085, -0.90985799,  0.06956941,\n         -1.05303371, -0.86464828, -0.79066396,  2.44238663,  0.17502323]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.44630742,  0.24157470, -0.05671391, -0.90471673,  0.04474342,\n          -0.98103559, -0.92129189, -0.63299358,  2.13165689,  0.16863680],\n         [ 1.44630742,  0.24157470, -0.05671391, -0.90471673,  0.04474342,\n          -0.98103559, -0.92129189, -0.63299358,  2.13165689,  0.16863680]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[0.68534946, 1.89008212, 2.18837070, 3.03637362, 2.08691359,\n          3.11269236, 3.05294871, 2.76465034, 1.96302009]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.3678 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.42585230,  0.08431870, -0.01928085, -0.90985799,  0.06956941,\n         -1.05303371, -0.86464828, -0.79066396,  2.44238663,  0.17502323]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [239]; unstable size 239; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 65 total_size 128 output_shape 128\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 109 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [1126]; unstable size 1126; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 109 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [109]; unstable size 109; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[-0.02372168,  0.26599926,  0.59504104,  1.23817945,  0.60576880,\n          1.16680086,  1.19513106,  0.77421427,  0.18743014]], device='cuda:0') None\n\nall verified at 2th iter\nbest_l after optimization: 6.494163513183594 with beta sum per layer: []\nalpha/beta optimization time: 1.5671911239624023\ninitial alpha-CROWN bounds: tensor([[0.01864117, 0.32097578, 0.64471436, 1.29487813, 0.65704823, 1.22854948,\n         1.24993300, 0.84288299, 0.23654032]], device='cuda:0')\nWorst class: (+ rhs) 0.018641173839569092\nverified with init bound!\nResult: unsat\nTime: 6.370591402053833\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_74_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.920593023300171",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpxwm078ih.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:19 2024 on Cerberus\nInternal results will be saved to /tmp/tmpxwm078ih.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.99456906,  0.92221719, -0.18530147, -1.26380289,  0.52791083,\n         -2.13896990, -2.99637532,  0.54190046,  1.94590044,  1.81956339]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.79191017,  0.95040131, -0.33032101, -1.12356019,  0.31390333,\n          -1.99346793, -2.62409258,  0.25408816,  2.13896036,  1.78995204],\n         [ 1.79191017,  0.95040131, -0.33032101, -1.12356019,  0.31390333,\n          -1.99346793, -2.62409258,  0.25408816,  2.13896036,  1.78995204]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 8.41508865e-01,  2.12223125e+00,  2.91547036e+00,  1.47800684e+00,\n           3.78537798e+00,  4.41600275e+00,  1.53782201e+00, -3.47050190e-01,\n           1.95813179e-03]]], device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.5420 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.367931604385376\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_86_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "6.15842866897583",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpqel8i2xw.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:26 2024 on Cerberus\nInternal results will be saved to /tmp/tmpqel8i2xw.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.14482355,  0.42702734,  1.17975521,  0.42308298, -0.50366199,\n          0.39120865, -1.85243201, -0.93027544,  0.79907751,  0.54692328]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.13597828,  0.66834402,  0.47835025,  0.33799794, -0.37628934,\n           0.34226054, -1.88110042, -0.92241907,  0.90814900,  0.81491107],\n         [ 0.13597828,  0.66834402,  0.47835025,  0.33799794, -0.37628934,\n           0.34226054, -1.88110042, -0.92241907,  0.90814900,  0.81491107]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 0.34237197, -0.18999377,  0.14035231,  0.85463959,  0.13608971,\n           2.35945058,  1.40076935, -0.42979875, -0.33656082]]],\n       device='cuda:0')\nnumber of violation:  3\nAttack finished in 1.2961 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.631067276000977\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_98_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.532458782196045",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpdzk5urqg.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:34 2024 on Cerberus\nInternal results will be saved to /tmp/tmpdzk5urqg.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333730936050415, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 3.94495416, -2.07960415,  1.75831628,  0.85984302, -0.04009467,\n         -0.15203522, -0.26500970, -0.92295241, -1.24032235, -1.26781774]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 3.64041114, -2.15951562,  1.84199905,  0.90193647, -0.02822447,\n          -0.08333384, -0.13002752, -0.84000218, -1.27070475, -1.38328218],\n         [ 3.64041114, -2.15951562,  1.84199905,  0.90193647, -0.02822447,\n          -0.08333384, -0.13002752, -0.84000218, -1.27070475, -1.38328218]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.79992676, 1.79841208, 2.73847461, 3.66863561, 3.72374487,\n          3.77043867, 4.48041344, 4.91111565, 5.02369308]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.3251 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 3.94495416, -2.07960415,  1.75831628,  0.85984302, -0.04009467,\n         -0.15203522, -0.26500970, -0.92295241, -1.24032235, -1.26781774]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [103]; unstable size 103; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 219 total_size 8192 output_shape (128, 8, 8)\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 41 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [219]; unstable size 219; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 41 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [41]; unstable size 41; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[5.12637568, 1.59213471, 2.38341904, 3.30685139, 3.30003595, 3.32878542,\n         3.79563498, 4.43923283, 4.36108780]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.886159181594849\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_10_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "7.727678537368774",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpz0nc51cb.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:42 2024 on Cerberus\nInternal results will be saved to /tmp/tmpz0nc51cb.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.43827474, -5.26821756, -0.24924204, -1.00314927, -1.42481220,\n         -1.00641870, -2.44114733, -2.40455437,  0.35379457, -4.80686331]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.35088444, -5.26208687, -0.27259591, -0.93527448, -1.39557958,\n          -0.94545192, -2.31221080, -2.32778692,  0.55532157, -4.74117327],\n         [ 1.35088444, -5.26208687, -0.27259591, -0.93527448, -1.39557958,\n          -0.94545192, -2.31221080, -2.32778692,  0.55532157, -4.74117327]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[6.61297131, 1.62348032, 2.28615904, 2.74646401, 2.29633641,\n          3.66309524, 3.67867136, 0.79556286, 6.09205770]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.8445 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.43827474, -5.26821756, -0.24924204, -1.00314927, -1.42481220,\n         -1.00641870, -2.44114733, -2.40455437,  0.35379457, -4.80686331]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [3119]; unstable size 3119; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 153 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 49 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [153]; unstable size 153; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 49 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1252]; unstable size 1252; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [79]; unstable size 79; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [41]; unstable size 41; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [67]; unstable size 67; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[5.06930351, 0.73012924, 1.20831871, 1.51309752, 0.97737789, 2.18857479,\n         2.55259299, 0.01476863, 4.82836056]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 6.070045471191406\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_21_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.546534538269043",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp3yvuzvhl.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:51 2024 on Cerberus\nInternal results will be saved to /tmp/tmp3yvuzvhl.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 2.14061451, -7.58005762,  1.55118299, -0.69311768, -1.58620059,\n         -3.66751027, -4.57932281, -5.63344622, -4.01265717, -7.37496614]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 2.07994413, -7.58967447,  1.58834994, -0.70260131, -1.58201480,\n          -3.65404916, -4.56366301, -5.60641527, -4.04987907, -7.38536358],\n         [ 2.07994413, -7.58967447,  1.58834994, -0.70260131, -1.58201480,\n          -3.65404916, -4.56366301, -5.60641527, -4.04987907, -7.38536358]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[9.66961861, 0.49159420, 2.78254557, 3.66195893, 5.73399353,\n          6.64360714, 7.68635941, 6.12982321, 9.46530724]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6129 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 2.14061451, -7.58005762,  1.55118299, -0.69311768, -1.58620059,\n         -3.66751027, -4.57932281, -5.63344622, -4.01265717, -7.37496614]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [3565]; unstable size 3565; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 12 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 282 total_size 16384 output_shape (64, 16, 16)\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [12]; unstable size 12; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 282 total_size 16384 output_shape (64, 16, 16)\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [282]; unstable size 282; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [14]; unstable size 14; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [7]; unstable size 7; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [9]; unstable size 9; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[9.27485943, 0.43070447, 2.65081573, 3.50906706, 5.58280325, 6.43017006,\n         7.46400547, 5.93557215, 9.23428822]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.1061317920684814\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_37_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.827599763870239",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpt32mrm95.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:15:59 2024 on Cerberus\nInternal results will be saved to /tmp/tmpt32mrm95.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.25571486,  3.77851868, -4.00916004, -3.22680521, -4.53054523,\n         -4.37565517, -5.53191614, -4.20376968,  0.47601944,  3.14984918]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.21722656,  3.55940342, -3.96903133, -3.15021801, -4.50026608,\n          -4.26555204, -5.46244669, -4.09355307,  0.32603645,  3.24944854],\n         [ 0.21722656,  3.55940342, -3.96903133, -3.15021801, -4.50026608,\n          -4.26555204, -5.46244669, -4.09355307,  0.32603645,  3.24944854]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.34217691, 7.52843475, 6.70962143, 8.05966949, 7.82495546,\n          9.02185059, 7.65295649, 3.23336697, 0.30995488]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6699 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.25571486,  3.77851868, -4.00916004, -3.22680521, -4.53054523,\n         -4.37565517, -5.53191614, -4.20376968,  0.47601944,  3.14984918]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2147]; unstable size 2147; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 70 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [70]; unstable size 70; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [731]; unstable size 731; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [60]; unstable size 60; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [24]; unstable size 24; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [42]; unstable size 42; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[2.43055463, 6.70373487, 6.04302597, 7.19202232, 7.07307339, 8.29016685,\n         6.64738560, 2.35014272, 0.03637815]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.341480731964111\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_50_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.70250129699707",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpwk0tpkyd.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:16:07 2024 on Cerberus\nInternal results will be saved to /tmp/tmpwk0tpkyd.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.88575494, -2.46948004, -0.98218197, -1.86838031, -2.75453448,\n         -3.04634857, -3.35564113, -0.76873565, -1.40446281,  0.81824809]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.84236693, -2.81302500, -0.87701511, -1.80246806, -2.65725541,\n          -2.97314787, -3.35251307, -0.61052889, -1.43700242,  0.62772131],\n         [-0.84236693, -2.81302500, -0.87701511, -1.80246806, -2.65725541,\n          -2.97314787, -3.35251307, -0.61052889, -1.43700242,  0.62772131]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.47008824, 3.44074631, 1.50473642, 2.43018937, 3.28497672,\n          3.60086918, 3.98023438, 1.23825026, 2.06472373]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6466 seconds.\nPGD attack failed\nModel prediction is: tensor([[-0.88575494, -2.46948004, -0.98218197, -1.86838031, -2.75453448,\n         -3.04634857, -3.35564113, -0.76873565, -1.40446281,  0.81824809]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2520]; unstable size 2520; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 126 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [126]; unstable size 126; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [956]; unstable size 956; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [68]; unstable size 68; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [27]; unstable size 27; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [27]; unstable size 27; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[1.05932522, 2.63591647, 1.12628949, 2.04380560, 2.82444215, 3.17685676,\n         3.54555583, 0.90777159, 1.59514797]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.16677713394165\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_64_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "9.688103914260864",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp6ekg6n5e.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:16:15 2024 on Cerberus\nInternal results will be saved to /tmp/tmp6ekg6n5e.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.04189730, -1.39616895,  0.58027250,  0.29884806,  0.46679962,\n          0.18952516,  1.31852853, -0.25026628, -4.26790476, -1.34158278]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.01591706, -1.55922532,  0.63024652,  0.30251116,  0.53322899,\n           0.23309767,  1.22137964, -0.18820685, -4.19713736, -1.40943885],\n         [-2.01591706, -1.55922532,  0.63024652,  0.30251116,  0.53322899,\n           0.23309767,  1.22137964, -0.18820685, -4.19713736, -1.40943885]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.23729658, 2.78060484, 0.59113312, 0.91886848, 0.68815064,\n          0.98828197, 1.40958643, 5.41851711, 2.63081837]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.7644 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.04189730, -1.39616895,  0.58027250,  0.29884806,  0.46679962,\n          0.18952516,  1.31852853, -0.25026628, -4.26790476, -1.34158278]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2044]; unstable size 2044; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 138 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 50 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [138]; unstable size 138; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 50 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1413]; unstable size 1413; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [124]; unstable size 124; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [49]; unstable size 49; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [90]; unstable size 90; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 1.81039143,  0.72419596, -0.10799143,  0.31532481,  0.10968953,\n          0.27898467,  0.39123279,  3.65472865,  0.96664143]], device='cuda:0') None\n\nall verified at 11th iter\nbest_l after optimization: 9.575700759887695 with beta sum per layer: []\nalpha/beta optimization time: 2.7766430377960205\ninitial alpha-CROWN bounds: tensor([[1.99569869e+00, 1.03388238e+00, 2.20707059e-03, 4.26994056e-01,\n         1.97594225e-01, 3.70850682e-01, 5.36541879e-01, 3.82398605e+00,\n         1.18794537e+00]], device='cuda:0')\nWorst class: (+ rhs) 0.0022070705890655518\nverified with init bound!\nResult: unsat\nTime: 8.241312742233276\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_78_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "188.9524791240692",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmph22v_kw3.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:16:26 2024 on Cerberus\nInternal results will be saved to /tmp/tmph22v_kw3.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n          1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.12218428, -3.37873507,  0.83384383,  1.15502465,  0.81430137,\n           1.14255941,  0.72699016, -0.51800579, -3.24479532, -3.37587047],\n         [-2.12218428, -3.37873507,  0.83384383,  1.15502465,  0.81430137,\n           1.14255941,  0.72699016, -0.51800579, -3.24479532, -3.37587047]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.27720881, 4.53375959, 0.32118082, 0.34072328, 0.01246524,\n          0.42803448, 1.67303038, 4.39981985, 4.53089523]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6043 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n          1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [1942]; unstable size 1942; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 171 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [171]; unstable size 171; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1141]; unstable size 1141; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [81]; unstable size 81; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [26]; unstable size 26; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [50]; unstable size 50; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.46627474,  3.58290339,  0.00764811,  0.03963864, -0.11114848,\n         -0.01987123,  1.07829618,  3.52050734,  3.87006450]], device='cuda:0') None\nbest_l after optimization: 14.863641738891602 with beta sum per layer: []\nalpha/beta optimization time: 9.678701877593994\ninitial alpha-CROWN bounds: tensor([[ 2.54750180e+00,  3.65459013e+00,  4.54752445e-02,  7.04848766e-02,\n         -9.75860953e-02,  2.63571739e-03,  1.12346363e+00,  3.59175563e+00,\n          3.92532039e+00]], device='cuda:0')\nWorst class: (+ rhs) -0.09758609533309937\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 64, 16, 16]), torch.Size([1, 9, 64, 8, 8]), torch.Size([1, 9, 512]), torch.Size([1, 9, 512])]\n\nProperties batch 0, size 1\nRemaining timeout: 285.1762444972992\n##### Instance 0 first 10 spec matrices: [[[-1.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.547501802444458.\n\nProperties batch 1, size 1\nRemaining timeout: 285.13469767570496\n##### Instance 0 first 10 spec matrices: [[[ 0. -1.  0.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.654590129852295.\n\nProperties batch 2, size 1\nRemaining timeout: 285.1013743877411\n##### Instance 0 first 10 spec matrices: [[[ 0.  0. -1.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.04547524452209473.\n\nProperties batch 3, size 1\nRemaining timeout: 285.07084679603577\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  1. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.07048487663269043.\n\nProperties batch 4, size 1\nRemaining timeout: 285.04390120506287\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  1.  0. -1.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.09758610]], device='cuda:0') need to verify.\nModel prediction is: tensor([-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n         1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /34 start_node /46 with alignment adjustment\nsetting alpha for layer /36 start_node /46 with alignment adjustment\nsetting alpha for layer /38 start_node /46 with alignment adjustment\nsetting alpha for layer /40 start_node /46 with alignment adjustment\nsetting alpha for layer /43 start_node /46 with alignment adjustment\nsetting alpha for layer /45 start_node /46 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 64, 16, 16]), torch.Size([1, 1, 64, 8, 8]), torch.Size([1, 1, 512]), torch.Size([1, 1, 512])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.09758610]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/46']\nKeeping slopes for these layers: ['/46']\nlayer 0 size torch.Size([32768]) unstable 1942\nlayer 1 size torch.Size([8192]) unstable 166\nlayer 2 size torch.Size([16384]) unstable 1113\nlayer 3 size torch.Size([4096]) unstable 77\nlayer 4 size torch.Size([512]) unstable 25\nlayer 5 size torch.Size([512]) unstable 49\n-----------------\n# of unstable neurons: 3372\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [5, 112] \nsplit level 1: [4, 444] \nsplit level 2: [5, 419] \nsplit level 3: [5, 444] \nsplit level 4: [5, 202] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 14 / 32 = 0.4375\npruning-in-iteration extra time: 0.029952526092529297\nTensors transferred: pre=3.8125M lA=1.0723M alpha=0.2082M beta=0.0002M\nThis batch time : update_bounds func: 0.9236\t prepare: 0.0031\t bound: 0.9154\t transfer: 0.0044\t finalize: 0.0007\nAccumulated time: update_bounds func: 0.9236\t prepare: 0.0031\t bound: 0.9154\t transfer: 0.0044\t finalize: 0.0007\nbatch bounding time:  0.923656702041626\nCurrent worst splitting domains lb-rhs (depth):\n-0.06755 (5), -0.05574 (5), -0.05415 (5), -0.04667 (5), -0.04550 (5), -0.03474 (5), -0.02976 (5), -0.02654 (5), -0.02516 (5), -0.02263 (5), -0.01366 (5), -0.01081 (5), -0.00789 (5), -0.00785 (5), -0.00582 (5), -0.00557 (5), -0.00478 (5), -0.00445 (5), \nlength of domains: 18\nTotal time: 1.1586\t pickout: 0.0010\t decision: 0.2260\t get_bound: 0.9284\t add_domain: 0.0032\nAccumulated time:\t pickout: 0.0010\t decision: 0.2260\t get_bound: 0.9284\t add_domain: 0.0032\nCurrent (lb-rhs): -0.06755101680755615\n14 domains visited\nCumulative time: 1.3746366500854492\n\nbatch:  torch.Size([18, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([18, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 36] [4, 166] [4, 36] [4, 166] [4, 36] [4, 395] [4, 36] [4, 395] [4, 36] [4, 395] \n(36, 3, 32, 32) torch.Size([36, 1, 10]) torch.Size([36, 1])\npruning_in_iteration open status: True\nratio of positive domain = 12 / 36 = 0.33333333333333337\npruning-in-iteration extra time: 0.030285120010375977\nTensors transferred: pre=4.2891M lA=1.4297M alpha=0.2342M beta=0.0002M\nThis batch time : update_bounds func: 0.7300\t prepare: 0.0028\t bound: 0.7213\t transfer: 0.0052\t finalize: 0.0008\nAccumulated time: update_bounds func: 1.6536\t prepare: 0.0059\t bound: 1.6367\t transfer: 0.0095\t finalize: 0.0014\nbatch bounding time:  0.7301294803619385\nCurrent worst splitting domains lb-rhs (depth):\n-0.05872 (6), -0.05512 (6), -0.04728 (6), -0.04578 (6), -0.04182 (6), -0.04008 (6), -0.03942 (6), -0.03906 (6), -0.03698 (6), -0.03045 (6), -0.02911 (6), -0.02629 (6), -0.02538 (6), -0.02119 (6), -0.02098 (6), -0.01910 (6), -0.01895 (6), -0.01839 (6), -0.01357 (6), -0.00458 (6), \nlength of domains: 24\nTotal time: 0.7631\t pickout: 0.0011\t decision: 0.0276\t get_bound: 0.7302\t add_domain: 0.0042\nAccumulated time:\t pickout: 0.0021\t decision: 0.2536\t get_bound: 1.6586\t add_domain: 0.0075\nCurrent (lb-rhs): -0.058716654777526855\n26 domains visited\nCumulative time: 2.1380109786987305\n\nbatch:  torch.Size([24, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([24, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 316] [4, 395] [4, 316] [4, 395] [4, 316] [4, 166] [4, 359] [4, 166] [4, 359] [4, 166] \n(48, 3, 32, 32) torch.Size([48, 1, 10]) torch.Size([48, 1])\npruning_in_iteration open status: True\nratio of positive domain = 10 / 48 = 0.20833333333333337\npruning-in-iteration extra time: 0.007773160934448242\nTensors transferred: pre=5.7188M lA=2.2637M alpha=0.3123M beta=0.0004M\nThis batch time : update_bounds func: 0.6618\t prepare: 0.0034\t bound: 0.6516\t transfer: 0.0059\t finalize: 0.0008\nAccumulated time: update_bounds func: 2.3154\t prepare: 0.0093\t bound: 2.2883\t transfer: 0.0154\t finalize: 0.0022\nbatch bounding time:  0.6618466377258301\nCurrent worst splitting domains lb-rhs (depth):\n-0.05447 (7), -0.05081 (7), -0.04514 (7), -0.04290 (7), -0.04260 (7), -0.04246 (7), -0.03844 (7), -0.03811 (7), -0.03644 (7), -0.03600 (7), -0.03532 (7), -0.03352 (7), -0.03320 (7), -0.03210 (7), -0.03058 (7), -0.02987 (7), -0.02700 (7), -0.02597 (7), -0.02535 (7), -0.02505 (7), \nlength of domains: 38\nTotal time: 0.7041\t pickout: 0.0011\t decision: 0.0358\t get_bound: 0.6619\t add_domain: 0.0053\nAccumulated time:\t pickout: 0.0032\t decision: 0.2894\t get_bound: 2.3205\t add_domain: 0.0127\nCurrent (lb-rhs): -0.054467201232910156\n36 domains visited\nCumulative time: 2.842438220977783\n\nbatch:  torch.Size([38, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([38, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 413] [5, 413] [5, 413] [4, 316] [5, 413] [4, 316] [5, 413] [4, 316] [4, 166] [5, 413] \n(76, 3, 32, 32) torch.Size([76, 1, 10]) torch.Size([76, 1])\npruning_in_iteration open status: False\nratio of positive domain = 7 / 76 = 0.09210526315789469\npruning-in-iteration extra time: 0.00015473365783691406\nTensors transferred: pre=9.0547M lA=4.5273M alpha=0.4945M beta=0.0007M\nThis batch time : update_bounds func: 0.6576\t prepare: 0.0054\t bound: 0.6368\t transfer: 0.0137\t finalize: 0.0016\nAccumulated time: update_bounds func: 2.9730\t prepare: 0.0147\t bound: 2.9251\t transfer: 0.0291\t finalize: 0.0038\nbatch bounding time:  0.6576581001281738\nCurrent worst splitting domains lb-rhs (depth):\n-0.05067 (8), -0.04709 (8), -0.04703 (8), -0.04333 (8), -0.04163 (8), -0.03980 (8), -0.03930 (8), -0.03879 (8), -0.03870 (8), -0.03560 (8), -0.03538 (8), -0.03508 (8), -0.03388 (8), -0.03363 (8), -0.03363 (8), -0.03348 (8), -0.03304 (8), -0.03292 (8), -0.03187 (8), -0.03141 (8), \nlength of domains: 69\nTotal time: 0.7085\t pickout: 0.0012\t decision: 0.0403\t get_bound: 0.6577\t add_domain: 0.0093\nAccumulated time:\t pickout: 0.0044\t decision: 0.3297\t get_bound: 2.9782\t add_domain: 0.0220\nCurrent (lb-rhs): -0.050666213035583496\n43 domains visited\nCumulative time: 3.5513696670532227\n\nbatch:  torch.Size([69, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([69, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 166] [4, 166] [4, 89] [5, 413] [4, 359] [5, 413] [4, 359] [5, 413] [4, 166] [4, 89] \n(138, 3, 32, 32) torch.Size([138, 1, 10]) torch.Size([138, 1])\npruning_in_iteration open status: False\nratio of positive domain = 22 / 138 = 0.1594202898550725\npruning-in-iteration extra time: 0.00019502639770507812\nTensors transferred: pre=16.4414M lA=8.2207M alpha=0.8978M beta=0.0013M\nThis batch time : update_bounds func: 0.7317\t prepare: 0.0072\t bound: 0.6981\t transfer: 0.0242\t finalize: 0.0021\nAccumulated time: update_bounds func: 3.7047\t prepare: 0.0218\t bound: 3.6233\t transfer: 0.0533\t finalize: 0.0059\nbatch bounding time:  0.73175048828125\nCurrent worst splitting domains lb-rhs (depth):\n-0.04770 (9), -0.04637 (9), -0.04419 (9), -0.04410 (9), -0.04219 (9), -0.04212 (9), -0.04041 (9), -0.03856 (9), -0.03784 (9), -0.03744 (9), -0.03717 (9), -0.03642 (9), -0.03585 (9), -0.03578 (9), -0.03458 (9), -0.03320 (9), -0.03319 (9), -0.03233 (9), -0.03229 (9), -0.03104 (9), \nlength of domains: 116\nTotal time: 0.8049\t pickout: 0.0029\t decision: 0.0572\t get_bound: 0.7318\t add_domain: 0.0129\nAccumulated time:\t pickout: 0.0073\t decision: 0.3869\t get_bound: 3.7100\t add_domain: 0.0349\nCurrent (lb-rhs): -0.04769933223724365\n65 domains visited\nCumulative time: 4.357405662536621\n\nbatch:  torch.Size([116, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([116, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 89] [5, 281] [4, 395] [4, 395] [4, 89] [5, 281] [4, 89] [5, 281] [4, 166] [4, 395] \n(232, 3, 32, 32) torch.Size([232, 1, 10]) torch.Size([232, 1])\npruning_in_iteration open status: True\nratio of positive domain = 52 / 232 = 0.22413793103448276\npruning-in-iteration extra time: 0.02866840362548828\nTensors transferred: pre=27.6406M lA=10.7227M alpha=1.5094M beta=0.0027M\nThis batch time : update_bounds func: 0.9232\t prepare: 0.0121\t bound: 0.8703\t transfer: 0.0371\t finalize: 0.0035\nAccumulated time: update_bounds func: 4.6279\t prepare: 0.0339\t bound: 4.4935\t transfer: 0.0904\t finalize: 0.0094\nbatch bounding time:  0.9232995510101318\nCurrent worst splitting domains lb-rhs (depth):\n-0.04478 (10), -0.04331 (10), -0.04140 (10), -0.04135 (10), -0.04002 (10), -0.03924 (10), -0.03905 (10), -0.03863 (10), -0.03750 (10), -0.03653 (10), -0.03563 (10), -0.03554 (10), -0.03529 (10), -0.03463 (10), -0.03442 (10), -0.03376 (10), -0.03374 (10), -0.03307 (10), -0.03290 (10), -0.03279 (10), \nlength of domains: 180\nTotal time: 1.0369\t pickout: 0.0045\t decision: 0.0873\t get_bound: 0.9234\t add_domain: 0.0217\nAccumulated time:\t pickout: 0.0118\t decision: 0.4743\t get_bound: 4.6334\t add_domain: 0.0566\nCurrent (lb-rhs): -0.04478418827056885\n117 domains visited\nCumulative time: 5.396352529525757\n\nbatch:  torch.Size([180, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([180, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 395] [4, 166] [4, 166] [5, 281] [4, 348] [4, 359] [5, 413] [5, 281] [4, 395] [4, 166] \n(360, 3, 32, 32) torch.Size([360, 1, 10]) torch.Size([360, 1])\npruning_in_iteration open status: True\nratio of positive domain = 111 / 360 = 0.30833333333333335\npruning-in-iteration extra time: 0.03455710411071777\nTensors transferred: pre=42.8906M lA=14.8330M alpha=2.3421M beta=0.0045M\nThis batch time : update_bounds func: 1.1332\t prepare: 0.0185\t bound: 1.0436\t transfer: 0.0655\t finalize: 0.0052\nAccumulated time: update_bounds func: 5.7611\t prepare: 0.0524\t bound: 5.5371\t transfer: 0.1559\t finalize: 0.0145\nbatch bounding time:  1.1333916187286377\nCurrent worst splitting domains lb-rhs (depth):\n-0.04209 (11), -0.04013 (11), -0.03864 (11), -0.03861 (11), -0.03725 (11), -0.03589 (11), -0.03584 (11), -0.03571 (11), -0.03476 (11), -0.03361 (11), -0.03359 (11), -0.03281 (11), -0.03279 (11), -0.03208 (11), -0.03164 (11), -0.03092 (11), -0.03074 (11), -0.03072 (11), -0.03063 (11), -0.03037 (11), \nlength of domains: 249\nTotal time: 1.3014\t pickout: 0.0067\t decision: 0.1323\t get_bound: 1.1335\t add_domain: 0.0289\nAccumulated time:\t pickout: 0.0185\t decision: 0.6065\t get_bound: 5.7669\t add_domain: 0.0856\nCurrent (lb-rhs): -0.04209411144256592\n228 domains visited\nCumulative time: 6.6994616985321045\n\nbatch:  torch.Size([249, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([249, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [4, 89] [4, 348] [4, 348] [4, 89] [4, 348] [4, 348] [5, 281] [4, 89] [5, 281] \n(498, 3, 32, 32) torch.Size([498, 1, 10]) torch.Size([498, 1])\npruning_in_iteration open status: True\nratio of positive domain = 164 / 498 = 0.3293172690763052\npruning-in-iteration extra time: 0.03361177444458008\nTensors transferred: pre=59.3320M lA=19.8965M alpha=3.2400M beta=0.0071M\nThis batch time : update_bounds func: 1.4021\t prepare: 0.0231\t bound: 1.3035\t transfer: 0.0675\t finalize: 0.0076\nAccumulated time: update_bounds func: 7.1632\t prepare: 0.0755\t bound: 6.8406\t transfer: 0.2235\t finalize: 0.0221\nbatch bounding time:  1.4023468494415283\nCurrent worst splitting domains lb-rhs (depth):\n-0.04000 (12), -0.03833 (12), -0.03647 (12), -0.03643 (12), -0.03630 (12), -0.03516 (12), -0.03432 (12), -0.03401 (12), -0.03391 (12), -0.03386 (12), -0.03378 (12), -0.03340 (12), -0.03244 (12), -0.03138 (12), -0.03110 (12), -0.03104 (12), -0.03094 (12), -0.03082 (12), -0.03076 (12), -0.03046 (12), \nlength of domains: 334\nTotal time: 1.6265\t pickout: 0.0090\t decision: 0.1728\t get_bound: 1.4024\t add_domain: 0.0423\nAccumulated time:\t pickout: 0.0275\t decision: 0.7794\t get_bound: 7.1693\t add_domain: 0.1279\nCurrent (lb-rhs): -0.039997100830078125\n392 domains visited\nCumulative time: 8.32870864868164\n\nbatch:  torch.Size([334, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([334, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] \n(668, 3, 32, 32) torch.Size([668, 1, 10]) torch.Size([668, 1])\npruning_in_iteration open status: True\nratio of positive domain = 318 / 668 = 0.4760479041916168\npruning-in-iteration extra time: 0.035948753356933594\nTensors transferred: pre=79.5859M lA=20.8496M alpha=4.3460M beta=0.0108M\nThis batch time : update_bounds func: 1.7754\t prepare: 0.0312\t bound: 1.6170\t transfer: 0.1171\t finalize: 0.0094\nAccumulated time: update_bounds func: 8.9386\t prepare: 0.1068\t bound: 8.4575\t transfer: 0.3406\t finalize: 0.0315\nbatch bounding time:  1.7756338119506836\nCurrent worst splitting domains lb-rhs (depth):\n-0.03834 (13), -0.03612 (13), -0.03471 (13), -0.03469 (13), -0.03453 (13), -0.03336 (13), -0.03205 (13), -0.03180 (13), -0.03179 (13), -0.03159 (13), -0.03142 (13), -0.03136 (13), -0.03074 (13), -0.02953 (13), -0.02950 (13), -0.02910 (13), -0.02905 (13), -0.02877 (13), -0.02870 (13), -0.02861 (13), \nlength of domains: 350\nTotal time: 2.2934\t pickout: 0.0119\t decision: 0.4592\t get_bound: 1.7757\t add_domain: 0.0466\nAccumulated time:\t pickout: 0.0393\t decision: 1.2386\t get_bound: 8.9450\t add_domain: 0.1745\nCurrent (lb-rhs): -0.03834187984466553\n710 domains visited\nCumulative time: 10.625191926956177\n\nbatch:  torch.Size([350, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([350, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [5, 45] [5, 45] [5, 45] [5, 45] [5, 45] [4, 348] [5, 281] [5, 45] [5, 45] \n(700, 3, 32, 32) torch.Size([700, 1, 10]) torch.Size([700, 1])\npruning_in_iteration open status: True\nratio of positive domain = 369 / 700 = 0.5271428571428571\npruning-in-iteration extra time: 0.04326009750366211\nTensors transferred: pre=83.3984M lA=19.7773M alpha=4.5542M beta=0.0120M\nThis batch time : update_bounds func: 1.9966\t prepare: 0.0330\t bound: 1.8417\t transfer: 0.1099\t finalize: 0.0113\nAccumulated time: update_bounds func: 10.9352\t prepare: 0.1398\t bound: 10.2992\t transfer: 0.4505\t finalize: 0.0429\nbatch bounding time:  1.9968280792236328\nCurrent worst splitting domains lb-rhs (depth):\n-0.03685 (14), -0.03455 (14), -0.03322 (14), -0.03321 (14), -0.03289 (14), -0.03191 (14), -0.03045 (14), -0.03037 (14), -0.03017 (14), -0.02998 (14), -0.02988 (14), -0.02975 (14), -0.02925 (14), -0.02843 (14), -0.02806 (14), -0.02780 (14), -0.02778 (14), -0.02720 (14), -0.02711 (14), -0.02703 (14), \nlength of domains: 331\nTotal time: 3.3780\t pickout: 0.0133\t decision: 1.3157\t get_bound: 1.9969\t add_domain: 0.0521\nAccumulated time:\t pickout: 0.0526\t decision: 2.5543\t get_bound: 10.9419\t add_domain: 0.2266\nCurrent (lb-rhs): -0.03684580326080322\n1079 domains visited\nCumulative time: 14.00641417503357\n\nbatch:  torch.Size([331, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([331, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] \n(662, 3, 32, 32) torch.Size([662, 1, 10]) torch.Size([662, 1])\npruning_in_iteration open status: True\nratio of positive domain = 349 / 662 = 0.527190332326284\npruning-in-iteration extra time: 0.045209646224975586\nTensors transferred: pre=78.8711M lA=18.6455M alpha=4.3069M beta=0.0126M\nThis batch time : update_bounds func: 2.2359\t prepare: 0.0323\t bound: 2.1060\t transfer: 0.0866\t finalize: 0.0104\nAccumulated time: update_bounds func: 13.1711\t prepare: 0.1720\t bound: 12.4052\t transfer: 0.5372\t finalize: 0.0532\nbatch bounding time:  2.2361109256744385\nCurrent worst splitting domains lb-rhs (depth):\n-0.03540 (15), -0.03313 (15), -0.03182 (15), -0.03176 (15), -0.03134 (15), -0.03046 (15), -0.02893 (15), -0.02885 (15), -0.02852 (15), -0.02851 (15), -0.02841 (15), -0.02831 (15), -0.02783 (15), -0.02707 (15), -0.02668 (15), -0.02628 (15), -0.02600 (15), -0.02579 (15), -0.02563 (15), -0.02543 (15), \nlength of domains: 313\nTotal time: 2.5389\t pickout: 0.0121\t decision: 0.2330\t get_bound: 2.2362\t add_domain: 0.0576\nAccumulated time:\t pickout: 0.0646\t decision: 2.7873\t get_bound: 13.1781\t add_domain: 0.2843\nCurrent (lb-rhs): -0.03539681434631348\n1428 domains visited\nCumulative time: 16.54830312728882\n\nbatch:  torch.Size([313, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([313, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [5, 354] [5, 454] [4, 251] [4, 251] [5, 354] [4, 348] [4, 348] [5, 354] [3, 1876] \n(626, 3, 32, 32) torch.Size([626, 1, 10]) torch.Size([626, 1])\npruning_in_iteration open status: True\nratio of positive domain = 272 / 626 = 0.43450479233226835\npruning-in-iteration extra time: 0.04077744483947754\nTensors transferred: pre=74.5820M lA=21.0879M alpha=4.0727M beta=0.0125M\nThis batch time : update_bounds func: 2.0992\t prepare: 0.0400\t bound: 1.9587\t transfer: 0.0905\t finalize: 0.0095\nAccumulated time: update_bounds func: 15.2702\t prepare: 0.2120\t bound: 14.3639\t transfer: 0.6277\t finalize: 0.0627\nbatch bounding time:  2.0993399620056152\nCurrent worst splitting domains lb-rhs (depth):\n-0.03438 (16), -0.03222 (16), -0.03080 (16), -0.03072 (16), -0.03030 (16), -0.02945 (16), -0.02792 (16), -0.02784 (16), -0.02758 (16), -0.02749 (16), -0.02736 (16), -0.02725 (16), -0.02681 (16), -0.02634 (16), -0.02595 (16), -0.02564 (16), -0.02526 (16), -0.02494 (16), -0.02473 (16), -0.02457 (16), \nlength of domains: 354\nTotal time: 2.4119\t pickout: 0.0111\t decision: 0.2517\t get_bound: 2.0994\t add_domain: 0.0497\nAccumulated time:\t pickout: 0.0758\t decision: 3.0390\t get_bound: 15.2775\t add_domain: 0.3339\nCurrent (lb-rhs): -0.03438127040863037\n1700 domains visited\nCumulative time: 18.962740898132324\n\nbatch:  torch.Size([354, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([354, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 55] [3, 1876] [4, 329] [4, 329] [4, 329] [5, 40] [5, 40] [5, 40] [4, 251] [4, 251] \n(708, 3, 32, 32) torch.Size([708, 1, 10]) torch.Size([708, 1])\npruning_in_iteration open status: True\nratio of positive domain = 330 / 708 = 0.4661016949152542\npruning-in-iteration extra time: 0.03909659385681152\nTensors transferred: pre=84.3516M lA=22.5176M alpha=4.6062M beta=0.0149M\nThis batch time : update_bounds func: 2.0496\t prepare: 0.0342\t bound: 1.9092\t transfer: 0.0953\t finalize: 0.0103\nAccumulated time: update_bounds func: 17.3198\t prepare: 0.2462\t bound: 16.2731\t transfer: 0.7229\t finalize: 0.0730\nbatch bounding time:  2.0497376918792725\nCurrent worst splitting domains lb-rhs (depth):\n-0.03352 (17), -0.03126 (17), -0.02996 (17), -0.02949 (17), -0.02940 (17), -0.02893 (17), -0.02857 (17), -0.02701 (17), -0.02697 (17), -0.02655 (17), -0.02652 (17), -0.02643 (17), -0.02638 (17), -0.02599 (17), -0.02541 (17), -0.02507 (17), -0.02480 (17), -0.02435 (17), -0.02399 (17), -0.02387 (17), \nlength of domains: 378\nTotal time: 7.6479\t pickout: 0.0128\t decision: 5.5347\t get_bound: 2.0498\t add_domain: 0.0505\nAccumulated time:\t pickout: 0.0886\t decision: 8.5737\t get_bound: 17.3273\t add_domain: 0.3845\nCurrent (lb-rhs): -0.03351736068725586\n2030 domains visited\nCumulative time: 26.61395573616028\n\nbatch:  torch.Size([378, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([378, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 454] [5, 313] [4, 348] [5, 454] [5, 40] [3, 3316] [3, 3316] [5, 454] [5, 201] [5, 201] \n(756, 3, 32, 32) torch.Size([756, 1, 10]) torch.Size([756, 1])\npruning_in_iteration open status: True\nratio of positive domain = 334 / 756 = 0.44179894179894175\npruning-in-iteration extra time: 0.03824281692504883\nTensors transferred: pre=90.0703M lA=25.1387M alpha=4.9185M beta=0.0173M\nThis batch time : update_bounds func: 2.3349\t prepare: 0.0371\t bound: 2.1738\t transfer: 0.1112\t finalize: 0.0121\nAccumulated time: update_bounds func: 19.6547\t prepare: 0.2833\t bound: 18.4468\t transfer: 0.8342\t finalize: 0.0851\nbatch bounding time:  2.3350956439971924\nCurrent worst splitting domains lb-rhs (depth):\n-0.03262 (18), -0.03036 (18), -0.02908 (18), -0.02860 (18), -0.02845 (18), -0.02803 (18), -0.02768 (18), -0.02606 (18), -0.02603 (18), -0.02568 (18), -0.02562 (18), -0.02549 (18), -0.02546 (18), -0.02513 (18), -0.02448 (18), -0.02417 (18), -0.02393 (18), -0.02345 (18), -0.02310 (18), -0.02309 (18), \nlength of domains: 422\nTotal time: 8.0898\t pickout: 0.0138\t decision: 5.6381\t get_bound: 2.3352\t add_domain: 0.1027\nAccumulated time:\t pickout: 0.1024\t decision: 14.2118\t get_bound: 19.6625\t add_domain: 0.4872\nCurrent (lb-rhs): -0.032624244689941406\n2364 domains visited\nCumulative time: 34.70693016052246\n\nbatch:  torch.Size([422, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([422, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 313] [5, 40] [5, 40] [3, 3316] [5, 454] [5, 454] [3, 3316] [3, 3316] [3, 3316] [4, 329] \n(844, 3, 32, 32) torch.Size([844, 1, 10]) torch.Size([844, 1])\npruning_in_iteration open status: True\nratio of positive domain = 382 / 844 = 0.45260663507109\npruning-in-iteration extra time: 0.05552816390991211\nTensors transferred: pre=100.5547M lA=27.5811M alpha=5.4910M beta=0.0209M\nThis batch time : update_bounds func: 2.6168\t prepare: 0.0610\t bound: 2.3486\t transfer: 0.1790\t finalize: 0.0273\nAccumulated time: update_bounds func: 22.2715\t prepare: 0.3443\t bound: 20.7954\t transfer: 1.0132\t finalize: 0.1125\nbatch bounding time:  2.6170737743377686\nCurrent worst splitting domains lb-rhs (depth):\n-0.03182 (19), -0.02933 (19), -0.02840 (19), -0.02830 (19), -0.02774 (19), -0.02761 (19), -0.02717 (19), -0.02689 (19), -0.02522 (19), -0.02515 (19), -0.02473 (19), -0.02463 (19), -0.02462 (19), -0.02461 (19), -0.02428 (19), -0.02392 (19), -0.02389 (19), -0.02369 (19), -0.02360 (19), -0.02323 (19), \nlength of domains: 462\nTotal time: 8.8668\t pickout: 0.0152\t decision: 6.1353\t get_bound: 2.6172\t add_domain: 0.0991\nAccumulated time:\t pickout: 0.1176\t decision: 20.3471\t get_bound: 22.2797\t add_domain: 0.5863\nCurrent (lb-rhs): -0.03182196617126465\n2746 domains visited\nCumulative time: 43.579970598220825\n\nbatch:  torch.Size([462, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([462, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 124] [5, 454] [3, 1876] [3, 1876] [3, 1876] [4, 329] [4, 329] [3, 1876] [5, 454] [5, 40] \n(924, 3, 32, 32) torch.Size([924, 1, 10]) torch.Size([924, 1])\npruning_in_iteration open status: True\nratio of positive domain = 377 / 924 = 0.40800865800865804\npruning-in-iteration extra time: 0.05311870574951172\nTensors transferred: pre=110.0859M lA=32.5850M alpha=6.0115M beta=0.0238M\nThis batch time : update_bounds func: 7.0184\t prepare: 0.0448\t bound: 6.8070\t transfer: 0.1520\t finalize: 0.0138\nAccumulated time: update_bounds func: 29.2898\t prepare: 0.3890\t bound: 27.6024\t transfer: 1.1652\t finalize: 0.1263\nbatch bounding time:  7.018716812133789\nCurrent worst splitting domains lb-rhs (depth):\n-0.03073 (20), -0.03011 (20), -0.02852 (20), -0.02757 (20), -0.02726 (20), -0.02686 (20), -0.02685 (20), -0.02654 (20), -0.02630 (20), -0.02599 (20), -0.02598 (20), -0.02500 (20), -0.02438 (20), -0.02394 (20), -0.02393 (20), -0.02375 (20), -0.02365 (20), -0.02356 (20), -0.02354 (20), -0.02328 (20), \nlength of domains: 547\nTotal time: 13.8408\t pickout: 0.0178\t decision: 6.7146\t get_bound: 7.0188\t add_domain: 0.0896\nAccumulated time:\t pickout: 0.1354\t decision: 27.0617\t get_bound: 29.2985\t add_domain: 0.6759\nCurrent (lb-rhs): -0.030732393264770508\n3123 domains visited\nCumulative time: 57.425203800201416\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 454] [3, 1876] [3, 1876] [3, 3627] [4, 329] [5, 96] [5, 96] [4, 329] [4, 329] [5, 96] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 448 / 1024 = 0.4375\npruning-in-iteration extra time: 0.04432415962219238\nTensors transferred: pre=122.0000M lA=34.3125M alpha=6.6621M beta=0.0273M\nThis batch time : update_bounds func: 3.3077\t prepare: 0.0507\t bound: 3.0784\t transfer: 0.1617\t finalize: 0.0161\nAccumulated time: update_bounds func: 32.5975\t prepare: 0.4397\t bound: 30.6808\t transfer: 1.3268\t finalize: 0.1423\nbatch bounding time:  3.307993173599243\nCurrent worst splitting domains lb-rhs (depth):\n-0.02996 (21), -0.02933 (21), -0.02777 (21), -0.02681 (21), -0.02654 (21), -0.02613 (21), -0.02600 (21), -0.02574 (21), -0.02541 (21), -0.02520 (21), -0.02516 (21), -0.02418 (21), -0.02382 (21), -0.02315 (21), -0.02306 (21), -0.02291 (21), -0.02283 (21), -0.02276 (21), -0.02266 (21), -0.02249 (21), \nlength of domains: 611\nTotal time: 11.7058\t pickout: 0.0650\t decision: 8.2382\t get_bound: 3.3081\t add_domain: 0.0945\nAccumulated time:\t pickout: 0.2004\t decision: 35.2999\t get_bound: 32.6066\t add_domain: 0.7704\nCurrent (lb-rhs): -0.029959678649902344\n3571 domains visited\nCumulative time: 69.13577795028687\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 96] [5, 96] [3, 1876] [5, 96] [4, 55] [3, 3294] [3, 1876] [3, 1876] [4, 55] [5, 96] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 376 / 1024 = 0.3671875\npruning-in-iteration extra time: 0.03938698768615723\nTensors transferred: pre=122.0000M lA=38.6016M alpha=6.6621M beta=0.0273M\nThis batch time : update_bounds func: 3.1800\t prepare: 0.0602\t bound: 2.7260\t transfer: 0.3283\t finalize: 0.0646\nAccumulated time: update_bounds func: 35.7775\t prepare: 0.4999\t bound: 33.4068\t transfer: 1.6551\t finalize: 0.2069\nbatch bounding time:  3.1805145740509033\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02860 (22), -0.02777 (21), -0.02614 (22), -0.02582 (22), -0.02539 (22), -0.02521 (22), -0.02498 (22), -0.02472 (22), -0.02461 (22), -0.02451 (22), -0.02439 (22), -0.02414 (22), -0.02355 (22), -0.02347 (22), -0.02311 (22), -0.02242 (22), -0.02225 (22), -0.02221 (22), -0.02206 (22), \nlength of domains: 747\nTotal time: 10.8018\t pickout: 0.0178\t decision: 7.4898\t get_bound: 3.1806\t add_domain: 0.1136\nAccumulated time:\t pickout: 0.2182\t decision: 42.7897\t get_bound: 35.7872\t add_domain: 0.8840\nCurrent (lb-rhs): -0.029248714447021484\n3947 domains visited\nCumulative time: 79.94262409210205\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1883] [5, 96] [5, 124] [5, 313] [3, 3316] [5, 96] [5, 454] [3, 3316] [3, 1883] [3, 1883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 358 / 1024 = 0.349609375\npruning-in-iteration extra time: 0.03836488723754883\nTensors transferred: pre=122.0000M lA=39.6738M alpha=6.6621M beta=0.0283M\nThis batch time : update_bounds func: 2.9991\t prepare: 0.0506\t bound: 2.7348\t transfer: 0.1975\t finalize: 0.0153\nAccumulated time: update_bounds func: 38.7766\t prepare: 0.5506\t bound: 36.1416\t transfer: 1.8526\t finalize: 0.2222\nbatch bounding time:  2.999394178390503\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02792 (23), -0.02777 (21), -0.02660 (23), -0.02582 (22), -0.02531 (23), -0.02498 (22), -0.02467 (23), -0.02451 (22), -0.02451 (23), -0.02447 (23), -0.02414 (22), -0.02389 (23), -0.02386 (23), -0.02370 (23), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (23), -0.02284 (23), \nlength of domains: 901\nTotal time: 10.8301\t pickout: 0.0183\t decision: 7.7069\t get_bound: 2.9995\t add_domain: 0.1054\nAccumulated time:\t pickout: 0.2365\t decision: 50.4966\t get_bound: 38.7867\t add_domain: 0.9894\nCurrent (lb-rhs): -0.029248714447021484\n4305 domains visited\nCumulative time: 90.77866435050964\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 96] [3, 1883] [3, 1883] [3, 1883] [5, 454] [5, 454] [3, 1883] [5, 454] [5, 40] [5, 454] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 294 / 1024 = 0.287109375\npruning-in-iteration extra time: 0.03925061225891113\nTensors transferred: pre=122.0000M lA=43.4863M alpha=6.6621M beta=0.0283M\nThis batch time : update_bounds func: 3.2524\t prepare: 0.0505\t bound: 3.0264\t transfer: 0.1588\t finalize: 0.0158\nAccumulated time: update_bounds func: 42.0289\t prepare: 0.6011\t bound: 39.1680\t transfer: 2.0114\t finalize: 0.2380\nbatch bounding time:  3.2527027130126953\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02711 (24), -0.02660 (23), -0.02616 (24), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02397 (24), -0.02389 (23), -0.02386 (24), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02307 (24), -0.02293 (24), \nlength of domains: 1119\nTotal time: 11.5907\t pickout: 0.0180\t decision: 7.6066\t get_bound: 3.2528\t add_domain: 0.7132\nAccumulated time:\t pickout: 0.2545\t decision: 58.1033\t get_bound: 42.0394\t add_domain: 1.7026\nCurrent (lb-rhs): -0.029248714447021484\n4599 domains visited\nCumulative time: 102.3750352859497\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 40] [3, 1258] [5, 40] [5, 40] [5, 40] [3, 3051] [5, 454] [5, 40] [5, 40] [4, 191] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 351 / 1024 = 0.3427734375\npruning-in-iteration extra time: 0.03952217102050781\nTensors transferred: pre=122.0000M lA=40.0908M alpha=6.6621M beta=0.0303M\nThis batch time : update_bounds func: 3.0686\t prepare: 0.0492\t bound: 2.7683\t transfer: 0.2334\t finalize: 0.0168\nAccumulated time: update_bounds func: 45.0976\t prepare: 0.6503\t bound: 41.9363\t transfer: 2.2447\t finalize: 0.2548\nbatch bounding time:  3.068965196609497\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02563 (25), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02349 (25), -0.02327 (25), -0.02311 (22), -0.02309 (24), -0.02258 (25), -0.02238 (25), \nlength of domains: 1280\nTotal time: 10.8665\t pickout: 0.0192\t decision: 7.6667\t get_bound: 3.0691\t add_domain: 0.1116\nAccumulated time:\t pickout: 0.2738\t decision: 65.7699\t get_bound: 45.1085\t add_domain: 1.8142\nCurrent (lb-rhs): -0.029248714447021484\n4950 domains visited\nCumulative time: 113.24623847007751\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1883] [3, 1868] [3, 3294] [4, 191] [3, 1667] [3, 3051] [3, 1883] [3, 3051] [4, 191] [5, 454] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 364 / 1024 = 0.35546875\npruning-in-iteration extra time: 0.04968667030334473\nTensors transferred: pre=122.0000M lA=39.3164M alpha=6.6621M beta=0.0293M\nThis batch time : update_bounds func: 3.6736\t prepare: 0.0518\t bound: 3.2854\t transfer: 0.3181\t finalize: 0.0170\nAccumulated time: update_bounds func: 48.7711\t prepare: 0.7021\t bound: 45.2217\t transfer: 2.5629\t finalize: 0.2718\nbatch bounding time:  3.673936367034912\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02513 (26), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02301 (26), -0.02275 (26), -0.02230 (23), -0.02202 (26), \nlength of domains: 1428\nTotal time: 11.3897\t pickout: 0.0183\t decision: 7.5722\t get_bound: 3.6740\t add_domain: 0.1252\nAccumulated time:\t pickout: 0.2921\t decision: 73.3421\t get_bound: 48.7825\t add_domain: 1.9394\nCurrent (lb-rhs): -0.029248714447021484\n5314 domains visited\nCumulative time: 124.64009928703308\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 313] [5, 40] [5, 124] [5, 454] [5, 124] [3, 670] [3, 1883] [5, 124] [5, 124] [5, 124] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 346 / 1024 = 0.337890625\npruning-in-iteration extra time: 0.043259382247924805\nTensors transferred: pre=122.0000M lA=40.3887M alpha=6.6621M beta=0.0312M\nThis batch time : update_bounds func: 3.7483\t prepare: 0.0532\t bound: 3.4025\t transfer: 0.2685\t finalize: 0.0232\nAccumulated time: update_bounds func: 52.5194\t prepare: 0.7553\t bound: 48.6242\t transfer: 2.8313\t finalize: 0.2950\nbatch bounding time:  3.748643636703491\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02463 (27), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02275 (27), -0.02251 (27), -0.02230 (23), -0.02227 (27), \nlength of domains: 1594\nTotal time: 11.6773\t pickout: 0.0205\t decision: 7.6589\t get_bound: 3.7487\t add_domain: 0.2491\nAccumulated time:\t pickout: 0.3126\t decision: 81.0010\t get_bound: 52.5313\t add_domain: 2.1885\nCurrent (lb-rhs): -0.029248714447021484\n5660 domains visited\nCumulative time: 136.32260584831238\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1667] [5, 124] [3, 1667] [3, 1883] [3, 1883] [3, 1883] [5, 313] [3, 1883] [3, 1868] [3, 1883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 297 / 1024 = 0.2900390625\npruning-in-iteration extra time: 0.03984808921813965\nTensors transferred: pre=122.0000M lA=43.3076M alpha=6.6621M beta=0.0332M\nThis batch time : update_bounds func: 3.4264\t prepare: 0.0533\t bound: 3.0313\t transfer: 0.3236\t finalize: 0.0173\nAccumulated time: update_bounds func: 55.9458\t prepare: 0.8086\t bound: 51.6555\t transfer: 3.1549\t finalize: 0.3123\nbatch bounding time:  3.4267373085021973\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02415 (28), -0.02414 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02288 (28), -0.02230 (23), -0.02224 (28), -0.02205 (28), \nlength of domains: 1809\nTotal time: 11.2331\t pickout: 0.0190\t decision: 7.6603\t get_bound: 3.4268\t add_domain: 0.1270\nAccumulated time:\t pickout: 0.3315\t decision: 88.6613\t get_bound: 55.9581\t add_domain: 2.3156\nCurrent (lb-rhs): -0.029248714447021484\n5957 domains visited\nCumulative time: 147.5598108768463\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 124] [3, 3294] [3, 1883] [5, 124] [3, 670] [5, 313] [3, 654] [5, 124] [3, 1876] [4, 191] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 353 / 1024 = 0.3447265625\npruning-in-iteration extra time: 0.040006160736083984\nTensors transferred: pre=122.0000M lA=40.0312M alpha=6.6621M beta=0.0352M\nThis batch time : update_bounds func: 3.2487\t prepare: 0.0529\t bound: 2.9558\t transfer: 0.2221\t finalize: 0.0169\nAccumulated time: update_bounds func: 59.1945\t prepare: 0.8615\t bound: 54.6113\t transfer: 3.3771\t finalize: 0.3292\nbatch bounding time:  3.2489898204803467\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02414 (22), -0.02389 (23), -0.02370 (29), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02241 (29), -0.02230 (23), -0.02198 (22), -0.02183 (28), \nlength of domains: 1968\nTotal time: 10.9609\t pickout: 0.0179\t decision: 7.5714\t get_bound: 3.2491\t add_domain: 0.1226\nAccumulated time:\t pickout: 0.3494\t decision: 96.2327\t get_bound: 59.2071\t add_domain: 2.4382\nCurrent (lb-rhs): -0.029248714447021484\n6310 domains visited\nCumulative time: 158.52485418319702\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 313] [5, 313] [4, 191] [3, 654] [5, 313] [2, 14023] [3, 1667] [3, 1883] [5, 313] [3, 681] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 248 / 1024 = 0.2421875\npruning-in-iteration extra time: 0.03499746322631836\nTensors transferred: pre=122.0000M lA=46.2861M alpha=6.6621M beta=0.0381M\nThis batch time : update_bounds func: 3.4823\t prepare: 0.0545\t bound: 3.1396\t transfer: 0.2705\t finalize: 0.0169\nAccumulated time: update_bounds func: 62.6768\t prepare: 0.9160\t bound: 57.7508\t transfer: 3.6476\t finalize: 0.3461\nbatch bounding time:  3.482748508453369\nTraceback (most recent call last):\n  File \"abcrown.py\", line 647, in <module>\n    main()\n  File \"abcrown.py\", line 570, in main\n    refined_betas=refined_betas, attack_images=all_adv_candidates, attack_margins=attack_margins)\n  File \"abcrown.py\", line 392, in complete_verifier\n    attack_images=this_spec_attack_images)\n  File \"abcrown.py\", line 206, in bab\n    timeout=timeout, refined_betas=refined_betas, rhs=rhs)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py\", line 561, in relu_bab_parallel\n    stop_func=stop_criterion, multi_spec_keep_func=multi_spec_keep_func)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py\", line 283, in batch_verification\n    branching_decision, rhs, intermediate_betas, check_infeasibility, dom_cs, (2*num_copy)*batch)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/branching_domains.py\", line 536, in add\n    [lb.append(new_lb[right_indexer + batch]) if new_lb is not None else None for lb, new_lb in zip(self.all_lb_alls, lb_alls)]\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/branching_domains.py\", line 536, in <listcomp>\n    [lb.append(new_lb[right_indexer + batch]) if new_lb is not None else None for lb, new_lb in zip(self.all_lb_alls, lb_alls)]\n  File \"/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/tensor_storage.py\", line 70, in append\n    new_tensor = self._allocate(new_size)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/tensor_storage.py\", line 51, in _allocate\n    return torch.empty(allocate_shape, dtype=self.dtype, device=self.device, pin_memory=True)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_88_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "8.021779298782349",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpsqdieu68.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:19:37 2024 on Cerberus\nInternal results will be saved to /tmp/tmpsqdieu68.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.39393386, -0.71249902, -1.62325943, -1.73206186, -0.67567652,\n         -2.47843599, -3.07558155, -1.90466976,  2.34261847, -0.12327250]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.49835101, -0.59551758, -1.62404847, -1.84851956, -0.67073911,\n          -2.59058094, -3.11322832, -1.92009211,  2.25205374, -0.12188021],\n         [ 0.49835101, -0.59551758, -1.62404847, -1.84851956, -0.67073911,\n          -2.59058094, -3.11322832, -1.92009211,  2.25205374, -0.12188021]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.75370276, 2.84757137, 3.87610221, 4.10057354, 2.92279291,\n          4.84263468, 5.36528206, 4.17214584, 2.37393403]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.9237 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.39393386, -0.71249902, -1.62325943, -1.73206186, -0.67567652,\n         -2.47843599, -3.07558155, -1.90466976,  2.34261847, -0.12327250]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2270]; unstable size 2270; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 82 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [82]; unstable size 82; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1004]; unstable size 1004; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [71]; unstable size 71; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [35]; unstable size 35; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [68]; unstable size 68; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[1.14224148, 1.87831450, 2.95792007, 3.11696887, 1.87635255, 3.81624699,\n         4.26846170, 2.82570076, 1.48256826]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 6.354446887969971\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_97_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.546961069107056",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp_ao2swxl.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:19:46 2024 on Cerberus\nInternal results will be saved to /tmp/tmp_ao2swxl.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.96105886, -4.99818325,  0.49796951, -1.34900188,  0.09806482,\n         -1.69785976, -2.05051923, -2.63068223,  0.86458349, -4.01851368]],\n       device='cuda:0')\npgd early stop\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.90111589, -5.00151634,  0.45460641, -1.36289418,  0.06061684,\n          -1.70949721, -2.10207391, -2.64322639,  0.98923618, -3.94865346],\n         [ 0.90111589, -5.00151634,  0.45460641, -1.36289418,  0.06061684,\n          -1.70949721, -2.10207391, -2.64322639,  0.98923618, -3.94865346]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.90263224,  0.44650948,  2.26400995,  0.84049904,  2.61061311,\n           3.00318980,  3.54434228, -0.08812028,  4.84976959]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.0573 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.025995492935181\n"
        }
    ]
}