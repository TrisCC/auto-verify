{
    "instances": [
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_0_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "7.114942789077759",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpbf3d3qw5.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 22:57:30 2024 on Cerberus\nInternal results will be saved to /tmp/tmpbf3d3qw5.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_0_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.67527163, -1.52271450,  0.63711810,  4.41396284,  0.79158354,\n          3.94307709,  1.38516212, -1.23928893, -1.21486461, -1.90701199]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.59891230, -1.67729628,  0.56920362,  4.30161333,  0.84799862,\n           4.37968922,  1.18421865, -1.15600204, -1.47534275, -1.94664717],\n         [-0.59891230, -1.67729628,  0.56920362,  4.30161333,  0.84799862,\n           4.37968922,  1.18421865, -1.15600204, -1.47534275, -1.94664717]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 4.90052557,  5.97890949,  3.73240972,  3.45361471, -0.07807589,\n           3.11739469,  5.45761538,  5.77695608,  6.24826050]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.9718 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 5.544947385787964\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_9_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "14.727089405059814",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpi5g3cvz4.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 22:57:38 2024 on Cerberus\nInternal results will be saved to /tmp/tmpi5g3cvz4.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_9_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n         -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.38077497,  7.11155891, -0.81550092,  0.36877590, -1.63488841,\n          -0.19252896,  0.41075173, -2.10415339,  2.38559103,  6.26932716],\n         [ 1.38077497,  7.11155891, -0.81550092,  0.36877590, -1.63488841,\n          -0.19252896,  0.41075173, -2.10415339,  2.38559103,  6.26932716]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.73078394, 7.92705965, 6.74278307, 8.74644756, 7.30408764,\n          6.70080709, 9.21571255, 4.72596788, 0.84223175]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.8956 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n         -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1386]; unstable size 1386; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [725]; unstable size 725; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [675]; unstable size 675; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.91427326,  4.55402946,  4.00052071,  5.54376221,  4.45263481,\n          3.79798222,  5.70426178,  1.83534813, -0.87034893]], device='cuda:0') None\nbest_l after optimization: 35.21476745605469 with beta sum per layer: []\nalpha/beta optimization time: 5.784532785415649\ninitial alpha-CROWN bounds: tensor([[ 3.25364685,  4.91604519,  4.39051914,  5.89016628,  4.82440948,\n          4.12997246,  6.11451817,  2.23822498, -0.54273653]], device='cuda:0')\nWorst class: (+ rhs) -0.5427365303039551\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 288.617680311203\n##### Instance 0 first 10 spec matrices: [[[-1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.2536468505859375.\n\nProperties batch 1, size 1\nRemaining timeout: 288.57490944862366\n##### Instance 0 first 10 spec matrices: [[[ 0.  1. -1.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.916045188903809.\n\nProperties batch 2, size 1\nRemaining timeout: 288.538419008255\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0. -1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.390519142150879.\n\nProperties batch 3, size 1\nRemaining timeout: 288.5057747364044\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 5.890166282653809.\n\nProperties batch 4, size 1\nRemaining timeout: 288.4803628921509\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0. -1.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.824409484863281.\n\nProperties batch 5, size 1\nRemaining timeout: 288.45500922203064\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0. -1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.129972457885742.\n\nProperties batch 6, size 1\nRemaining timeout: 288.4279041290283\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0. -1.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 6.114518165588379.\n\nProperties batch 7, size 1\nRemaining timeout: 288.402117729187\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0.  0. -1.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.238224983215332.\n\nProperties batch 8, size 1\nRemaining timeout: 288.37392020225525\n##### Instance 0 first 10 spec matrices: [[[ 0.  1.  0.  0.  0.  0.  0.  0.  0. -1.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.54273653]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 1.33182096,  8.01652718, -0.83784550,  0.33827630, -1.59518743,\n        -0.11049002,  0.60660005, -2.18435478,  2.48944187,  5.76314831],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.54273653]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1386\nlayer 1 size torch.Size([8192]) unstable 713\nlayer 2 size torch.Size([8192]) unstable 658\nlayer 3 size torch.Size([250]) unstable 51\n-----------------\n# of unstable neurons: 2808\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 56] \nsplit level 1: [3, 27] \nsplit level 2: [3, 169] \nsplit level 3: [3, 203] \nsplit level 4: [3, 43] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 30 / 32 = 0.9375\npruning-in-iteration extra time: 0.03116464614868164\nTensors transferred: pre=3.0153M lA=0.0942M alpha=0.1733M beta=0.0002M\nThis batch time : update_bounds func: 0.7639\t prepare: 0.0029\t bound: 0.7550\t transfer: 0.0052\t finalize: 0.0007\nAccumulated time: update_bounds func: 0.7639\t prepare: 0.0029\t bound: 0.7550\t transfer: 0.0052\t finalize: 0.0007\nbatch bounding time:  0.7639663219451904\nCurrent worst splitting domains lb-rhs (depth):\n-0.04987 (5), -0.02502 (5), \nlength of domains: 2\nTotal time: 1.0210\t pickout: 0.0009\t decision: 0.2509\t get_bound: 0.7679\t add_domain: 0.0013\nAccumulated time:\t pickout: 0.0009\t decision: 0.2509\t get_bound: 0.7679\t add_domain: 0.0013\nCurrent (lb-rhs): -0.04987049102783203\n30 domains visited\nCumulative time: 1.260392665863037\n\nbatch:  torch.Size([2, 32, 32, 32]) pre split depth:  4\nbatch:  torch.Size([2, 32, 32, 32]) post split depth:  4\nsplitting decisions: \nsplit level 0: [3, 40] [3, 40] \nsplit level 1: [3, 112] [3, 112] \nsplit level 2: [3, 207] [3, 207] \nsplit level 3: [3, 116] [3, 116] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 32 / 32 = 1.0\npruning-in-iteration extra time: 0.00016880035400390625\nTensors transferred: pre=3.0153M lA=1.5076M alpha=0.1733M beta=0.0003M\nThis batch time : update_bounds func: 0.0177\t prepare: 0.0026\t bound: 0.0079\t transfer: 0.0066\t finalize: 0.0006\nAccumulated time: update_bounds func: 0.7816\t prepare: 0.0055\t bound: 0.7629\t transfer: 0.0118\t finalize: 0.0013\nbatch bounding time:  0.01781773567199707\nlength of domains: 0\nTotal time: 0.0459\t pickout: 0.0007\t decision: 0.0222\t get_bound: 0.0222\t add_domain: 0.0007\nAccumulated time:\t pickout: 0.0017\t decision: 0.2731\t get_bound: 0.7902\t add_domain: 0.0019\nNo domains left, verification finished!\n62 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 1.3068971633911133\n\nResult: unsat\nTime: 13.01458477973938\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_17_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.946368932723999",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmps7bp3qoj.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 22:57:54 2024 on Cerberus\nInternal results will be saved to /tmp/tmps7bp3qoj.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_17_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.71794301, -2.73549056,  0.46490240,  2.03653789,  2.18022227,\n          1.30874860, -0.23802117,  5.58075476, -1.34880483, -1.73270965]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.48575890, -2.88859630,  0.55571985,  2.02314472,  2.39839435,\n           1.23517108, -0.19022425,  4.85715294, -1.25886095, -1.80295110],\n         [-0.48575890, -2.88859630,  0.55571985,  2.02314472,  2.39839435,\n           1.23517108, -0.19022425,  4.85715294, -1.25886095, -1.80295110]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.34291172, 7.74574947, 4.30143309, 2.83400822, 2.45875859,\n          3.62198186, 5.04737711, 6.11601400, 6.66010380]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.7624 seconds.\nPGD attack failed\nModel prediction is: tensor([[-0.71794301, -2.73549056,  0.46490240,  2.03653789,  2.18022227,\n          1.30874860, -0.23802117,  5.58075476, -1.34880483, -1.73270965]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1626]; unstable size 1626; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [939]; unstable size 939; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [715]; unstable size 715; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [67]; unstable size 67; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[2.33360076, 3.98111701, 2.04257345, 1.23024130, 1.41948557, 1.87243688,\n         3.18333197, 3.42235136, 2.19060946]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.308523416519165\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_27_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "TIMEOUT",
            "took": "300",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp5rttag37.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 22:58:03 2024 on Cerberus\nInternal results will be saved to /tmp/tmp5rttag37.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_27_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n          0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 2.89373541, -4.66593504,  1.82701683,  0.85894787,  2.86403251,\n           0.92236614, -0.28302976,  1.52670109, -3.56230187, -0.78883207],\n         [ 2.89373541, -4.66593504,  1.82701683,  0.85894787,  2.86403251,\n           0.92236614, -0.28302976,  1.52670109, -3.56230187, -0.78883207]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[7.55967045, 1.06671858, 2.03478765, 0.02970290, 1.97136927,\n          3.17676520, 1.36703432, 6.45603752, 3.68256760]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.7135 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n          0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1437]; unstable size 1437; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [675]; unstable size 675; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 115 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [555]; unstable size 555; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 5.91306973, -0.15005422,  0.85581970, -0.97932053,  0.71283913,\n          1.95830774, -0.64448214,  4.80564022,  1.95628190]], device='cuda:0') None\nbest_l after optimization: 15.584518432617188 with beta sum per layer: []\nalpha/beta optimization time: 5.650400400161743\ninitial alpha-CROWN bounds: tensor([[ 6.05828190, -0.03196883,  0.96516728, -0.86231685,  0.83091259,\n          2.05264425, -0.49636269,  4.94511318,  2.12304664]], device='cuda:0')\nWorst class: (+ rhs) -0.8623168468475342\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 288.9049639701843\n##### Instance 0 first 10 spec matrices: [[[ 1. -1.  0.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 6.058281898498535.\n\nProperties batch 1, size 1\nRemaining timeout: 288.8693177700043\n##### Instance 0 first 10 spec matrices: [[[ 1.  0. -1.  0.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.03196883]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n         0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.03196883]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1437\nlayer 1 size torch.Size([8192]) unstable 665\nlayer 2 size torch.Size([8192]) unstable 541\nlayer 3 size torch.Size([250]) unstable 53\n-----------------\n# of unstable neurons: 2696\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 123] \nsplit level 1: [3, 247] \nsplit level 2: [3, 248] \nsplit level 3: [3, 126] \nsplit level 4: [3, 55] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 32 / 32 = 1.0\npruning-in-iteration extra time: 0.0002551078796386719\nTensors transferred: pre=3.0153M lA=1.5076M alpha=0.1661M beta=0.0002M\nThis batch time : update_bounds func: 0.0236\t prepare: 0.0026\t bound: 0.0104\t transfer: 0.0098\t finalize: 0.0008\nAccumulated time: update_bounds func: 0.0236\t prepare: 0.0026\t bound: 0.0104\t transfer: 0.0098\t finalize: 0.0008\nbatch bounding time:  0.023630619049072266\nlength of domains: 0\nTotal time: 0.2652\t pickout: 0.0014\t decision: 0.2358\t get_bound: 0.0269\t add_domain: 0.0010\nAccumulated time:\t pickout: 0.0014\t decision: 0.2358\t get_bound: 0.0269\t add_domain: 0.0010\nNo domains left, verification finished!\n32 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 0.49391746520996094\n\n\nProperties batch 2, size 1\nRemaining timeout: 288.294527053833\n##### Instance 0 first 10 spec matrices: [[[ 1.  0.  0. -1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.9651672840118408.\n\nProperties batch 3, size 1\nRemaining timeout: 288.2522258758545\n##### Instance 0 first 10 spec matrices: [[[ 1.  0.  0.  0. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.86231685]], device='cuda:0') need to verify.\nModel prediction is: tensor([ 3.42392373, -4.39966679,  1.84774685,  0.68908328,  2.66574335,\n         0.62991524, -0.17185465,  1.23229563, -3.23374295, -0.60079861],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.86231685]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1437\nlayer 1 size torch.Size([8192]) unstable 665\nlayer 2 size torch.Size([8192]) unstable 541\nlayer 3 size torch.Size([250]) unstable 53\n-----------------\n# of unstable neurons: 2696\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 134] \nsplit level 1: [3, 247] \nsplit level 2: [3, 123] \nsplit level 3: [3, 71] \nsplit level 4: [3, 52] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 11 / 32 = 0.34375\npruning-in-iteration extra time: 0.022922515869140625\nTensors transferred: pre=3.0153M lA=0.9894M alpha=0.1661M beta=0.0002M\nThis batch time : update_bounds func: 0.7327\t prepare: 0.0026\t bound: 0.7250\t transfer: 0.0041\t finalize: 0.0009\nAccumulated time: update_bounds func: 0.7562\t prepare: 0.0052\t bound: 0.7354\t transfer: 0.0139\t finalize: 0.0016\nbatch bounding time:  0.73276686668396\nCurrent worst splitting domains lb-rhs (depth):\n-0.51872 (5), -0.50867 (5), -0.47914 (5), -0.47639 (5), -0.46053 (5), -0.44768 (5), -0.40703 (5), -0.39984 (5), -0.26980 (5), -0.25875 (5), -0.25381 (5), -0.24612 (5), -0.24009 (5), -0.22822 (5), -0.22390 (5), -0.21487 (5), -0.19789 (5), -0.18600 (5), -0.16737 (5), -0.14893 (5), \nlength of domains: 21\nTotal time: 0.7697\t pickout: 0.0010\t decision: 0.0277\t get_bound: 0.7362\t add_domain: 0.0049\nAccumulated time:\t pickout: 0.0010\t decision: 0.0277\t get_bound: 0.7362\t add_domain: 0.0049\nCurrent (lb-rhs): -0.5187230110168457\n11 domains visited\nCumulative time: 0.776522159576416\n\nbatch:  torch.Size([21, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([21, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] [3, 214] \n(42, 3, 32, 32) torch.Size([42, 1, 10]) torch.Size([42, 1])\npruning_in_iteration open status: True\nratio of positive domain = 14 / 42 = 0.33333333333333337\npruning-in-iteration extra time: 0.023227930068969727\nTensors transferred: pre=3.9575M lA=1.3192M alpha=0.2180M beta=0.0002M\nThis batch time : update_bounds func: 0.5730\t prepare: 0.0025\t bound: 0.5661\t transfer: 0.0038\t finalize: 0.0005\nAccumulated time: update_bounds func: 1.3292\t prepare: 0.0077\t bound: 1.3014\t transfer: 0.0177\t finalize: 0.0021\nbatch bounding time:  0.5730586051940918\nCurrent worst splitting domains lb-rhs (depth):\n-0.47139 (6), -0.46284 (6), -0.43350 (6), -0.43137 (6), -0.41344 (6), -0.40248 (6), -0.36403 (6), -0.36149 (6), -0.35578 (6), -0.33286 (6), -0.29661 (6), -0.28733 (6), -0.27547 (6), -0.24269 (6), -0.22407 (6), -0.21333 (6), -0.20804 (6), -0.19957 (6), -0.19530 (6), -0.19490 (6), \nlength of domains: 28\nTotal time: 0.6051\t pickout: 0.0012\t decision: 0.0258\t get_bound: 0.5731\t add_domain: 0.0050\nAccumulated time:\t pickout: 0.0021\t decision: 0.0536\t get_bound: 1.3093\t add_domain: 0.0098\nCurrent (lb-rhs): -0.4713866710662842\n25 domains visited\nCumulative time: 1.3819999694824219\n\nbatch:  torch.Size([28, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([28, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] [3, 248] \n(56, 3, 32, 32) torch.Size([56, 1, 10]) torch.Size([56, 1])\npruning_in_iteration open status: True\nratio of positive domain = 12 / 56 = 0.2142857142857143\npruning-in-iteration extra time: 0.00017714500427246094\nTensors transferred: pre=5.2767M lA=2.6384M alpha=0.2906M beta=0.0004M\nThis batch time : update_bounds func: 0.4712\t prepare: 0.0025\t bound: 0.4627\t transfer: 0.0053\t finalize: 0.0007\nAccumulated time: update_bounds func: 1.8004\t prepare: 0.0103\t bound: 1.7641\t transfer: 0.0230\t finalize: 0.0028\nbatch bounding time:  0.4712378978729248\nCurrent worst splitting domains lb-rhs (depth):\n-0.43693 (7), -0.42708 (7), -0.39961 (7), -0.39623 (7), -0.37680 (7), -0.36551 (7), -0.34704 (7), -0.33570 (7), -0.32635 (7), -0.32499 (7), -0.31908 (7), -0.29468 (7), -0.28918 (7), -0.28457 (7), -0.27956 (7), -0.27038 (7), -0.25666 (7), -0.24449 (7), -0.23416 (7), -0.22628 (7), \nlength of domains: 44\nTotal time: 0.5113\t pickout: 0.0012\t decision: 0.0333\t get_bound: 0.4713\t add_domain: 0.0054\nAccumulated time:\t pickout: 0.0034\t decision: 0.0869\t get_bound: 1.7806\t add_domain: 0.0153\nCurrent (lb-rhs): -0.4369349479675293\n37 domains visited\nCumulative time: 1.893606424331665\n\nbatch:  torch.Size([44, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([44, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] [3, 195] \n(88, 3, 32, 32) torch.Size([88, 1, 10]) torch.Size([88, 1])\npruning_in_iteration open status: True\nratio of positive domain = 25 / 88 = 0.28409090909090906\npruning-in-iteration extra time: 0.020081758499145508\nTensors transferred: pre=8.2920M lA=2.9681M alpha=0.4567M beta=0.0007M\nThis batch time : update_bounds func: 0.5517\t prepare: 0.0038\t bound: 0.5370\t transfer: 0.0099\t finalize: 0.0009\nAccumulated time: update_bounds func: 2.3521\t prepare: 0.0141\t bound: 2.3011\t transfer: 0.0329\t finalize: 0.0037\nbatch bounding time:  0.5517716407775879\nCurrent worst splitting domains lb-rhs (depth):\n-0.40897 (8), -0.39871 (8), -0.37132 (8), -0.36745 (8), -0.34902 (8), -0.33765 (8), -0.31770 (8), -0.30631 (8), -0.29811 (8), -0.29695 (8), -0.29110 (8), -0.29056 (8), -0.28809 (8), -0.26677 (8), -0.26006 (8), -0.25461 (8), -0.25300 (8), -0.24933 (8), -0.24835 (8), -0.24094 (8), \nlength of domains: 63\nTotal time: 0.6001\t pickout: 0.0016\t decision: 0.0394\t get_bound: 0.5518\t add_domain: 0.0073\nAccumulated time:\t pickout: 0.0050\t decision: 0.1263\t get_bound: 2.3324\t add_domain: 0.0225\nCurrent (lb-rhs): -0.4089667797088623\n62 domains visited\nCumulative time: 2.4940624237060547\n\nbatch:  torch.Size([63, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([63, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] [3, 45] \n(126, 3, 32, 32) torch.Size([126, 1, 10]) torch.Size([126, 1])\npruning_in_iteration open status: False\nratio of positive domain = 13 / 126 = 0.10317460317460314\npruning-in-iteration extra time: 0.00021219253540039062\nTensors transferred: pre=11.8726M lA=5.9363M alpha=0.6539M beta=0.0011M\nThis batch time : update_bounds func: 0.5607\t prepare: 0.0049\t bound: 0.5294\t transfer: 0.0249\t finalize: 0.0014\nAccumulated time: update_bounds func: 2.9129\t prepare: 0.0190\t bound: 2.8306\t transfer: 0.0578\t finalize: 0.0051\nbatch bounding time:  0.5607953071594238\nCurrent worst splitting domains lb-rhs (depth):\n-0.38090 (9), -0.37239 (9), -0.37092 (9), -0.36096 (9), -0.34137 (9), -0.33827 (9), -0.33674 (9), -0.33229 (9), -0.32070 (9), -0.31300 (9), -0.30942 (9), -0.30053 (9), -0.28946 (9), -0.27811 (9), -0.27461 (9), -0.26700 (9), -0.26687 (9), -0.26371 (9), -0.26330 (9), -0.26262 (9), \nlength of domains: 113\nTotal time: 0.6239\t pickout: 0.0020\t decision: 0.0474\t get_bound: 0.5608\t add_domain: 0.0136\nAccumulated time:\t pickout: 0.0070\t decision: 0.1737\t get_bound: 2.8932\t add_domain: 0.0361\nCurrent (lb-rhs): -0.38089632987976074\n75 domains visited\nCumulative time: 3.1187310218811035\n\nbatch:  torch.Size([113, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([113, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] [3, 83] \n(226, 3, 32, 32) torch.Size([226, 1, 10]) torch.Size([226, 1])\npruning_in_iteration open status: True\nratio of positive domain = 110 / 226 = 0.48672566371681414\npruning-in-iteration extra time: 0.03153586387634277\nTensors transferred: pre=21.2953M lA=5.4652M alpha=1.1729M beta=0.0022M\nThis batch time : update_bounds func: 0.6870\t prepare: 0.0080\t bound: 0.6542\t transfer: 0.0213\t finalize: 0.0033\nAccumulated time: update_bounds func: 3.5998\t prepare: 0.0270\t bound: 3.4847\t transfer: 0.0791\t finalize: 0.0083\nbatch bounding time:  0.6870784759521484\nCurrent worst splitting domains lb-rhs (depth):\n-0.35947 (10), -0.35085 (10), -0.34871 (10), -0.33884 (10), -0.31798 (10), -0.31492 (10), -0.31355 (10), -0.30911 (10), -0.29946 (10), -0.29200 (10), -0.28763 (10), -0.27890 (10), -0.26958 (10), -0.25749 (10), -0.25432 (10), -0.24743 (10), -0.24447 (10), -0.24396 (10), -0.23997 (10), -0.23954 (10), \nlength of domains: 116\nTotal time: 0.7837\t pickout: 0.0034\t decision: 0.0751\t get_bound: 0.6872\t add_domain: 0.0180\nAccumulated time:\t pickout: 0.0105\t decision: 0.2488\t get_bound: 3.5804\t add_domain: 0.0541\nCurrent (lb-rhs): -0.3594698905944824\n185 domains visited\nCumulative time: 3.903217077255249\n\nbatch:  torch.Size([116, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([116, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] [3, 95] \n(232, 3, 32, 32) torch.Size([232, 1, 10]) torch.Size([232, 1])\npruning_in_iteration open status: False\nratio of positive domain = 39 / 232 = 0.1681034482758621\npruning-in-iteration extra time: 0.00015926361083984375\nTensors transferred: pre=21.8606M lA=10.9303M alpha=1.2041M beta=0.0024M\nThis batch time : update_bounds func: 0.6853\t prepare: 0.0079\t bound: 0.6445\t transfer: 0.0297\t finalize: 0.0030\nAccumulated time: update_bounds func: 4.2851\t prepare: 0.0349\t bound: 4.1292\t transfer: 0.1088\t finalize: 0.0114\nbatch bounding time:  0.6854872703552246\nCurrent worst splitting domains lb-rhs (depth):\n-0.33915 (11), -0.33049 (11), -0.32864 (11), -0.32759 (11), -0.32070 (11), -0.31964 (11), -0.31778 (11), -0.30983 (11), -0.29697 (11), -0.29359 (11), -0.29238 (11), -0.28871 (11), -0.28763 (11), -0.28690 (11), -0.28415 (11), -0.28057 (11), -0.27859 (11), -0.27084 (11), -0.27044 (11), -0.26671 (11), \nlength of domains: 193\nTotal time: 0.7893\t pickout: 0.0034\t decision: 0.0693\t get_bound: 0.6856\t add_domain: 0.0309\nAccumulated time:\t pickout: 0.0139\t decision: 0.3181\t get_bound: 4.2659\t add_domain: 0.0851\nCurrent (lb-rhs): -0.3391537666320801\n224 domains visited\nCumulative time: 4.693625450134277\n\nbatch:  torch.Size([193, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([193, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] [3, 55] \n(386, 3, 32, 32) torch.Size([386, 1, 10]) torch.Size([386, 1])\npruning_in_iteration open status: True\nratio of positive domain = 190 / 386 = 0.49222797927461137\npruning-in-iteration extra time: 0.03081989288330078\nTensors transferred: pre=36.3716M lA=9.2342M alpha=2.0033M beta=0.0044M\nThis batch time : update_bounds func: 0.8420\t prepare: 0.0127\t bound: 0.7836\t transfer: 0.0383\t finalize: 0.0070\nAccumulated time: update_bounds func: 5.1271\t prepare: 0.0476\t bound: 4.9128\t transfer: 0.1471\t finalize: 0.0184\nbatch bounding time:  0.8420407772064209\nCurrent worst splitting domains lb-rhs (depth):\n-0.31952 (12), -0.31131 (12), -0.31026 (12), -0.30937 (12), -0.30279 (12), -0.30224 (12), -0.29970 (12), -0.29263 (12), -0.27878 (12), -0.27646 (12), -0.27427 (12), -0.27156 (12), -0.27051 (12), -0.27051 (12), -0.26701 (12), -0.26425 (12), -0.25998 (12), -0.25235 (12), -0.25228 (12), -0.24906 (12), \nlength of domains: 196\nTotal time: 0.9965\t pickout: 0.0062\t decision: 0.1209\t get_bound: 0.8421\t add_domain: 0.0272\nAccumulated time:\t pickout: 0.0201\t decision: 0.4390\t get_bound: 5.1081\t add_domain: 0.1123\nCurrent (lb-rhs): -0.3195216655731201\n414 domains visited\nCumulative time: 5.69124960899353\n\nbatch:  torch.Size([196, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([196, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] [3, 50] \n(392, 3, 32, 32) torch.Size([392, 1, 10]) torch.Size([392, 1])\npruning_in_iteration open status: True\nratio of positive domain = 222 / 392 = 0.5663265306122449\npruning-in-iteration extra time: 0.030319929122924805\nTensors transferred: pre=36.9369M lA=8.0093M alpha=2.0344M beta=0.0049M\nThis batch time : update_bounds func: 0.7769\t prepare: 0.0138\t bound: 0.7218\t transfer: 0.0372\t finalize: 0.0038\nAccumulated time: update_bounds func: 5.9040\t prepare: 0.0613\t bound: 5.6346\t transfer: 0.1843\t finalize: 0.0222\nbatch bounding time:  0.7770431041717529\nCurrent worst splitting domains lb-rhs (depth):\n-0.30481 (13), -0.29672 (13), -0.29490 (13), -0.29435 (13), -0.28757 (13), -0.28673 (13), -0.28491 (13), -0.27730 (13), -0.26466 (13), -0.26204 (13), -0.26015 (13), -0.25702 (13), -0.25625 (13), -0.25574 (13), -0.25263 (13), -0.24975 (13), -0.24498 (13), -0.23744 (13), -0.23678 (13), -0.23403 (13), \nlength of domains: 170\nTotal time: 0.9152\t pickout: 0.0056\t decision: 0.1074\t get_bound: 0.7771\t add_domain: 0.0251\nAccumulated time:\t pickout: 0.0257\t decision: 0.5465\t get_bound: 5.8852\t add_domain: 0.1374\nCurrent (lb-rhs): -0.3048112392425537\n636 domains visited\nCumulative time: 6.6079912185668945\n\nbatch:  torch.Size([170, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([170, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 241] [3, 1] \n(340, 3, 32, 32) torch.Size([340, 1, 10]) torch.Size([340, 1])\npruning_in_iteration open status: True\nratio of positive domain = 157 / 340 = 0.46176470588235297\npruning-in-iteration extra time: 0.032781362533569336\nTensors transferred: pre=32.0371M lA=8.6218M alpha=1.7646M beta=0.0049M\nThis batch time : update_bounds func: 0.8386\t prepare: 0.0118\t bound: 0.7817\t transfer: 0.0414\t finalize: 0.0033\nAccumulated time: update_bounds func: 6.7426\t prepare: 0.0731\t bound: 6.4164\t transfer: 0.2256\t finalize: 0.0256\nbatch bounding time:  0.8386521339416504\nCurrent worst splitting domains lb-rhs (depth):\n-0.29670 (14), -0.28881 (14), -0.28705 (14), -0.28641 (14), -0.27990 (14), -0.27885 (14), -0.27710 (14), -0.26964 (14), -0.25611 (14), -0.25369 (14), -0.25189 (14), -0.24857 (14), -0.24808 (14), -0.24733 (14), -0.24442 (14), -0.24156 (14), -0.23736 (14), -0.22968 (14), -0.22925 (14), -0.22633 (14), \nlength of domains: 183\nTotal time: 0.9620\t pickout: 0.0050\t decision: 0.0938\t get_bound: 0.8387\t add_domain: 0.0245\nAccumulated time:\t pickout: 0.0307\t decision: 0.6403\t get_bound: 6.7239\t add_domain: 0.1619\nCurrent (lb-rhs): -0.29670190811157227\n793 domains visited\nCumulative time: 7.571181297302246\n\nbatch:  torch.Size([183, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([183, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 241] [3, 241] [3, 241] [3, 241] [3, 241] [3, 241] [3, 1] [3, 241] [3, 1] [3, 1] \n(366, 3, 32, 32) torch.Size([366, 1, 10]) torch.Size([366, 1])\npruning_in_iteration open status: True\nratio of positive domain = 183 / 366 = 0.5\npruning-in-iteration extra time: 0.03263211250305176\nTensors transferred: pre=34.4870M lA=8.6689M alpha=1.8995M beta=0.0056M\nThis batch time : update_bounds func: 0.8541\t prepare: 0.0171\t bound: 0.7911\t transfer: 0.0410\t finalize: 0.0046\nAccumulated time: update_bounds func: 7.5966\t prepare: 0.0902\t bound: 7.2075\t transfer: 0.2666\t finalize: 0.0301\nbatch bounding time:  0.8541676998138428\nCurrent worst splitting domains lb-rhs (depth):\n-0.28925 (15), -0.28126 (15), -0.27959 (15), -0.27886 (15), -0.27232 (15), -0.27128 (15), -0.26941 (15), -0.26195 (15), -0.24885 (15), -0.24620 (15), -0.24452 (15), -0.24128 (15), -0.24049 (15), -0.23979 (15), -0.23700 (15), -0.23391 (15), -0.22978 (15), -0.22231 (15), -0.22158 (15), -0.21881 (15), \nlength of domains: 183\nTotal time: 0.9873\t pickout: 0.0058\t decision: 0.1022\t get_bound: 0.8542\t add_domain: 0.0250\nAccumulated time:\t pickout: 0.0365\t decision: 0.7424\t get_bound: 7.5781\t add_domain: 0.1869\nCurrent (lb-rhs): -0.28924560546875\n976 domains visited\nCumulative time: 8.559779167175293\n\nbatch:  torch.Size([183, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([183, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [2, 1206] [3, 126] [3, 126] [3, 241] [3, 241] [3, 1] [3, 1] [3, 126] [3, 126] \n(366, 3, 32, 32) torch.Size([366, 1, 10]) torch.Size([366, 1])\npruning_in_iteration open status: True\nratio of positive domain = 100 / 366 = 0.27322404371584696\npruning-in-iteration extra time: 0.03364109992980957\nTensors transferred: pre=34.4870M lA=12.5322M alpha=1.8995M beta=0.0063M\nThis batch time : update_bounds func: 1.0139\t prepare: 0.0161\t bound: 0.9289\t transfer: 0.0646\t finalize: 0.0040\nAccumulated time: update_bounds func: 8.6106\t prepare: 0.1063\t bound: 8.1364\t transfer: 0.3312\t finalize: 0.0341\nbatch bounding time:  1.014173984527588\nCurrent worst splitting domains lb-rhs (depth):\n-0.28220 (16), -0.28135 (16), -0.27413 (16), -0.27373 (16), -0.27291 (16), -0.27183 (16), -0.27069 (16), -0.26854 (16), -0.26549 (16), -0.26459 (16), -0.26269 (16), -0.26225 (16), -0.26180 (16), -0.26055 (16), -0.25507 (16), -0.25287 (16), -0.24245 (16), -0.24022 (16), -0.23833 (16), -0.23810 (16), \nlength of domains: 266\nTotal time: 1.1837\t pickout: 0.0062\t decision: 0.1244\t get_bound: 1.0143\t add_domain: 0.0388\nAccumulated time:\t pickout: 0.0427\t decision: 0.8669\t get_bound: 8.5924\t add_domain: 0.2257\nCurrent (lb-rhs): -0.2821979522705078\n1076 domains visited\nCumulative time: 9.745459079742432\n\nbatch:  torch.Size([266, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([266, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1] [2, 3243] [2, 7594] [2, 7054] [3, 1] [2, 2190] [2, 6307] [3, 56] [2, 2190] [2, 1206] \n(532, 3, 32, 32) torch.Size([532, 1, 10]) torch.Size([532, 1])\npruning_in_iteration open status: False\nratio of positive domain = 82 / 532 = 0.15413533834586468\npruning-in-iteration extra time: 0.000171661376953125\nTensors transferred: pre=50.1287M lA=25.0643M alpha=2.7610M beta=0.0096M\nThis batch time : update_bounds func: 1.4410\t prepare: 0.0328\t bound: 1.2632\t transfer: 0.1376\t finalize: 0.0069\nAccumulated time: update_bounds func: 10.0515\t prepare: 0.1391\t bound: 9.3996\t transfer: 0.4688\t finalize: 0.0410\nbatch bounding time:  1.441295862197876\nCurrent worst splitting domains lb-rhs (depth):\n-0.27554 (17), -0.27521 (17), -0.27478 (17), -0.27388 (17), -0.26762 (17), -0.26734 (17), -0.26732 (17), -0.26678 (17), -0.26653 (17), -0.26589 (17), -0.26548 (17), -0.26499 (17), -0.26431 (17), -0.26397 (17), -0.26379 (17), -0.25926 (17), -0.25827 (17), -0.25810 (17), -0.25800 (17), -0.25676 (17), \nlength of domains: 450\nTotal time: 1.6981\t pickout: 0.0079\t decision: 0.1645\t get_bound: 1.4414\t add_domain: 0.0843\nAccumulated time:\t pickout: 0.0506\t decision: 1.0314\t get_bound: 10.0338\t add_domain: 0.3100\nCurrent (lb-rhs): -0.27553772926330566\n1158 domains visited\nCumulative time: 11.445619583129883\n\nbatch:  torch.Size([450, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([450, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 6307] [2, 2190] [2, 1206] [3, 126] [3, 56] [3, 230] [2, 6307] [3, 230] [2, 6307] \n(900, 3, 32, 32) torch.Size([900, 1, 10]) torch.Size([900, 1])\npruning_in_iteration open status: True\nratio of positive domain = 202 / 900 = 0.22444444444444445\npruning-in-iteration extra time: 0.05183720588684082\nTensors transferred: pre=84.8042M lA=32.8852M alpha=4.6709M beta=0.0180M\nThis batch time : update_bounds func: 2.8131\t prepare: 0.0555\t bound: 2.4916\t transfer: 0.2499\t finalize: 0.0151\nAccumulated time: update_bounds func: 12.8646\t prepare: 0.1946\t bound: 11.8912\t transfer: 0.7187\t finalize: 0.0560\nbatch bounding time:  2.8134212493896484\nCurrent worst splitting domains lb-rhs (depth):\n-0.26964 (18), -0.26939 (18), -0.26929 (18), -0.26886 (18), -0.26860 (18), -0.26832 (18), -0.26787 (18), -0.26185 (18), -0.26157 (18), -0.26148 (18), -0.26128 (18), -0.26094 (18), -0.26049 (18), -0.26047 (18), -0.26028 (18), -0.26027 (18), -0.26018 (18), -0.25964 (18), -0.25964 (18), -0.25960 (18), \nlength of domains: 698\nTotal time: 3.7750\t pickout: 0.0151\t decision: 0.7926\t get_bound: 2.8135\t add_domain: 0.1539\nAccumulated time:\t pickout: 0.0657\t decision: 1.8239\t get_bound: 12.8473\t add_domain: 0.4638\nCurrent (lb-rhs): -0.2696366310119629\n1360 domains visited\nCumulative time: 15.224959135055542\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 2732] [3, 56] [3, 56] [2, 6307] [3, 56] [2, 6307] [3, 56] [2, 1206] [3, 56] [2, 1206] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 324 / 1024 = 0.31640625\npruning-in-iteration extra time: 0.06853127479553223\nTensors transferred: pre=96.4883M lA=32.9794M alpha=5.3145M beta=0.0215M\nThis batch time : update_bounds func: 3.3746\t prepare: 0.0593\t bound: 3.0241\t transfer: 0.2681\t finalize: 0.0219\nAccumulated time: update_bounds func: 16.2392\t prepare: 0.2540\t bound: 14.9153\t transfer: 0.9868\t finalize: 0.0779\nbatch bounding time:  3.375058650970459\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26504 (19), -0.26475 (19), -0.26431 (19), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25834 (18), -0.25742 (19), -0.25730 (18), -0.25717 (19), -0.25709 (19), -0.25661 (19), -0.25660 (19), -0.25602 (19), -0.25579 (19), -0.25579 (19), \nlength of domains: 886\nTotal time: 6.2553\t pickout: 0.0164\t decision: 2.7066\t get_bound: 3.3752\t add_domain: 0.1571\nAccumulated time:\t pickout: 0.0821\t decision: 4.5305\t get_bound: 16.2225\t add_domain: 0.6210\nCurrent (lb-rhs): -0.2693934440612793\n1684 domains visited\nCumulative time: 21.48489284515381\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1963] [2, 5139] [3, 230] [3, 230] [3, 56] [3, 126] [2, 2190] [3, 56] [2, 6307] [2, 7064] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 362 / 1024 = 0.353515625\npruning-in-iteration extra time: 0.07835841178894043\nTensors transferred: pre=96.4883M lA=31.1891M alpha=5.3145M beta=0.0234M\nThis batch time : update_bounds func: 3.4486\t prepare: 0.0894\t bound: 3.0713\t transfer: 0.2691\t finalize: 0.0174\nAccumulated time: update_bounds func: 19.6877\t prepare: 0.3434\t bound: 17.9865\t transfer: 1.2559\t finalize: 0.0953\nbatch bounding time:  3.4489028453826904\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25999 (20), -0.25984 (20), -0.25964 (18), -0.25964 (20), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), \nlength of domains: 1036\nTotal time: 6.2223\t pickout: 0.0161\t decision: 1.1860\t get_bound: 3.4491\t add_domain: 1.5711\nAccumulated time:\t pickout: 0.0982\t decision: 5.7165\t get_bound: 19.6716\t add_domain: 2.1921\nCurrent (lb-rhs): -0.2693934440612793\n2046 domains visited\nCumulative time: 27.71270442008972\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [2, 4715] [2, 5518] [3, 230] [2, 5139] [3, 56] [2, 2732] [3, 230] [3, 230] [2, 2190] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 368 / 1024 = 0.359375\npruning-in-iteration extra time: 0.04218602180480957\nTensors transferred: pre=96.4883M lA=30.9535M alpha=5.3145M beta=0.0244M\nThis batch time : update_bounds func: 2.7892\t prepare: 0.0361\t bound: 2.5685\t transfer: 0.1733\t finalize: 0.0104\nAccumulated time: update_bounds func: 22.4769\t prepare: 0.3795\t bound: 20.5551\t transfer: 1.4292\t finalize: 0.1057\nbatch bounding time:  2.7893638610839844\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25546 (21), -0.25525 (21), -0.25506 (18), -0.25496 (19), -0.25486 (21), -0.25362 (19), -0.25348 (18), \nlength of domains: 1180\nTotal time: 4.2488\t pickout: 0.0182\t decision: 1.3239\t get_bound: 2.7894\t add_domain: 0.1171\nAccumulated time:\t pickout: 0.1164\t decision: 7.0405\t get_bound: 22.4610\t add_domain: 2.3092\nCurrent (lb-rhs): -0.2693934440612793\n2414 domains visited\nCumulative time: 31.965343952178955\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 5518] [3, 126] [2, 1204] [2, 7475] [3, 20] [2, 6379] [2, 5965] [3, 227] [3, 84] [3, 227] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 289 / 1024 = 0.2822265625\npruning-in-iteration extra time: 0.043543338775634766\nTensors transferred: pre=96.4883M lA=34.6755M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 3.0986\t prepare: 0.0479\t bound: 2.8709\t transfer: 0.1581\t finalize: 0.0208\nAccumulated time: update_bounds func: 25.5755\t prepare: 0.4274\t bound: 23.4260\t transfer: 1.5873\t finalize: 0.1265\nbatch bounding time:  3.098900556564331\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 1403\nTotal time: 4.2934\t pickout: 0.0148\t decision: 1.0321\t get_bound: 3.0990\t add_domain: 0.1476\nAccumulated time:\t pickout: 0.1312\t decision: 8.0725\t get_bound: 25.5600\t add_domain: 2.4568\nCurrent (lb-rhs): -0.2693934440612793\n2703 domains visited\nCumulative time: 36.2622709274292\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 227] [3, 227] [3, 20] [2, 5518] [2, 1963] [3, 126] [2, 5518] [2, 1206] [3, 84] [3, 20] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 264 / 1024 = 0.2578125\npruning-in-iteration extra time: 0.04320931434631348\nTensors transferred: pre=96.4883M lA=35.8062M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 3.0677\t prepare: 0.0351\t bound: 2.8819\t transfer: 0.1394\t finalize: 0.0104\nAccumulated time: update_bounds func: 28.6432\t prepare: 0.4625\t bound: 26.3079\t transfer: 1.7267\t finalize: 0.1369\nbatch bounding time:  3.067918062210083\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 1651\nTotal time: 4.2740\t pickout: 0.0148\t decision: 1.0683\t get_bound: 3.0680\t add_domain: 0.1229\nAccumulated time:\t pickout: 0.1460\t decision: 9.1408\t get_bound: 28.6280\t add_domain: 2.5797\nCurrent (lb-rhs): -0.2693934440612793\n2967 domains visited\nCumulative time: 40.54072976112366\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 7064] [2, 2454] [3, 84] [2, 5965] [3, 84] [2, 5518] [2, 4883] [2, 2454] [2, 4883] [3, 84] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 242 / 1024 = 0.236328125\npruning-in-iteration extra time: 0.045598745346069336\nTensors transferred: pre=96.4883M lA=36.8898M alpha=5.3145M beta=0.0264M\nThis batch time : update_bounds func: 3.6302\t prepare: 0.0343\t bound: 3.0397\t transfer: 0.5388\t finalize: 0.0165\nAccumulated time: update_bounds func: 32.2734\t prepare: 0.4969\t bound: 29.3476\t transfer: 2.2655\t finalize: 0.1534\nbatch bounding time:  3.630577802658081\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 1921\nTotal time: 4.9110\t pickout: 0.0145\t decision: 1.0394\t get_bound: 3.6307\t add_domain: 0.2264\nAccumulated time:\t pickout: 0.1605\t decision: 10.1803\t get_bound: 32.2587\t add_domain: 2.8060\nCurrent (lb-rhs): -0.2693934440612793\n3209 domains visited\nCumulative time: 45.457764625549316\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 2190] [2, 7054] [2, 5518] [2, 3931] [3, 84] [2, 6379] [2, 7140] [2, 7054] [2, 5518] [2, 7140] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 265 / 1024 = 0.2587890625\npruning-in-iteration extra time: 0.05239415168762207\nTensors transferred: pre=96.4883M lA=35.7591M alpha=5.3145M beta=0.0273M\nThis batch time : update_bounds func: 3.2505\t prepare: 0.0771\t bound: 2.9918\t transfer: 0.1691\t finalize: 0.0106\nAccumulated time: update_bounds func: 35.5239\t prepare: 0.5740\t bound: 32.3394\t transfer: 2.4347\t finalize: 0.1640\nbatch bounding time:  3.2507944107055664\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2168\nTotal time: 5.2025\t pickout: 0.0167\t decision: 1.1508\t get_bound: 3.2509\t add_domain: 0.7841\nAccumulated time:\t pickout: 0.1772\t decision: 11.3310\t get_bound: 35.5096\t add_domain: 3.5901\nCurrent (lb-rhs): -0.2693934440612793\n3474 domains visited\nCumulative time: 50.664958477020264\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 227] [2, 2454] [3, 20] [3, 20] [2, 7054] [2, 2732] [2, 7064] [2, 2732] [2, 7064] [2, 5518] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 260 / 1024 = 0.25390625\npruning-in-iteration extra time: 0.03832578659057617\nTensors transferred: pre=96.4883M lA=35.9947M alpha=5.3145M beta=0.0283M\nThis batch time : update_bounds func: 3.1264\t prepare: 0.0411\t bound: 2.9099\t transfer: 0.1164\t finalize: 0.0099\nAccumulated time: update_bounds func: 38.6503\t prepare: 0.6151\t bound: 35.2493\t transfer: 2.5511\t finalize: 0.1739\nbatch bounding time:  3.1266164779663086\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2420\nTotal time: 4.3359\t pickout: 0.0148\t decision: 1.0553\t get_bound: 3.1267\t add_domain: 0.1390\nAccumulated time:\t pickout: 0.1920\t decision: 12.3864\t get_bound: 38.6363\t add_domain: 3.7291\nCurrent (lb-rhs): -0.2693934440612793\n3734 domains visited\nCumulative time: 55.00421738624573\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 84] [3, 84] [2, 4883] [2, 6307] [3, 84] [2, 7054] [2, 4883] [2, 7054] [2, 2454] [2, 7064] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 292 / 1024 = 0.28515625\npruning-in-iteration extra time: 0.03989529609680176\nTensors transferred: pre=96.4883M lA=34.4870M alpha=5.3145M beta=0.0293M\nThis batch time : update_bounds func: 2.9881\t prepare: 0.0393\t bound: 2.7583\t transfer: 0.1773\t finalize: 0.0116\nAccumulated time: update_bounds func: 41.6384\t prepare: 0.6544\t bound: 38.0077\t transfer: 2.7283\t finalize: 0.1854\nbatch bounding time:  2.988567352294922\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2640\nTotal time: 4.1896\t pickout: 0.0149\t decision: 1.0605\t get_bound: 2.9887\t add_domain: 0.1256\nAccumulated time:\t pickout: 0.2069\t decision: 13.4469\t get_bound: 41.6249\t add_domain: 3.8547\nCurrent (lb-rhs): -0.2693934440612793\n4026 domains visited\nCumulative time: 59.19878840446472\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 500] [2, 2454] [2, 1961] [2, 1961] [2, 5654] [3, 84] [2, 1961] [2, 1961] [3, 84] [2, 2454] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 315 / 1024 = 0.3076171875\npruning-in-iteration extra time: 0.04327082633972168\nTensors transferred: pre=96.4883M lA=33.4034M alpha=5.3145M beta=0.0312M\nThis batch time : update_bounds func: 2.9048\t prepare: 0.0372\t bound: 2.7085\t transfer: 0.1441\t finalize: 0.0141\nAccumulated time: update_bounds func: 44.5432\t prepare: 0.6916\t bound: 40.7162\t transfer: 2.8724\t finalize: 0.1996\nbatch bounding time:  2.9050419330596924\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 2837\nTotal time: 4.1720\t pickout: 0.0170\t decision: 1.1168\t get_bound: 2.9051\t add_domain: 0.1330\nAccumulated time:\t pickout: 0.2239\t decision: 14.5637\t get_bound: 44.5301\t add_domain: 3.9877\nCurrent (lb-rhs): -0.2693934440612793\n4341 domains visited\nCumulative time: 63.37472677230835\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 5654] [2, 6307] [3, 227] [2, 5518] [2, 2190] [3, 84] [2, 2190] [2, 5654] [2, 5139] [2, 2190] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 345 / 1024 = 0.3369140625\npruning-in-iteration extra time: 0.04251885414123535\nTensors transferred: pre=96.4883M lA=31.9900M alpha=5.3145M beta=0.0332M\nThis batch time : update_bounds func: 3.0061\t prepare: 0.0392\t bound: 2.7551\t transfer: 0.1997\t finalize: 0.0113\nAccumulated time: update_bounds func: 47.5494\t prepare: 0.7307\t bound: 43.4713\t transfer: 3.0721\t finalize: 0.2108\nbatch bounding time:  3.006361961364746\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3004\nTotal time: 4.2641\t pickout: 0.0148\t decision: 1.1271\t get_bound: 3.0064\t add_domain: 0.1158\nAccumulated time:\t pickout: 0.2387\t decision: 15.6907\t get_bound: 47.5365\t add_domain: 4.1035\nCurrent (lb-rhs): -0.2693934440612793\n4686 domains visited\nCumulative time: 67.64237594604492\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 2772] [3, 84] [3, 84] [3, 84] [3, 84] [3, 227] [3, 227] [2, 6330] [3, 84] [3, 84] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 329 / 1024 = 0.3212890625\npruning-in-iteration extra time: 0.04882621765136719\nTensors transferred: pre=96.4883M lA=32.7438M alpha=5.3145M beta=0.0352M\nThis batch time : update_bounds func: 2.9885\t prepare: 0.0393\t bound: 2.7696\t transfer: 0.1668\t finalize: 0.0115\nAccumulated time: update_bounds func: 50.5378\t prepare: 0.7700\t bound: 46.2409\t transfer: 3.2389\t finalize: 0.2224\nbatch bounding time:  2.988706350326538\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3187\nTotal time: 4.2232\t pickout: 0.0152\t decision: 1.0981\t get_bound: 2.9888\t add_domain: 0.1211\nAccumulated time:\t pickout: 0.2539\t decision: 16.7889\t get_bound: 50.5253\t add_domain: 4.2246\nCurrent (lb-rhs): -0.2693934440612793\n5015 domains visited\nCumulative time: 71.86991477012634\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 500] [2, 7534] [2, 500] [2, 1242] [3, 84] [2, 500] [3, 227] [2, 500] [3, 227] [2, 7064] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 304 / 1024 = 0.296875\npruning-in-iteration extra time: 0.04673123359680176\nTensors transferred: pre=96.4883M lA=33.9217M alpha=5.3145M beta=0.0361M\nThis batch time : update_bounds func: 2.9820\t prepare: 0.0368\t bound: 2.7574\t transfer: 0.1731\t finalize: 0.0140\nAccumulated time: update_bounds func: 53.5199\t prepare: 0.8068\t bound: 48.9983\t transfer: 3.4119\t finalize: 0.2363\nbatch bounding time:  2.982332706451416\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3395\nTotal time: 4.1926\t pickout: 0.0150\t decision: 1.0698\t get_bound: 2.9824\t add_domain: 0.1255\nAccumulated time:\t pickout: 0.2688\t decision: 17.8586\t get_bound: 53.5077\t add_domain: 4.3501\nCurrent (lb-rhs): -0.2693934440612793\n5319 domains visited\nCumulative time: 76.06673121452332\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 20] [3, 20] [2, 1961] [3, 104] [2, 5139] [3, 104] [2, 2454] [2, 7054] [3, 84] [2, 6330] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 248 / 1024 = 0.2421875\npruning-in-iteration extra time: 0.04503941535949707\nTensors transferred: pre=96.4883M lA=36.5600M alpha=5.3145M beta=0.0371M\nThis batch time : update_bounds func: 3.1922\t prepare: 0.0402\t bound: 2.9580\t transfer: 0.1767\t finalize: 0.0164\nAccumulated time: update_bounds func: 56.7121\t prepare: 0.8469\t bound: 51.9562\t transfer: 3.5887\t finalize: 0.2528\nbatch bounding time:  3.192532777786255\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3659\nTotal time: 4.5033\t pickout: 0.0163\t decision: 1.0824\t get_bound: 3.1926\t add_domain: 0.2119\nAccumulated time:\t pickout: 0.2851\t decision: 18.9410\t get_bound: 56.7003\t add_domain: 4.5620\nCurrent (lb-rhs): -0.2693934440612793\n5567 domains visited\nCumulative time: 80.57463073730469\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 3889] [2, 3889] [2, 3889] [2, 7064] [2, 7534] [2, 1961] [3, 227] [2, 5518] [2, 7064] [2, 7534] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 176 / 1024 = 0.171875\npruning-in-iteration extra time: 0.00019598007202148438\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0381M\nThis batch time : update_bounds func: 3.3226\t prepare: 0.0687\t bound: 3.0632\t transfer: 0.1779\t finalize: 0.0116\nAccumulated time: update_bounds func: 60.0347\t prepare: 0.9156\t bound: 55.0194\t transfer: 3.7666\t finalize: 0.2643\nbatch bounding time:  3.3229784965515137\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 3995\nTotal time: 4.6350\t pickout: 0.0151\t decision: 1.1284\t get_bound: 3.3231\t add_domain: 0.1684\nAccumulated time:\t pickout: 0.3003\t decision: 20.0695\t get_bound: 60.0234\t add_domain: 4.7304\nCurrent (lb-rhs): -0.2693934440612793\n5743 domains visited\nCumulative time: 85.21530604362488\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6330] [2, 3889] [3, 84] [2, 7534] [3, 84] [2, 7064] [2, 7054] [2, 4715] [3, 84] [3, 73] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 218 / 1024 = 0.212890625\npruning-in-iteration extra time: 0.03806924819946289\nTensors transferred: pre=96.4883M lA=37.9734M alpha=5.3145M beta=0.0400M\nThis batch time : update_bounds func: 3.3408\t prepare: 0.0351\t bound: 3.0519\t transfer: 0.2412\t finalize: 0.0118\nAccumulated time: update_bounds func: 63.3755\t prepare: 0.9507\t bound: 58.0713\t transfer: 4.0078\t finalize: 0.2761\nbatch bounding time:  3.341097593307495\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 4289\nTotal time: 8.4230\t pickout: 0.0164\t decision: 1.1141\t get_bound: 3.3412\t add_domain: 3.9513\nAccumulated time:\t pickout: 0.3167\t decision: 21.1835\t get_bound: 63.3646\t add_domain: 8.6817\nCurrent (lb-rhs): -0.2693934440612793\n5961 domains visited\nCumulative time: 93.64328217506409\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 104] [3, 84] [3, 104] [3, 104] [3, 104] [3, 104] [2, 4715] [2, 4715] [2, 4715] [2, 4715] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 243 / 1024 = 0.2373046875\npruning-in-iteration extra time: 0.039351463317871094\nTensors transferred: pre=96.4883M lA=36.7956M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 3.1552\t prepare: 0.0433\t bound: 2.9441\t transfer: 0.1550\t finalize: 0.0117\nAccumulated time: update_bounds func: 66.5307\t prepare: 0.9940\t bound: 61.0154\t transfer: 4.1628\t finalize: 0.2878\nbatch bounding time:  3.155402421951294\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 4558\nTotal time: 4.6062\t pickout: 0.0193\t decision: 1.2730\t get_bound: 3.1555\t add_domain: 0.1584\nAccumulated time:\t pickout: 0.3360\t decision: 22.4565\t get_bound: 66.5201\t add_domain: 8.8400\nCurrent (lb-rhs): -0.2693934440612793\n6204 domains visited\nCumulative time: 98.25336909294128\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 3870] [2, 3931] [2, 5518] [2, 3889] [2, 3870] [3, 104] [2, 3870] [3, 84] [2, 7054] [2, 3870] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 221 / 1024 = 0.2158203125\npruning-in-iteration extra time: 0.032593727111816406\nTensors transferred: pre=96.4883M lA=37.8321M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 3.3303\t prepare: 0.0451\t bound: 2.9963\t transfer: 0.2760\t finalize: 0.0121\nAccumulated time: update_bounds func: 69.8610\t prepare: 1.0391\t bound: 64.0117\t transfer: 4.4388\t finalize: 0.2999\nbatch bounding time:  3.33282732963562\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 4849\nTotal time: 4.6422\t pickout: 0.0147\t decision: 1.0892\t get_bound: 3.3329\t add_domain: 0.2053\nAccumulated time:\t pickout: 0.3507\t decision: 23.5457\t get_bound: 69.8530\t add_domain: 9.0454\nCurrent (lb-rhs): -0.2693934440612793\n6425 domains visited\nCumulative time: 102.8993911743164\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 5518] [3, 84] [3, 84] [2, 5518] [2, 5518] [2, 3931] [2, 3931] [3, 84] [2, 3931] [3, 84] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 232 / 1024 = 0.2265625\npruning-in-iteration extra time: 0.035376548767089844\nTensors transferred: pre=96.4883M lA=37.3138M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 3.3037\t prepare: 0.0375\t bound: 2.9775\t transfer: 0.2767\t finalize: 0.0111\nAccumulated time: update_bounds func: 73.1647\t prepare: 1.0766\t bound: 66.9892\t transfer: 4.7154\t finalize: 0.3110\nbatch bounding time:  3.3062775135040283\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 5129\nTotal time: 4.5642\t pickout: 0.0151\t decision: 1.0542\t get_bound: 3.3064\t add_domain: 0.1885\nAccumulated time:\t pickout: 0.3658\t decision: 24.5999\t get_bound: 73.1594\t add_domain: 9.2339\nCurrent (lb-rhs): -0.2693934440612793\n6657 domains visited\nCumulative time: 107.46808815002441\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 73] [3, 84] [2, 5518] [2, 5518] [2, 5518] [2, 5518] [2, 5518] [2, 5518] [2, 5518] [2, 4883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 155 / 1024 = 0.1513671875\npruning-in-iteration extra time: 0.00019073486328125\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 3.3740\t prepare: 0.0361\t bound: 3.0164\t transfer: 0.3095\t finalize: 0.0112\nAccumulated time: update_bounds func: 76.5388\t prepare: 1.1127\t bound: 70.0055\t transfer: 5.0249\t finalize: 0.3222\nbatch bounding time:  3.379404067993164\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 5486\nTotal time: 4.7431\t pickout: 0.0154\t decision: 1.1242\t get_bound: 3.3795\t add_domain: 0.2241\nAccumulated time:\t pickout: 0.3812\t decision: 25.7241\t get_bound: 76.5389\t add_domain: 9.4580\nCurrent (lb-rhs): -0.2693934440612793\n6812 domains visited\nCumulative time: 112.21958374977112\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1963] [2, 1963] [2, 7064] [2, 1963] [2, 1963] [2, 4883] [2, 1963] [2, 6330] [3, 73] [2, 6330] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 166 / 1024 = 0.162109375\npruning-in-iteration extra time: 0.0001983642578125\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0391M\nThis batch time : update_bounds func: 3.2382\t prepare: 0.0398\t bound: 2.9811\t transfer: 0.2048\t finalize: 0.0116\nAccumulated time: update_bounds func: 79.7770\t prepare: 1.1524\t bound: 72.9867\t transfer: 5.2297\t finalize: 0.3338\nbatch bounding time:  3.238682508468628\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 5832\nTotal time: 4.5088\t pickout: 0.0152\t decision: 1.0827\t get_bound: 3.2388\t add_domain: 0.1721\nAccumulated time:\t pickout: 0.3964\t decision: 26.8067\t get_bound: 79.7776\t add_domain: 9.6301\nCurrent (lb-rhs): -0.2693934440612793\n6978 domains visited\nCumulative time: 116.73294496536255\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1242] [2, 3417] [2, 3417] [2, 3417] [2, 7786] [2, 1242] [2, 3889] [2, 4883] [2, 2732] [2, 7786] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 94 / 1024 = 0.091796875\npruning-in-iteration extra time: 0.00017571449279785156\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0410M\nThis batch time : update_bounds func: 3.2229\t prepare: 0.0380\t bound: 2.9568\t transfer: 0.2146\t finalize: 0.0120\nAccumulated time: update_bounds func: 82.9999\t prepare: 1.1904\t bound: 75.9435\t transfer: 5.4443\t finalize: 0.3458\nbatch bounding time:  3.2234861850738525\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 6250\nTotal time: 4.5982\t pickout: 0.0167\t decision: 1.1656\t get_bound: 3.2236\t add_domain: 0.1923\nAccumulated time:\t pickout: 0.4131\t decision: 27.9723\t get_bound: 83.0012\t add_domain: 9.8224\nCurrent (lb-rhs): -0.2693934440612793\n7072 domains visited\nCumulative time: 121.33534860610962\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 4883] [2, 7054] [2, 4883] [2, 4883] [2, 2732] [2, 1242] [2, 4883] [1, 2615] [2, 2732] [2, 1242] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 102 / 1024 = 0.099609375\npruning-in-iteration extra time: 0.00023937225341796875\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0420M\nThis batch time : update_bounds func: 3.2328\t prepare: 0.0388\t bound: 2.9604\t transfer: 0.2138\t finalize: 0.0190\nAccumulated time: update_bounds func: 86.2327\t prepare: 1.2292\t bound: 78.9039\t transfer: 5.6581\t finalize: 0.3648\nbatch bounding time:  3.233426332473755\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 6660\nTotal time: 4.5023\t pickout: 0.0156\t decision: 1.0664\t get_bound: 3.2336\t add_domain: 0.1868\nAccumulated time:\t pickout: 0.4286\t decision: 29.0388\t get_bound: 86.2348\t add_domain: 10.0091\nCurrent (lb-rhs): -0.2693934440612793\n7174 domains visited\nCumulative time: 125.84230279922485\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 3425] [2, 6033] [2, 6033] [2, 6033] [2, 6033] [2, 6033] [2, 1965] [2, 6033] [1, 2615] [2, 1965] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 53 / 1024 = 0.0517578125\npruning-in-iteration extra time: 0.00023293495178222656\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0430M\nThis batch time : update_bounds func: 3.2375\t prepare: 0.0401\t bound: 3.0175\t transfer: 0.1682\t finalize: 0.0107\nAccumulated time: update_bounds func: 89.4702\t prepare: 1.2693\t bound: 81.9214\t transfer: 5.8263\t finalize: 0.3755\nbatch bounding time:  3.2379391193389893\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 7119\nTotal time: 4.5290\t pickout: 0.0152\t decision: 1.0829\t get_bound: 3.2380\t add_domain: 0.1928\nAccumulated time:\t pickout: 0.4439\t decision: 30.1217\t get_bound: 89.4728\t add_domain: 10.2019\nCurrent (lb-rhs): -0.2693934440612793\n7227 domains visited\nCumulative time: 130.3755075931549\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 1965] [1, 2615] [2, 7786] [2, 1965] [2, 7786] [2, 7786] [2, 1965] [2, 1041] [2, 5965] [2, 2300] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 59 / 1024 = 0.0576171875\npruning-in-iteration extra time: 0.0002071857452392578\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0430M\nThis batch time : update_bounds func: 3.2958\t prepare: 0.0451\t bound: 3.0127\t transfer: 0.2189\t finalize: 0.0177\nAccumulated time: update_bounds func: 92.7660\t prepare: 1.3144\t bound: 84.9341\t transfer: 6.0452\t finalize: 0.3932\nbatch bounding time:  3.2993452548980713\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 7572\nTotal time: 4.6737\t pickout: 0.0163\t decision: 1.0559\t get_bound: 3.2994\t add_domain: 0.3020\nAccumulated time:\t pickout: 0.4602\t decision: 31.1777\t get_bound: 92.7722\t add_domain: 10.5038\nCurrent (lb-rhs): -0.2693934440612793\n7286 domains visited\nCumulative time: 135.05394792556763\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [2, 6329] [2, 6329] [2, 2300] [2, 1204] [2, 1965] [1, 2615] [2, 1204] [2, 1965] [1, 2615] [2, 2300] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: False\nratio of positive domain = 80 / 1024 = 0.078125\npruning-in-iteration extra time: 0.00024700164794921875\nTensors transferred: pre=96.4883M lA=48.2441M alpha=5.3145M beta=0.0449M\nThis batch time : update_bounds func: 3.6434\t prepare: 0.0502\t bound: 2.9868\t transfer: 0.5939\t finalize: 0.0116\nAccumulated time: update_bounds func: 96.4094\t prepare: 1.3646\t bound: 87.9209\t transfer: 6.6391\t finalize: 0.4048\nbatch bounding time:  3.6509528160095215\nCurrent worst splitting domains lb-rhs (depth):\n-0.26939 (18), -0.26832 (18), -0.26399 (19), -0.26327 (19), -0.26018 (18), -0.25964 (18), -0.25881 (18), -0.25842 (20), -0.25834 (18), -0.25793 (20), -0.25730 (18), -0.25602 (19), -0.25579 (19), -0.25506 (18), -0.25496 (19), -0.25362 (19), -0.25348 (18), -0.25341 (21), -0.25311 (19), -0.25219 (19), \nlength of domains: 8004\nTotal time: 5.0528\t pickout: 0.0156\t decision: 1.1104\t get_bound: 3.6510\t add_domain: 0.2758\nAccumulated time:\t pickout: 0.4758\t decision: 32.2880\t get_bound: 96.4233\t add_domain: 10.7796\nCurrent (lb-rhs): -0.2693934440612793\n7366 domains visited\nCumulative time: 140.1185200214386\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 183] [2, 1376] [3, 183] [2, 1041] [2, 1041] [2, 424] [2, 5965] [3, 183] [2, 1041] [2, 4845] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 282 / 1024 = 0.275390625\npruning-in-iteration extra time: 0.049095869064331055\nTensors transferred: pre=96.4883M lA=34.9582M alpha=5.3145M beta=0.0459M\nThis batch time : update_bounds func: 3.5760\t prepare: 0.0356\t bound: 2.8519\t transfer: 0.6753\t finalize: 0.0125\nAccumulated time: update_bounds func: 99.9854\t prepare: 1.4002\t bound: 90.7728\t transfer: 7.3144\t finalize: 0.4173\nbatch bounding time:  3.5796079635620117\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_27_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "OK",
            "result": "TIMEOUT",
            "took": "300",
            "stderr": "",
            "stdout": "Running in parallel with 12 processes\nOverapprox Round 1/3 has 1 set(s)\nLayer 1/7: LinearOnnxSubnetworkLayer (zono shape: (32768, 4509))... 6.503 sec\nLayer 2/7: ReluLayer (zono shape: (8192, 4509))... 3.089 sec\nLayer 3/7: LinearOnnxSubnetworkLayer (zono shape: (8192, 5191))... 4.37 sec\nLayer 4/7: ReluLayer (zono shape: (8192, 5191))... 5.133 sec\nLayer 5/7: LinearOnnxSubnetworkLayer (zono shape: (8192, 5762))... 11.519 sec\nLayer 6/7: ReluLayer (zono shape: (250, 5762))... 0.121 sec\nLayer 7/7: LinearOnnxSubnetworkLayer (zono shape: (250, 5820))... 0.559 sec\nOverapprox Round 2/3 has 3 set(s)\nLayer 1/7: LinearOnnxSubnetworkLayer (zono shape: (32768, 4509))... 23.479 sec\nLayer 2/7: ReluLayer (zono shape: (8192, 4509))... 9.137 sec\nLayer 3/7: LinearOnnxSubnetworkLayer (zono shape: (8192, 5189))... 15.956 sec\nLayer 4/7: ReluLayer (zono shape: (8192, 5189))... 11.802 sec\nLayer 5/7: LinearOnnxSubnetworkLayer (zono shape: (8192, 5759))... 22.097 sec\nLayer 6/7: ReluLayer (zono shape: (250, 5759))... 0.087 sec\nLayer 7/7: LinearOnnxSubnetworkLayer (zono shape: (250, 5817))... 0.852 sec\nOverapprox Round 3/3 has 4 set(s)\nLayer 1/7: LinearOnnxSubnetworkLayer (zono shape: (32768, 4509))... 24.386 sec\nLayer 2/7: ReluLayer (zono shape: (8192, 4509))...\nUsing LP to check 680/8192 potential ReLU splits... 28.943 sec\nLayer 3/7: LinearOnnxSubnetworkLayer (zono shape: (8192, 5174))... 16.961 sec\nLayer 4/7: ReluLayer (zono shape: (8192, 5174))...\nUsing LP to check 569/8192 potential ReLU splits..."
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_36_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "8.697463512420654",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpm6sxl2b1.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:03:06 2024 on Cerberus\nInternal results will be saved to /tmp/tmpm6sxl2b1.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_36_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-1.96812832, -6.49893856,  1.24639082,  1.69520974,  4.02628088,\n          2.75464153,  2.39966416,  3.83796740, -4.41950607, -2.72571588]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.01335669, -6.61650658,  1.13246620,  1.72124791,  3.89798760,\n           2.86626458,  2.36144567,  4.41441536, -4.41264629, -2.79052591],\n         [-2.01335669, -6.61650658,  1.13246620,  1.72124791,  3.89798760,\n           2.86626458,  2.36144567,  4.41441536, -4.41264629, -2.79052591]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.91134453, 10.51449394,  2.76552153,  2.17673969,  1.03172302,\n           1.53654194, -0.51642776,  8.31063366,  6.68851376]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 2.1015 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 6.096297979354858\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_44_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.896490097045898",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmphfb32bkg.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:03:15 2024 on Cerberus\nInternal results will be saved to /tmp/tmphfb32bkg.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_44_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 8.53005219,  2.65516281,  3.62637854, -1.70141554,  3.90650630,\n         -4.50923967, -6.23203850, -2.73792219,  3.00681901,  4.35398483]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 8.30019855,  3.03155708,  3.46851993, -1.78878641,  3.78387809,\n          -4.47537851, -6.29589128, -2.85683608,  2.87485886,  4.81936932],\n         [ 8.30019855,  3.03155708,  3.46851993, -1.78878641,  3.78387809,\n          -4.47537851, -6.29589128, -2.85683608,  2.87485886,  4.81936932]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.26864147,  4.83167839, 10.08898544,  4.51632023, 12.77557755,\n          14.59609032, 11.15703487,  5.42533970,  3.48082924]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6325 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 8.53005219,  2.65516281,  3.62637854, -1.70141554,  3.90650630,\n         -4.50923967, -6.23203850, -2.73792219,  3.00681901,  4.35398483]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [986]; unstable size 986; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 433 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 406 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [433]; unstable size 433; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 406 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [406]; unstable size 406; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 28 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [28]; unstable size 28; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 4.16252232,  3.01627827,  8.77518082,  3.12779617, 11.22480965,\n         13.38881111,  9.78200912,  3.72122002,  2.91803217]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.180932521820068\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_55_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "7.348095893859863",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpi882f_05.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:03:24 2024 on Cerberus\nInternal results will be saved to /tmp/tmpi882f_05.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 4.24215508, -0.69684935,  0.55492795, -0.75052458,  0.43542138,\n         -1.24407911, -1.12097180, -2.80693984,  7.00109816,  0.78772008]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 4.52412033, -0.46530545,  0.71960282, -0.91084248,  0.43153140,\n          -1.32478404, -1.19332063, -2.72541595,  6.33062458,  0.94560248],\n         [ 4.52412033, -0.46530545,  0.71960282, -0.91084248,  0.43153140,\n          -1.32478404, -1.19332063, -2.72541595,  6.33062458,  0.94560248]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.80650425, 6.79592991, 5.61102200, 7.24146700, 5.89909315,\n          7.65540886, 7.52394533, 9.05604076, 5.38502216]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.7155 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 4.24215508, -0.69684935,  0.55492795, -0.75052458,  0.43542138,\n         -1.24407911, -1.12097180, -2.80693984,  7.00109816,  0.78772008]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [958]; unstable size 958; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 489 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 415 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [489]; unstable size 489; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 415 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [415]; unstable size 415; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 54 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [54]; unstable size 54; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[0.48258638, 5.41662645, 3.66289663, 5.48767567, 3.82613897, 5.83310986,\n         5.75515890, 7.10578346, 3.81086159]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.638326406478882\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_71_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "17.41506314277649",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpqreeys0z.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:03:33 2024 on Cerberus\nInternal results will be saved to /tmp/tmpqreeys0z.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_71_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n          1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-1.91160369, -1.75271857,  0.69264895,  2.51886392,  1.29905379,\n           1.52446699,  3.16162252,  0.31408176, -3.63012004, -0.84955752],\n         [-1.91160369, -1.75271857,  0.69264895,  2.51886392,  1.29905379,\n           1.52446699,  3.16162252,  0.31408176, -3.63012004, -0.84955752]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.07322598, 4.91434097, 2.46897364, 0.64275861, 1.86256874,\n          1.63715553, 2.84754086, 6.79174232, 4.01117992]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.9172 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n          1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1915]; unstable size 1915; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 111 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [772]; unstable size 772; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 111 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [630]; unstable size 630; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 67 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [67]; unstable size 67; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.80872035,  2.45961571,  0.37849236, -0.61176801,  0.52175808,\n          0.03984714,  0.67118931,  4.57306910,  1.52069569]], device='cuda:0') None\nbest_l after optimization: 14.235376358032227 with beta sum per layer: []\nalpha/beta optimization time: 5.984632968902588\ninitial alpha-CROWN bounds: tensor([[ 3.03656006,  2.69559813,  0.59893179, -0.46514320,  0.67544270,\n          0.21066594,  0.89655590,  4.81512547,  1.77163982]], device='cuda:0')\nWorst class: (+ rhs) -0.46514320373535156\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 128, 8, 8]), torch.Size([1, 9, 250])]\n\nProperties batch 0, size 1\nRemaining timeout: 288.1027572154999\n##### Instance 0 first 10 spec matrices: [[[-1.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.03656005859375.\n\nProperties batch 1, size 1\nRemaining timeout: 288.0656986236572\n##### Instance 0 first 10 spec matrices: [[[ 0. -1.  0.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.6955981254577637.\n\nProperties batch 2, size 1\nRemaining timeout: 288.04173731803894\n##### Instance 0 first 10 spec matrices: [[[ 0.  0. -1.  0.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.5989317893981934.\n\nProperties batch 3, size 1\nRemaining timeout: 288.0166153907776\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0. -1.  0.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.46514320]], device='cuda:0') need to verify.\nModel prediction is: tensor([-2.00457525, -1.67054403,  0.72028756,  2.41133142,  1.22753203,\n         1.47416234,  3.83728242,  0.14393537, -3.74496412, -0.80569816],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /22 start_node /30 with alignment adjustment\nsetting alpha for layer /24 start_node /30 with alignment adjustment\nsetting alpha for layer /26 start_node /30 with alignment adjustment\nsetting alpha for layer /29 start_node /30 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 128, 8, 8]), torch.Size([1, 1, 250])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.46514320]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/30']\nKeeping slopes for these layers: ['/30']\nlayer 0 size torch.Size([32768]) unstable 1915\nlayer 1 size torch.Size([8192]) unstable 749\nlayer 2 size torch.Size([8192]) unstable 614\nlayer 3 size torch.Size([250]) unstable 63\n-----------------\n# of unstable neurons: 3341\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [3, 189] \nsplit level 1: [3, 236] \nsplit level 2: [3, 76] \nsplit level 3: [3, 238] \nsplit level 4: [3, 44] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 7 / 32 = 0.21875\npruning-in-iteration extra time: 0.016900300979614258\nTensors transferred: pre=3.0153M lA=1.1778M alpha=0.2065M beta=0.0002M\nThis batch time : update_bounds func: 0.7269\t prepare: 0.0031\t bound: 0.7144\t transfer: 0.0087\t finalize: 0.0006\nAccumulated time: update_bounds func: 0.7269\t prepare: 0.0031\t bound: 0.7144\t transfer: 0.0087\t finalize: 0.0006\nbatch bounding time:  0.7269375324249268\nCurrent worst splitting domains lb-rhs (depth):\n-0.14887 (5), -0.14311 (5), -0.13851 (5), -0.13621 (5), -0.10090 (5), -0.09864 (5), -0.09558 (5), -0.09537 (5), -0.09237 (5), -0.09080 (5), -0.08998 (5), -0.08598 (5), -0.06068 (5), -0.05457 (5), -0.05413 (5), -0.05390 (5), -0.05302 (5), -0.04999 (5), -0.04704 (5), -0.04551 (5), \nlength of domains: 25\nTotal time: 0.9705\t pickout: 0.0013\t decision: 0.2328\t get_bound: 0.7315\t add_domain: 0.0048\nAccumulated time:\t pickout: 0.0013\t decision: 0.2328\t get_bound: 0.7315\t add_domain: 0.0048\nCurrent (lb-rhs): -0.14887094497680664\n7 domains visited\nCumulative time: 1.191833257675171\n\nbatch:  torch.Size([25, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([25, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] [3, 40] \n(50, 3, 32, 32) torch.Size([50, 1, 10]) torch.Size([50, 1])\npruning_in_iteration open status: False\nratio of positive domain = 10 / 50 = 0.19999999999999996\npruning-in-iteration extra time: 0.00014543533325195312\nTensors transferred: pre=4.7113M lA=2.3557M alpha=0.3227M beta=0.0003M\nThis batch time : update_bounds func: 0.4540\t prepare: 0.0040\t bound: 0.4400\t transfer: 0.0093\t finalize: 0.0008\nAccumulated time: update_bounds func: 1.1809\t prepare: 0.0071\t bound: 1.1544\t transfer: 0.0179\t finalize: 0.0014\nbatch bounding time:  0.45410847663879395\nCurrent worst splitting domains lb-rhs (depth):\n-0.11342 (6), -0.10721 (6), -0.10490 (6), -0.10249 (6), -0.10028 (6), -0.09935 (6), -0.09675 (6), -0.09580 (6), -0.06432 (6), -0.06324 (6), -0.06069 (6), -0.05826 (6), -0.05811 (6), -0.05549 (6), -0.05466 (6), -0.05458 (6), -0.05427 (6), -0.05199 (6), -0.05098 (6), -0.05044 (6), \nlength of domains: 40\nTotal time: 0.4873\t pickout: 0.0011\t decision: 0.0266\t get_bound: 0.4542\t add_domain: 0.0054\nAccumulated time:\t pickout: 0.0025\t decision: 0.2594\t get_bound: 1.1857\t add_domain: 0.0102\nCurrent (lb-rhs): -0.11342072486877441\n17 domains visited\nCumulative time: 1.679450511932373\n\nbatch:  torch.Size([40, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([40, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 86] [3, 119] [3, 119] [3, 119] [3, 86] [3, 86] [3, 119] [3, 119] [3, 86] [3, 119] \n(80, 3, 32, 32) torch.Size([80, 1, 10]) torch.Size([80, 1])\npruning_in_iteration open status: True\nratio of positive domain = 40 / 80 = 0.5\npruning-in-iteration extra time: 0.028212785720825195\nTensors transferred: pre=7.5381M lA=1.8845M alpha=0.5164M beta=0.0005M\nThis batch time : update_bounds func: 0.5725\t prepare: 0.0032\t bound: 0.5581\t transfer: 0.0102\t finalize: 0.0009\nAccumulated time: update_bounds func: 1.7534\t prepare: 0.0103\t bound: 1.7125\t transfer: 0.0281\t finalize: 0.0022\nbatch bounding time:  0.5725808143615723\nCurrent worst splitting domains lb-rhs (depth):\n-0.07957 (7), -0.07407 (7), -0.07299 (7), -0.07111 (7), -0.06886 (7), -0.06856 (7), -0.06591 (7), -0.06535 (7), -0.06301 (7), -0.06225 (7), -0.06194 (7), -0.03045 (7), -0.02705 (7), -0.02641 (7), -0.02465 (7), -0.02439 (7), -0.02411 (7), -0.02389 (7), -0.02070 (7), -0.01987 (7), \nlength of domains: 40\nTotal time: 0.6134\t pickout: 0.0016\t decision: 0.0331\t get_bound: 0.5726\t add_domain: 0.0060\nAccumulated time:\t pickout: 0.0041\t decision: 0.2925\t get_bound: 1.7583\t add_domain: 0.0162\nCurrent (lb-rhs): -0.07957100868225098\n57 domains visited\nCumulative time: 2.29400372505188\n\nbatch:  torch.Size([40, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([40, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 119] [3, 86] [3, 86] [3, 86] [3, 119] [3, 86] [3, 86] [3, 86] [3, 86] [3, 119] \n(80, 3, 32, 32) torch.Size([80, 1, 10]) torch.Size([80, 1])\npruning_in_iteration open status: True\nratio of positive domain = 64 / 80 = 0.8\npruning-in-iteration extra time: 0.02889394760131836\nTensors transferred: pre=7.5381M lA=0.7538M alpha=0.5164M beta=0.0006M\nThis batch time : update_bounds func: 0.5526\t prepare: 0.0032\t bound: 0.5374\t transfer: 0.0108\t finalize: 0.0011\nAccumulated time: update_bounds func: 2.3060\t prepare: 0.0136\t bound: 2.2499\t transfer: 0.0390\t finalize: 0.0033\nbatch bounding time:  0.5526700019836426\nCurrent worst splitting domains lb-rhs (depth):\n-0.04667 (8), -0.04101 (8), -0.03977 (8), -0.03795 (8), -0.03642 (8), -0.03548 (8), -0.03272 (8), -0.03257 (8), -0.03241 (8), -0.03021 (8), -0.02906 (8), -0.02898 (8), -0.02764 (8), -0.02535 (8), -0.02335 (8), -0.02225 (8), \nlength of domains: 16\nTotal time: 0.5878\t pickout: 0.0016\t decision: 0.0305\t get_bound: 0.5527\t add_domain: 0.0030\nAccumulated time:\t pickout: 0.0058\t decision: 0.3230\t get_bound: 2.3110\t add_domain: 0.0192\nCurrent (lb-rhs): -0.04667210578918457\n121 domains visited\nCumulative time: 2.8824799060821533\n\nbatch:  torch.Size([16, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([16, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] [3, 186] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 21 / 32 = 0.65625\npruning-in-iteration extra time: 0.026942729949951172\nTensors transferred: pre=3.0153M lA=0.5182M alpha=0.2065M beta=0.0003M\nThis batch time : update_bounds func: 0.5271\t prepare: 0.0025\t bound: 0.5177\t transfer: 0.0065\t finalize: 0.0005\nAccumulated time: update_bounds func: 2.8332\t prepare: 0.0160\t bound: 2.7676\t transfer: 0.0454\t finalize: 0.0037\nbatch bounding time:  0.5272314548492432\nCurrent worst splitting domains lb-rhs (depth):\n-0.01575 (9), -0.01069 (9), -0.00833 (9), -0.00693 (9), -0.00548 (9), -0.00480 (9), -0.00447 (9), -0.00222 (9), -0.00112 (9), -0.00060 (9), -0.00032 (9), \nlength of domains: 11\nTotal time: 0.5523\t pickout: 0.0010\t decision: 0.0212\t get_bound: 0.5273\t add_domain: 0.0028\nAccumulated time:\t pickout: 0.0068\t decision: 0.3441\t get_bound: 2.8384\t add_domain: 0.0220\nCurrent (lb-rhs): -0.015745878219604492\n142 domains visited\nCumulative time: 3.435089349746704\n\nbatch:  torch.Size([11, 32, 32, 32]) pre split depth:  2\nbatch:  torch.Size([11, 32, 32, 32]) post split depth:  2\nsplitting decisions: \nsplit level 0: [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] [3, 91] \nsplit level 1: [3, 207] [3, 207] [3, 207] [3, 207] [3, 179] [3, 207] [3, 207] [3, 207] [3, 207] [3, 207] \n(44, 3, 32, 32) torch.Size([44, 1, 10]) torch.Size([44, 1])\n\nall verified at 0th iter\npruning_in_iteration open status: False\nratio of positive domain = 44 / 44 = 1.0\npruning-in-iteration extra time: 0.00020003318786621094\nTensors transferred: pre=4.1460M lA=2.0730M alpha=0.2840M beta=0.0005M\nThis batch time : update_bounds func: 0.0180\t prepare: 0.0028\t bound: 0.0075\t transfer: 0.0070\t finalize: 0.0007\nAccumulated time: update_bounds func: 2.8512\t prepare: 0.0189\t bound: 2.7750\t transfer: 0.0524\t finalize: 0.0044\nbatch bounding time:  0.01811838150024414\nlength of domains: 0\nTotal time: 0.0556\t pickout: 0.0010\t decision: 0.0264\t get_bound: 0.0222\t add_domain: 0.0059\nAccumulated time:\t pickout: 0.0078\t decision: 0.3705\t get_bound: 2.8606\t add_domain: 0.0280\nNo domains left, verification finished!\n186 domains visited\n/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(arguments.Config[\"bab\"][\"decision_thresh\"] + 1e-7), np.inf\nCumulative time: 3.4919593334198\n\n\nProperties batch 4, size 1\nRemaining timeout: 284.4445745944977\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0. -1.  0.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.6754426956176758.\n\nProperties batch 5, size 1\nRemaining timeout: 284.40398049354553\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0. -1.  1.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.21066594123840332.\n\nProperties batch 6, size 1\nRemaining timeout: 284.3735282421112\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1. -1.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.8965559005737305.\n\nProperties batch 7, size 1\nRemaining timeout: 284.3440682888031\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1.  0. -1.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 4.815125465393066.\n\nProperties batch 8, size 1\nRemaining timeout: 284.31157088279724\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  1.  0.  0. -1.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 1.7716398239135742.\nResult: unsat\nTime: 15.722300052642822\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_79_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "7.038072824478149",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpkbrd_vzr.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:03:51 2024 on Cerberus\nInternal results will be saved to /tmp/tmpkbrd_vzr.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_79_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 5.74447012,  4.42454815, -2.42684126,  1.39197063, -0.32172263,\n         -1.55043101, -3.09571075, -1.65020883,  9.32281113,  3.43952250]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 5.94260406,  4.11544514, -2.25063801,  1.45511723, -0.20617530,\n          -1.56343246, -3.12074471, -1.37484074,  8.60002995,  3.29764056],\n         [ 5.94260406,  4.11544514, -2.25063801,  1.45511723, -0.20617530,\n          -1.56343246, -3.12074471, -1.37484074,  8.60002995,  3.29764056]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 2.65742588,  4.48458481, 10.85066795,  7.14491272,  8.80620480,\n          10.16346264, 11.72077465,  9.97487068,  5.30238914]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6214 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 5.74447012,  4.42454815, -2.42684126,  1.39197063, -0.32172263,\n         -1.55043101, -3.09571075, -1.65020883,  9.32281113,  3.43952250]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [651]; unstable size 651; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 409 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 315 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [409]; unstable size 409; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 315 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [315]; unstable size 315; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 29 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [29]; unstable size 29; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.02741289,  3.13008547,  9.57361889,  6.11081219,  7.53568268,\n          9.06027222, 10.50288391,  8.74797821,  4.31495571]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.375462770462036\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_89_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.535550355911255",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpovafsnzl.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:04:00 2024 on Cerberus\nInternal results will be saved to /tmp/tmpovafsnzl.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_89_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.0098334401845932, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 4.45820999,  7.45845509, -3.23980594, -2.27957010, -2.48054481,\n         -2.96445680, -1.49695182,  0.53347808,  2.00121641, 12.13869095]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 4.48702860,  8.27062798, -3.10395789, -2.26830912, -2.43770671,\n          -3.00064445, -1.41095793,  0.37030119,  2.13667798, 11.57998276],\n         [ 4.48702860,  8.27062798, -3.10395789, -2.26830912, -2.43770671,\n          -3.00064445, -1.41095793,  0.37030119,  2.13667798, 11.57998276]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 7.09295416,  3.30935478, 14.68394089, 13.84829140, 14.01768970,\n          14.58062744, 12.99094105, 11.20968151,  9.44330502]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5297 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 4.45820999,  7.45845509, -3.23980594, -2.27957010, -2.48054481,\n         -2.96445680, -1.49695182,  0.53347808,  2.00121641, 12.13869095]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [1424]; unstable size 1424; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using full alpha with unstable size 32 total_size 32 output_shape 32\nlayer /22 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [779]; unstable size 779; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using full alpha with unstable size 119 total_size 128 output_shape 128\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [711]; unstable size 711; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 72 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [72]; unstable size 72; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 3.64581919,  1.50040722, 10.60078812, 10.46342564,  9.78866386,\n         10.80334949,  9.39673996,  6.79292250,  5.55253506]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 4.940890312194824\n"
        },
        {
            "network": "cifar10_2_255_simplified",
            "property": "cifar10_spec_idx_98_eps_0.00784_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.746645927429199",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpxiscyw9l.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:04:08 2024 on Cerberus\nInternal results will be saved to /tmp/tmpxiscyw9l.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.00784_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_2_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.009833455085754395, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[11.13540268, -2.12778544,  5.17189455,  3.04508901,  0.87349188,\n          1.41967106,  3.88908839, -2.58672547, -2.09313250,  1.06364989]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[10.81753254, -2.22862196,  5.32995367,  3.01262641,  0.91334116,\n           1.51196551,  3.93635917, -2.66108012, -2.23742366,  0.85475343],\n         [10.81753254, -2.22862196,  5.32995367,  3.01262641,  0.91334116,\n           1.51196551,  3.93635917, -2.66108012, -2.23742366,  0.85475343]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[13.04615402,  5.48757887,  7.80490589,  9.90419102,  9.30556679,\n           6.88117313, 13.47861290, 13.05495644,  9.96277905]]],\n       device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6895 seconds.\nPGD attack failed\nModel prediction is: tensor([[11.13540268, -2.12778544,  5.17189455,  3.04508901,  0.87349188,\n          1.41967106,  3.88908839, -2.58672547, -2.09313250,  1.06364989]],\n       device='cuda:0')\nlayer /22 using sparse-features alpha with shape [453]; unstable size 453; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /22 start_node /input.4 using sparse-spec alpha with unstable size 255 total_size 8192 output_shape (32, 16, 16)\nlayer /22 start_node /input.8 using sparse-spec alpha with unstable size 185 total_size 8192 output_shape (128, 8, 8)\nlayer /22 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /22 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /24 using sparse-features alpha with shape [255]; unstable size 255; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /24 start_node /input.8 using sparse-spec alpha with unstable size 185 total_size 8192 output_shape (128, 8, 8)\nlayer /24 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /24 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /26 using sparse-features alpha with shape [185]; unstable size 185; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /26 start_node /input.12 using sparse-spec alpha with unstable size 18 total_size 250 output_shape torch.Size([250])\nlayer /26 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /29 using sparse-features alpha with shape [18]; unstable size 18; total size 250 (torch.Size([1, 250]))\nlayer /29 start_node /30 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[11.61046314,  5.03864193,  7.01849842,  8.97960567,  8.52255058,\n          6.12826872, 12.15253353, 12.06939697,  8.96183014]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.157505750656128\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_11_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "8.26267671585083",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpub9m6ebz.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:04:16 2024 on Cerberus\nInternal results will be saved to /tmp/tmpub9m6ebz.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_11_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333730936050415, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.21427953,  1.82282424, -0.95329773, -0.56605446, -0.84413540,\n         -1.23256934, -1.73097754, -0.13519278,  1.34799814,  2.77015758]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.29628807,  2.17531061, -1.04846025, -0.64505720, -0.91715002,\n          -1.33519125, -1.74386621, -0.27812806,  1.57999504,  2.65876412],\n         [ 0.29628807,  2.17531061, -1.04846025, -0.64505720, -0.91715002,\n          -1.33519125, -1.74386621, -0.27812806,  1.57999504,  2.65876412]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[2.36247611, 0.48345351, 3.70722437, 3.30382133, 3.57591414,\n          3.99395537, 4.40263033, 2.93689227, 1.07876909]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.3302 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.21427953,  1.82282424, -0.95329773, -0.56605446, -0.84413540,\n         -1.23256934, -1.73097754, -0.13519278,  1.34799814,  2.77015758]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [271]; unstable size 271; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 72 total_size 128 output_shape 128\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 104 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [764]; unstable size 764; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 104 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [104]; unstable size 104; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 1.12246096, -0.09214255,  2.15115285,  1.82449389,  2.04218793,\n          2.28744054,  2.44691110,  0.59095818, -0.05210555]], device='cuda:0') None\n\nall verified at 15th iter\nbest_l after optimization: 13.648921012878418 with beta sum per layer: []\nalpha/beta optimization time: 1.7373898029327393\ninitial alpha-CROWN bounds: tensor([[1.25593817e+00, 6.03049994e-05, 2.30788589e+00, 1.95581317e+00,\n         2.18180227e+00, 2.43680477e+00, 2.61702156e+00, 8.00340235e-01,\n         9.32547152e-02]], device='cuda:0')\nWorst class: (+ rhs) 6.0304999351501465e-05\nverified with init bound!\nResult: unsat\nTime: 6.74525260925293\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_23_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.6858625411987305",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmp3ns2dzdy.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:04:26 2024 on Cerberus\nInternal results will be saved to /tmp/tmp3ns2dzdy.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_23_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-1.15086710,  3.25605297, -1.23933947, -0.37451699, -0.90260547,\n         -0.36854154, -0.54609567, -0.02293947, -1.39983487,  3.49594378]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-1.04514432,  3.73228550, -1.37737966, -0.42511427, -1.09885812,\n          -0.39768636, -0.55368650, -0.32959464, -1.25875306,  3.67082763],\n         [-1.04514432,  3.73228550, -1.37737966, -0.42511427, -1.09885812,\n          -0.39768636, -0.55368650, -0.32959464, -1.25875306,  3.67082763]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 4.71597195, -0.06145787,  5.04820728,  4.09594202,  4.76968575,\n           4.06851387,  4.22451401,  4.00042248,  4.92958069]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.2512 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.146442651748657\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_39_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.744659900665283",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmps4lmzkkk.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:04:33 2024 on Cerberus\nInternal results will be saved to /tmp/tmps4lmzkkk.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_39_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.85258293,  0.00403488, -0.37607461,  0.47144255, -0.00784957,\n          1.03636110, -0.47195727, -0.07978809,  0.87156272, -0.89035553]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.85305285,  0.83073950, -0.57770157,  0.24319662, -0.20337465,\n           0.67435169, -0.66313702, -0.24651816,  0.99708223, -0.31799167],\n         [-0.85305285,  0.83073950, -0.57770157,  0.24319662, -0.20337465,\n           0.67435169, -0.66313702, -0.24651816,  0.99708223, -0.31799167]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 1.52740455, -0.15638781,  1.25205326,  0.43115509,  0.87772632,\n           1.33748865,  0.92086983, -0.32273054,  0.99234337]]],\n       device='cuda:0')\nnumber of violation:  2\nAttack finished in 1.3120 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.186899662017822\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_55_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "9.280189990997314",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpsk4qm47b.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:04:40 2024 on Cerberus\nInternal results will be saved to /tmp/tmpsk4qm47b.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_55_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333704859018326, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.42585230,  0.08431870, -0.01928085, -0.90985799,  0.06956941,\n         -1.05303371, -0.86464828, -0.79066396,  2.44238663,  0.17502323]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.44630742,  0.24157470, -0.05671391, -0.90471673,  0.04474342,\n          -0.98103559, -0.92129189, -0.63299358,  2.13165689,  0.16863680],\n         [ 1.44630742,  0.24157470, -0.05671391, -0.90471673,  0.04474342,\n          -0.98103559, -0.92129189, -0.63299358,  2.13165689,  0.16863680]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[0.68534946, 1.89008212, 2.18837070, 3.03637362, 2.08691359,\n          3.11269236, 3.05294871, 2.76465034, 1.96302009]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.7141 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.42585230,  0.08431870, -0.01928085, -0.90985799,  0.06956941,\n         -1.05303371, -0.86464828, -0.79066396,  2.44238663,  0.17502323]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [239]; unstable size 239; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 65 total_size 128 output_shape 128\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 109 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [1126]; unstable size 1126; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 109 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [109]; unstable size 109; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[-0.02372168,  0.26599985,  0.59504092,  1.23817968,  0.60576904,\n          1.16680110,  1.19513106,  0.77421403,  0.18743014]], device='cuda:0') None\n\nall verified at 2th iter\nbest_l after optimization: 6.4941630363464355 with beta sum per layer: []\nalpha/beta optimization time: 1.897547721862793\ninitial alpha-CROWN bounds: tensor([[0.01864122, 0.32097566, 0.64471412, 1.29487765, 0.65704846, 1.22855031,\n         1.24993277, 0.84288275, 0.23654032]], device='cuda:0')\nWorst class: (+ rhs) 0.018641218543052673\nverified with init bound!\nResult: unsat\nTime: 7.5427045822143555\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_74_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "7.308282375335693",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpp0yply_6.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:04:52 2024 on Cerberus\nInternal results will be saved to /tmp/tmpp0yply_6.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_74_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.99456906,  0.92221719, -0.18530147, -1.26380289,  0.52791083,\n         -2.13896990, -2.99637532,  0.54190046,  1.94590044,  1.81956339]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.79191017,  0.95040131, -0.33032101, -1.12356019,  0.31390333,\n          -1.99346793, -2.62409258,  0.25408816,  2.13896036,  1.78995204],\n         [ 1.79191017,  0.95040131, -0.33032101, -1.12356019,  0.31390333,\n          -1.99346793, -2.62409258,  0.25408816,  2.13896036,  1.78995204]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 8.41508865e-01,  2.12223125e+00,  2.91547036e+00,  1.47800684e+00,\n           3.78537798e+00,  4.41600275e+00,  1.53782201e+00, -3.47050190e-01,\n           1.95813179e-03]]], device='cuda:0')\nnumber of violation:  1\nAttack finished in 1.3653 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.927107095718384\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_86_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.864334344863892",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpb4896tr3.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:00 2024 on Cerberus\nInternal results will be saved to /tmp/tmpb4896tr3.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_86_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333708584308624, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.14482355,  0.42702734,  1.17975521,  0.42308298, -0.50366199,\n          0.39120865, -1.85243201, -0.93027544,  0.79907751,  0.54692328]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.13597828,  0.66834402,  0.47835025,  0.33799794, -0.37628934,\n           0.34226054, -1.88110042, -0.92241907,  0.90814900,  0.81491107],\n         [ 0.13597828,  0.66834402,  0.47835025,  0.33799794, -0.37628934,\n           0.34226054, -1.88110042, -0.92241907,  0.90814900,  0.81491107]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 0.34237197, -0.18999377,  0.14035231,  0.85463959,  0.13608971,\n           2.35945058,  1.40076935, -0.42979875, -0.33656082]]],\n       device='cuda:0')\nnumber of violation:  3\nAttack finished in 1.3631 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 4.258336067199707\n"
        },
        {
            "network": "cifar10_8_255_simplified",
            "property": "cifar10_spec_idx_98_eps_0.03137_n1",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.7861008644104",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmps15maie0.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:07 2024 on Cerberus\nInternal results will be saved to /tmp/tmps15maie0.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_98_eps_0.03137_n1.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/cifar10_8_255_simplified.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.039333730936050415, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 3.94495416, -2.07960415,  1.75831628,  0.85984302, -0.04009467,\n         -0.15203522, -0.26500970, -0.92295241, -1.24032235, -1.26781774]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 3.64041114, -2.15951562,  1.84199905,  0.90193647, -0.02822447,\n          -0.08333384, -0.13002752, -0.84000218, -1.27070475, -1.38328218],\n         [ 3.64041114, -2.15951562,  1.84199905,  0.90193647, -0.02822447,\n          -0.08333384, -0.13002752, -0.84000218, -1.27070475, -1.38328218]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[5.79992676, 1.79841208, 2.73847461, 3.66863561, 3.72374487,\n          3.77043867, 4.48041344, 4.91111565, 5.02369308]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.3141 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 3.94495416, -2.07960415,  1.75831628,  0.85984302, -0.04009467,\n         -0.15203522, -0.26500970, -0.92295241, -1.24032235, -1.26781774]],\n       device='cuda:0')\nlayer /18 using sparse-features alpha with shape [103]; unstable size 103; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /18 start_node /input.4 using sparse-spec alpha with unstable size 219 total_size 8192 output_shape (128, 8, 8)\nlayer /18 start_node /input.8 using sparse-spec alpha with unstable size 41 total_size 250 output_shape torch.Size([250])\nlayer /18 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /20 using sparse-features alpha with shape [219]; unstable size 219; total size 8192 (torch.Size([1, 128, 8, 8]))\nlayer /20 start_node /input.8 using sparse-spec alpha with unstable size 41 total_size 250 output_shape torch.Size([250])\nlayer /20 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /23 using sparse-features alpha with shape [41]; unstable size 41; total size 250 (torch.Size([1, 250]))\nlayer /23 start_node /24 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[5.12637568, 1.59213459, 2.38341951, 3.30685139, 3.30003595, 3.32878542,\n         3.79563498, 4.43923378, 4.36108780]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.2088775634765625\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_10_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.7540798187255859",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_10_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.668476819992065",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpb1fgr9qv.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:15 2024 on Cerberus\nInternal results will be saved to /tmp/tmpb1fgr9qv.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_10_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 1.43827474, -5.26821756, -0.24924204, -1.00314927, -1.42481220,\n         -1.00641870, -2.44114733, -2.40455437,  0.35379457, -4.80686331]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 1.35088444, -5.26208687, -0.27259591, -0.93527448, -1.39557958,\n          -0.94545192, -2.31221080, -2.32778692,  0.55532157, -4.74117327],\n         [ 1.35088444, -5.26208687, -0.27259591, -0.93527448, -1.39557958,\n          -0.94545192, -2.31221080, -2.32778692,  0.55532157, -4.74117327]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[6.61297131, 1.62348032, 2.28615904, 2.74646401, 2.29633641,\n          3.66309524, 3.67867136, 0.79556286, 6.09205770]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6527 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 1.43827474, -5.26821756, -0.24924204, -1.00314927, -1.42481220,\n         -1.00641870, -2.44114733, -2.40455437,  0.35379457, -4.80686331]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [3119]; unstable size 3119; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 153 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 49 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [153]; unstable size 153; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 49 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1252]; unstable size 1252; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 79 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [79]; unstable size 79; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 41 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [41]; unstable size 41; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 67 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [67]; unstable size 67; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[5.06930304, 0.73012829, 1.20831919, 1.51309657, 0.97737694, 2.18857384,\n         2.55259347, 0.01476750, 4.82835960]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.196306228637695\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_21_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.7171075344085693",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_21_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.639968395233154",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpt1ievu9g.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:23 2024 on Cerberus\nInternal results will be saved to /tmp/tmpt1ievu9g.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_21_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 2.14061451, -7.58005762,  1.55118299, -0.69311768, -1.58620059,\n         -3.66751027, -4.57932281, -5.63344622, -4.01265717, -7.37496614]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 2.07994413, -7.58967447,  1.58834994, -0.70260131, -1.58201480,\n          -3.65404916, -4.56366301, -5.60641527, -4.04987907, -7.38536358],\n         [ 2.07994413, -7.58967447,  1.58834994, -0.70260131, -1.58201480,\n          -3.65404916, -4.56366301, -5.60641527, -4.04987907, -7.38536358]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[9.66961861, 0.49159420, 2.78254557, 3.66195893, 5.73399353,\n          6.64360714, 7.68635941, 6.12982321, 9.46530724]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6421 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 2.14061451, -7.58005762,  1.55118299, -0.69311768, -1.58620059,\n         -3.66751027, -4.57932281, -5.63344622, -4.01265717, -7.37496614]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [3565]; unstable size 3565; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 12 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 282 total_size 16384 output_shape (64, 16, 16)\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [12]; unstable size 12; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 282 total_size 16384 output_shape (64, 16, 16)\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [282]; unstable size 282; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 14 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [14]; unstable size 14; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 7 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [7]; unstable size 7; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 9 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [9]; unstable size 9; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[9.27485943, 0.43070441, 2.65081620, 3.50906682, 5.58280325, 6.43017006,\n         7.46400595, 5.93557167, 9.23428917]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.095215320587158\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_37_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.8088569641113281",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_37_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.573041677474976",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpw7ksdiz6.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:31 2024 on Cerberus\nInternal results will be saved to /tmp/tmpw7ksdiz6.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_37_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.25571486,  3.77851868, -4.00916004, -3.22680521, -4.53054523,\n         -4.37565517, -5.53191614, -4.20376968,  0.47601944,  3.14984918]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.21722656,  3.55940342, -3.96903133, -3.15021801, -4.50026608,\n          -4.26555204, -5.46244669, -4.09355307,  0.32603645,  3.24944854],\n         [ 0.21722656,  3.55940342, -3.96903133, -3.15021801, -4.50026608,\n          -4.26555204, -5.46244669, -4.09355307,  0.32603645,  3.24944854]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.34217691, 7.52843475, 6.70962143, 8.05966949, 7.82495546,\n          9.02185059, 7.65295649, 3.23336697, 0.30995488]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5897 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.25571486,  3.77851868, -4.00916004, -3.22680521, -4.53054523,\n         -4.37565517, -5.53191614, -4.20376968,  0.47601944,  3.14984918]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2147]; unstable size 2147; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 70 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [70]; unstable size 70; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [731]; unstable size 731; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 60 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [60]; unstable size 60; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 24 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [24]; unstable size 24; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 42 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [42]; unstable size 42; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[2.43055367, 6.70373583, 6.04302597, 7.19202232, 7.07307339, 8.29016685,\n         6.64738464, 2.35014248, 0.03637803]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.0520408153533936\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_50_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.7486631870269775",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_50_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "6.560091257095337",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpiclmmwbh.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:39 2024 on Cerberus\nInternal results will be saved to /tmp/tmpiclmmwbh.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_50_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-0.88575494, -2.46948004, -0.98218197, -1.86838031, -2.75453448,\n         -3.04634857, -3.35564113, -0.76873565, -1.40446281,  0.81824809]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-0.84236693, -2.81302500, -0.87701511, -1.80246806, -2.65725541,\n          -2.97314787, -3.35251307, -0.61052889, -1.43700242,  0.62772131],\n         [-0.84236693, -2.81302500, -0.87701511, -1.80246806, -2.65725541,\n          -2.97314787, -3.35251307, -0.61052889, -1.43700242,  0.62772131]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.47008824, 3.44074631, 1.50473642, 2.43018937, 3.28497672,\n          3.60086918, 3.98023438, 1.23825026, 2.06472373]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.5676 seconds.\nPGD attack failed\nModel prediction is: tensor([[-0.88575494, -2.46948004, -0.98218197, -1.86838031, -2.75453448,\n         -3.04634857, -3.35564113, -0.76873565, -1.40446281,  0.81824809]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2520]; unstable size 2520; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 126 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [126]; unstable size 126; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [956]; unstable size 956; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 68 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [68]; unstable size 68; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [27]; unstable size 27; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 27 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [27]; unstable size 27; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[1.05932486, 2.63591647, 1.12628865, 2.04380560, 2.82444191, 3.17685556,\n         3.54555607, 0.90777087, 1.59514737]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 5.077543020248413\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_64_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.7592246532440186",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_64_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "9.393116235733032",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpqlgvc1ox.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:47 2024 on Cerberus\nInternal results will be saved to /tmp/tmpqlgvc1ox.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_64_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.04189730, -1.39616895,  0.58027250,  0.29884806,  0.46679962,\n          0.18952516,  1.31852853, -0.25026628, -4.26790476, -1.34158278]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.01591706, -1.55922532,  0.63024652,  0.30251116,  0.53322899,\n           0.23309767,  1.22137964, -0.18820685, -4.19713736, -1.40943885],\n         [-2.01591706, -1.55922532,  0.63024652,  0.30251116,  0.53322899,\n           0.23309767,  1.22137964, -0.18820685, -4.19713736, -1.40943885]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.23729658, 2.78060484, 0.59113312, 0.91886848, 0.68815064,\n          0.98828197, 1.40958643, 5.41851711, 2.63081837]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6172 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.04189730, -1.39616895,  0.58027250,  0.29884806,  0.46679962,\n          0.18952516,  1.31852853, -0.25026628, -4.26790476, -1.34158278]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2044]; unstable size 2044; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 138 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 50 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [138]; unstable size 138; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 50 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1413]; unstable size 1413; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 124 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [124]; unstable size 124; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 49 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [49]; unstable size 49; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 90 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [90]; unstable size 90; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 1.81039071,  0.72419584, -0.10799134,  0.31532452,  0.10968959,\n          0.27898484,  0.39123270,  3.65472770,  0.96664190]], device='cuda:0') None\n\nall verified at 11th iter\nbest_l after optimization: 9.575695037841797 with beta sum per layer: []\nalpha/beta optimization time: 2.6363344192504883\ninitial alpha-CROWN bounds: tensor([[1.99569702e+00, 1.03388250e+00, 2.20683217e-03, 4.26993877e-01,\n         1.97593808e-01, 3.70850712e-01, 5.36541998e-01, 3.82398367e+00,\n         1.18794513e+00]], device='cuda:0')\nWorst class: (+ rhs) 0.00220683217048645\nverified with init bound!\nResult: unsat\nTime: 7.827572345733643\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_78_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.8105766773223877",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_78_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "167.6458671092987",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpxen9jcvk.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:05:58 2024 on Cerberus\nInternal results will be saved to /tmp/tmpxen9jcvk.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_78_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n          1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[-2.12218428, -3.37873507,  0.83384383,  1.15502465,  0.81430137,\n           1.14255941,  0.72699016, -0.51800579, -3.24479532, -3.37587047],\n         [-2.12218428, -3.37873507,  0.83384383,  1.15502465,  0.81430137,\n           1.14255941,  0.72699016, -0.51800579, -3.24479532, -3.37587047]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[3.27720881, 4.53375959, 0.32118082, 0.34072328, 0.01246524,\n          0.42803448, 1.67303038, 4.39981985, 4.53089523]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.6303 seconds.\nPGD attack failed\nModel prediction is: tensor([[-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n          1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [1942]; unstable size 1942; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 171 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [171]; unstable size 171; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1141]; unstable size 1141; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 81 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [81]; unstable size 81; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 26 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [26]; unstable size 26; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 50 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [50]; unstable size 50; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[ 2.46627474,  3.58290315,  0.00764871,  0.03963888, -0.11114848,\n         -0.01987123,  1.07829618,  3.52050567,  3.87006450]], device='cuda:0') None\nbest_l after optimization: 14.863643646240234 with beta sum per layer: []\nalpha/beta optimization time: 9.751640796661377\ninitial alpha-CROWN bounds: tensor([[ 2.54750204e+00,  3.65459013e+00,  4.54752445e-02,  7.04847574e-02,\n         -9.75859165e-02,  2.63571739e-03,  1.12346411e+00,  3.59175682e+00,\n          3.92532063e+00]], device='cuda:0')\nWorst class: (+ rhs) -0.09758591651916504\nTotal VNNLIB file length: 9, max property batch size: 1, total number of batches: 9\nlA shape: [torch.Size([1, 9, 32, 32, 32]), torch.Size([1, 9, 32, 16, 16]), torch.Size([1, 9, 64, 16, 16]), torch.Size([1, 9, 64, 8, 8]), torch.Size([1, 9, 512]), torch.Size([1, 9, 512])]\n\nProperties batch 0, size 1\nRemaining timeout: 285.0880341529846\n##### Instance 0 first 10 spec matrices: [[[-1.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 2.547502040863037.\n\nProperties batch 1, size 1\nRemaining timeout: 285.04878282546997\n##### Instance 0 first 10 spec matrices: [[[ 0. -1.  0.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 3.654590129852295.\n\nProperties batch 2, size 1\nRemaining timeout: 285.02320408821106\n##### Instance 0 first 10 spec matrices: [[[ 0.  0. -1.  1.  0.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.04547524452209473.\n\nProperties batch 3, size 1\nRemaining timeout: 284.9996888637543\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  1. -1.  0.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nInitial alpha-CROWN verified for spec index [0] with bound 0.07048475742340088.\n\nProperties batch 4, size 1\nRemaining timeout: 284.97550773620605\n##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  1.  0. -1.  0.  0.  0.  0.]]]\nthresholds: [0.] ######\nRemaining spec index [0] with bounds tensor([[-0.09758592]], device='cuda:0') need to verify.\nModel prediction is: tensor([-2.14970326, -3.42058086,  0.84200227,  1.14106989,  0.81408489,\n         1.07924342,  0.76167828, -0.52608585, -3.17128658, -3.46410918],\n       device='cuda:0')\nbuild_the_model_with_refined_bounds batch [0/1]\nsetting alpha for layer /34 start_node /46 with alignment adjustment\nsetting alpha for layer /36 start_node /46 with alignment adjustment\nsetting alpha for layer /38 start_node /46 with alignment adjustment\nsetting alpha for layer /40 start_node /46 with alignment adjustment\nsetting alpha for layer /43 start_node /46 with alignment adjustment\nsetting alpha for layer /45 start_node /46 with alignment adjustment\nall slope initialized\ndirectly get lb and ub from refined bounds\nlA shapes: [torch.Size([1, 1, 32, 32, 32]), torch.Size([1, 1, 32, 16, 16]), torch.Size([1, 1, 64, 16, 16]), torch.Size([1, 1, 64, 8, 8]), torch.Size([1, 1, 512]), torch.Size([1, 1, 512])]\nc shape: torch.Size([1, 1, 10])\nalpha-CROWN with fixed intermediate bounds: tensor([[-0.09758592]], device='cuda:0') tensor([[inf]], device='cuda:0')\nKeeping slopes for these layers: ['/46']\nKeeping slopes for these layers: ['/46']\nlayer 0 size torch.Size([32768]) unstable 1942\nlayer 1 size torch.Size([8192]) unstable 166\nlayer 2 size torch.Size([16384]) unstable 1113\nlayer 3 size torch.Size([4096]) unstable 77\nlayer 4 size torch.Size([512]) unstable 25\nlayer 5 size torch.Size([512]) unstable 49\n-----------------\n# of unstable neurons: 3372\n-----------------\n\nbatch:  torch.Size([1, 32, 32, 32]) pre split depth:  5\nbatch:  torch.Size([1, 32, 32, 32]) post split depth:  5\nsplitting decisions: \nsplit level 0: [5, 112] \nsplit level 1: [4, 444] \nsplit level 2: [5, 419] \nsplit level 3: [5, 444] \nsplit level 4: [5, 202] \n(32, 3, 32, 32) torch.Size([32, 1, 10]) torch.Size([32, 1])\npruning_in_iteration open status: True\nratio of positive domain = 14 / 32 = 0.4375\npruning-in-iteration extra time: 0.030731201171875\nTensors transferred: pre=3.8125M lA=1.0723M alpha=0.2082M beta=0.0002M\nThis batch time : update_bounds func: 0.9421\t prepare: 0.0039\t bound: 0.9294\t transfer: 0.0082\t finalize: 0.0006\nAccumulated time: update_bounds func: 0.9421\t prepare: 0.0039\t bound: 0.9294\t transfer: 0.0082\t finalize: 0.0006\nbatch bounding time:  0.9421567916870117\nCurrent worst splitting domains lb-rhs (depth):\n-0.06755 (5), -0.05573 (5), -0.05415 (5), -0.04667 (5), -0.04550 (5), -0.03473 (5), -0.02976 (5), -0.02654 (5), -0.02516 (5), -0.02263 (5), -0.01366 (5), -0.01080 (5), -0.00789 (5), -0.00785 (5), -0.00581 (5), -0.00557 (5), -0.00478 (5), -0.00445 (5), \nlength of domains: 18\nTotal time: 1.2379\t pickout: 0.0010\t decision: 0.2843\t get_bound: 0.9493\t add_domain: 0.0034\nAccumulated time:\t pickout: 0.0010\t decision: 0.2843\t get_bound: 0.9493\t add_domain: 0.0034\nCurrent (lb-rhs): -0.0675499439239502\n14 domains visited\nCumulative time: 1.4902386665344238\n\nbatch:  torch.Size([18, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([18, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 36] [4, 166] [4, 36] [4, 166] [4, 36] [4, 395] [4, 36] [4, 395] [4, 36] [4, 395] \n(36, 3, 32, 32) torch.Size([36, 1, 10]) torch.Size([36, 1])\npruning_in_iteration open status: True\nratio of positive domain = 12 / 36 = 0.33333333333333337\npruning-in-iteration extra time: 0.030669212341308594\nTensors transferred: pre=4.2891M lA=1.4297M alpha=0.2342M beta=0.0002M\nThis batch time : update_bounds func: 0.7592\t prepare: 0.0027\t bound: 0.7500\t transfer: 0.0057\t finalize: 0.0006\nAccumulated time: update_bounds func: 1.7012\t prepare: 0.0066\t bound: 1.6794\t transfer: 0.0139\t finalize: 0.0012\nbatch bounding time:  0.7592437267303467\nCurrent worst splitting domains lb-rhs (depth):\n-0.05872 (6), -0.05512 (6), -0.04727 (6), -0.04578 (6), -0.04182 (6), -0.04007 (6), -0.03942 (6), -0.03905 (6), -0.03698 (6), -0.03044 (6), -0.02910 (6), -0.02629 (6), -0.02538 (6), -0.02119 (6), -0.02098 (6), -0.01909 (6), -0.01895 (6), -0.01839 (6), -0.01356 (6), -0.00458 (6), \nlength of domains: 24\nTotal time: 0.7931\t pickout: 0.0011\t decision: 0.0282\t get_bound: 0.7593\t add_domain: 0.0045\nAccumulated time:\t pickout: 0.0021\t decision: 0.3125\t get_bound: 1.7086\t add_domain: 0.0078\nCurrent (lb-rhs): -0.0587158203125\n26 domains visited\nCumulative time: 2.2836437225341797\n\nbatch:  torch.Size([24, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([24, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 316] [4, 395] [4, 316] [4, 395] [4, 316] [4, 166] [4, 359] [4, 166] [4, 359] [4, 166] \n(48, 3, 32, 32) torch.Size([48, 1, 10]) torch.Size([48, 1])\npruning_in_iteration open status: True\nratio of positive domain = 10 / 48 = 0.20833333333333337\npruning-in-iteration extra time: 0.0077669620513916016\nTensors transferred: pre=5.7188M lA=2.2637M alpha=0.3123M beta=0.0004M\nThis batch time : update_bounds func: 0.6817\t prepare: 0.0036\t bound: 0.6681\t transfer: 0.0091\t finalize: 0.0008\nAccumulated time: update_bounds func: 2.3830\t prepare: 0.0102\t bound: 2.3476\t transfer: 0.0230\t finalize: 0.0021\nbatch bounding time:  0.6818392276763916\nCurrent worst splitting domains lb-rhs (depth):\n-0.05447 (7), -0.05081 (7), -0.04514 (7), -0.04289 (7), -0.04260 (7), -0.04246 (7), -0.03844 (7), -0.03811 (7), -0.03644 (7), -0.03600 (7), -0.03532 (7), -0.03352 (7), -0.03320 (7), -0.03210 (7), -0.03058 (7), -0.02986 (7), -0.02700 (7), -0.02597 (7), -0.02535 (7), -0.02505 (7), \nlength of domains: 38\nTotal time: 0.7268\t pickout: 0.0013\t decision: 0.0382\t get_bound: 0.6819\t add_domain: 0.0054\nAccumulated time:\t pickout: 0.0034\t decision: 0.3507\t get_bound: 2.3905\t add_domain: 0.0132\nCurrent (lb-rhs): -0.05446732044219971\n36 domains visited\nCumulative time: 3.0114214420318604\n\nbatch:  torch.Size([38, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([38, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 413] [5, 413] [5, 413] [4, 316] [5, 413] [4, 316] [5, 413] [4, 316] [4, 166] [5, 413] \n(76, 3, 32, 32) torch.Size([76, 1, 10]) torch.Size([76, 1])\npruning_in_iteration open status: False\nratio of positive domain = 7 / 76 = 0.09210526315789469\npruning-in-iteration extra time: 0.00016117095947265625\nTensors transferred: pre=9.0547M lA=4.5273M alpha=0.4945M beta=0.0007M\nThis batch time : update_bounds func: 0.7057\t prepare: 0.0056\t bound: 0.6806\t transfer: 0.0175\t finalize: 0.0018\nAccumulated time: update_bounds func: 3.0887\t prepare: 0.0158\t bound: 3.0282\t transfer: 0.0404\t finalize: 0.0039\nbatch bounding time:  0.7057375907897949\nCurrent worst splitting domains lb-rhs (depth):\n-0.05066 (8), -0.04708 (8), -0.04703 (8), -0.04333 (8), -0.04163 (8), -0.03980 (8), -0.03930 (8), -0.03879 (8), -0.03871 (8), -0.03560 (8), -0.03538 (8), -0.03508 (8), -0.03388 (8), -0.03363 (8), -0.03363 (8), -0.03348 (8), -0.03304 (8), -0.03292 (8), -0.03187 (8), -0.03141 (8), \nlength of domains: 69\nTotal time: 0.7753\t pickout: 0.0012\t decision: 0.0594\t get_bound: 0.7058\t add_domain: 0.0088\nAccumulated time:\t pickout: 0.0046\t decision: 0.4102\t get_bound: 3.0963\t add_domain: 0.0220\nCurrent (lb-rhs): -0.05066478252410889\n43 domains visited\nCumulative time: 3.787858486175537\n\nbatch:  torch.Size([69, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([69, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 166] [4, 166] [4, 89] [5, 413] [4, 359] [5, 413] [4, 359] [5, 413] [4, 166] [4, 89] \n(138, 3, 32, 32) torch.Size([138, 1, 10]) torch.Size([138, 1])\npruning_in_iteration open status: False\nratio of positive domain = 22 / 138 = 0.1594202898550725\npruning-in-iteration extra time: 0.00021910667419433594\nTensors transferred: pre=16.4414M lA=8.2207M alpha=0.8978M beta=0.0013M\nThis batch time : update_bounds func: 0.7499\t prepare: 0.0077\t bound: 0.7099\t transfer: 0.0299\t finalize: 0.0023\nAccumulated time: update_bounds func: 3.8386\t prepare: 0.0235\t bound: 3.7381\t transfer: 0.0703\t finalize: 0.0062\nbatch bounding time:  0.7499914169311523\nCurrent worst splitting domains lb-rhs (depth):\n-0.04770 (9), -0.04637 (9), -0.04419 (9), -0.04410 (9), -0.04219 (9), -0.04212 (9), -0.04041 (9), -0.03856 (9), -0.03784 (9), -0.03745 (9), -0.03717 (9), -0.03642 (9), -0.03585 (9), -0.03578 (9), -0.03458 (9), -0.03320 (9), -0.03319 (9), -0.03233 (9), -0.03229 (9), -0.03104 (9), \nlength of domains: 116\nTotal time: 0.8303\t pickout: 0.0029\t decision: 0.0613\t get_bound: 0.7501\t add_domain: 0.0160\nAccumulated time:\t pickout: 0.0076\t decision: 0.4714\t get_bound: 3.8463\t add_domain: 0.0380\nCurrent (lb-rhs): -0.04769933223724365\n65 domains visited\nCumulative time: 4.619576692581177\n\nbatch:  torch.Size([116, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([116, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 89] [5, 281] [4, 395] [4, 395] [4, 89] [5, 281] [4, 89] [5, 281] [4, 166] [4, 395] \n(232, 3, 32, 32) torch.Size([232, 1, 10]) torch.Size([232, 1])\npruning_in_iteration open status: True\nratio of positive domain = 53 / 232 = 0.22844827586206895\npruning-in-iteration extra time: 0.0273892879486084\nTensors transferred: pre=27.6406M lA=10.6631M alpha=1.5094M beta=0.0027M\nThis batch time : update_bounds func: 0.9299\t prepare: 0.0133\t bound: 0.8673\t transfer: 0.0455\t finalize: 0.0036\nAccumulated time: update_bounds func: 4.7685\t prepare: 0.0368\t bound: 4.6054\t transfer: 0.1158\t finalize: 0.0098\nbatch bounding time:  0.9300410747528076\nCurrent worst splitting domains lb-rhs (depth):\n-0.04478 (10), -0.04330 (10), -0.04140 (10), -0.04135 (10), -0.04002 (10), -0.03923 (10), -0.03905 (10), -0.03863 (10), -0.03750 (10), -0.03653 (10), -0.03563 (10), -0.03554 (10), -0.03529 (10), -0.03463 (10), -0.03442 (10), -0.03376 (10), -0.03374 (10), -0.03307 (10), -0.03290 (10), -0.03279 (10), \nlength of domains: 179\nTotal time: 1.0477\t pickout: 0.0049\t decision: 0.0873\t get_bound: 0.9301\t add_domain: 0.0253\nAccumulated time:\t pickout: 0.0125\t decision: 0.5588\t get_bound: 4.7764\t add_domain: 0.0634\nCurrent (lb-rhs): -0.0447840690612793\n118 domains visited\nCumulative time: 5.669264554977417\n\nbatch:  torch.Size([179, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([179, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 395] [4, 166] [4, 166] [5, 281] [4, 348] [4, 359] [5, 413] [5, 281] [4, 395] [4, 166] \n(358, 3, 32, 32) torch.Size([358, 1, 10]) torch.Size([358, 1])\npruning_in_iteration open status: True\nratio of positive domain = 109 / 358 = 0.3044692737430168\npruning-in-iteration extra time: 0.0321650505065918\nTensors transferred: pre=42.6523M lA=14.8330M alpha=2.3291M beta=0.0044M\nThis batch time : update_bounds func: 1.1015\t prepare: 0.0173\t bound: 1.0283\t transfer: 0.0503\t finalize: 0.0053\nAccumulated time: update_bounds func: 5.8699\t prepare: 0.0540\t bound: 5.6337\t transfer: 0.1661\t finalize: 0.0151\nbatch bounding time:  1.1015918254852295\nCurrent worst splitting domains lb-rhs (depth):\n-0.04209 (11), -0.04013 (11), -0.03864 (11), -0.03861 (11), -0.03725 (11), -0.03589 (11), -0.03584 (11), -0.03572 (11), -0.03476 (11), -0.03361 (11), -0.03359 (11), -0.03281 (11), -0.03281 (11), -0.03208 (11), -0.03164 (11), -0.03092 (11), -0.03074 (11), -0.03073 (11), -0.03063 (11), -0.03037 (11), \nlength of domains: 249\nTotal time: 1.2673\t pickout: 0.0071\t decision: 0.1311\t get_bound: 1.1017\t add_domain: 0.0275\nAccumulated time:\t pickout: 0.0196\t decision: 0.6899\t get_bound: 5.8781\t add_domain: 0.0909\nCurrent (lb-rhs): -0.04209423065185547\n227 domains visited\nCumulative time: 6.937971353530884\n\nbatch:  torch.Size([249, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([249, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [4, 89] [4, 348] [4, 348] [4, 89] [4, 348] [4, 348] [5, 281] [4, 89] [5, 281] \n(498, 3, 32, 32) torch.Size([498, 1, 10]) torch.Size([498, 1])\npruning_in_iteration open status: True\nratio of positive domain = 166 / 498 = 0.33333333333333337\npruning-in-iteration extra time: 0.033935546875\nTensors transferred: pre=59.3320M lA=19.7773M alpha=3.2400M beta=0.0071M\nThis batch time : update_bounds func: 1.3789\t prepare: 0.0236\t bound: 1.2709\t transfer: 0.0768\t finalize: 0.0073\nAccumulated time: update_bounds func: 7.2489\t prepare: 0.0776\t bound: 6.9046\t transfer: 0.2428\t finalize: 0.0224\nbatch bounding time:  1.3791286945343018\nCurrent worst splitting domains lb-rhs (depth):\n-0.04000 (12), -0.03833 (12), -0.03647 (12), -0.03643 (12), -0.03630 (12), -0.03516 (12), -0.03432 (12), -0.03401 (12), -0.03391 (12), -0.03386 (12), -0.03378 (12), -0.03340 (12), -0.03244 (12), -0.03138 (12), -0.03110 (12), -0.03104 (12), -0.03094 (12), -0.03082 (12), -0.03076 (12), -0.03047 (12), \nlength of domains: 332\nTotal time: 1.6049\t pickout: 0.0090\t decision: 0.1682\t get_bound: 1.3792\t add_domain: 0.0485\nAccumulated time:\t pickout: 0.0286\t decision: 0.8581\t get_bound: 7.2573\t add_domain: 0.1394\nCurrent (lb-rhs): -0.03999733924865723\n393 domains visited\nCumulative time: 8.544559478759766\n\nbatch:  torch.Size([332, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([332, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [4, 348] [5, 281] \n(664, 3, 32, 32) torch.Size([664, 1, 10]) torch.Size([664, 1])\npruning_in_iteration open status: True\nratio of positive domain = 315 / 664 = 0.4743975903614458\npruning-in-iteration extra time: 0.06929969787597656\nTensors transferred: pre=79.1094M lA=20.7900M alpha=4.3200M beta=0.0108M\nThis batch time : update_bounds func: 1.6178\t prepare: 0.0316\t bound: 1.4962\t transfer: 0.0799\t finalize: 0.0093\nAccumulated time: update_bounds func: 8.8667\t prepare: 0.1092\t bound: 8.4008\t transfer: 0.3228\t finalize: 0.0318\nbatch bounding time:  1.6179885864257812\nCurrent worst splitting domains lb-rhs (depth):\n-0.03834 (13), -0.03612 (13), -0.03471 (13), -0.03469 (13), -0.03453 (13), -0.03336 (13), -0.03205 (13), -0.03180 (13), -0.03179 (13), -0.03159 (13), -0.03142 (13), -0.03136 (13), -0.03074 (13), -0.02953 (13), -0.02950 (13), -0.02911 (13), -0.02905 (13), -0.02877 (13), -0.02873 (13), -0.02861 (13), \nlength of domains: 349\nTotal time: 2.3024\t pickout: 0.0129\t decision: 0.6301\t get_bound: 1.6181\t add_domain: 0.0414\nAccumulated time:\t pickout: 0.0415\t decision: 1.4881\t get_bound: 8.8754\t add_domain: 0.1807\nCurrent (lb-rhs): -0.03834211826324463\n708 domains visited\nCumulative time: 10.849753379821777\n\nbatch:  torch.Size([349, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([349, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 348] [5, 45] [5, 45] [5, 45] [5, 45] [5, 45] [4, 348] [5, 281] [5, 45] [5, 45] \n(698, 3, 32, 32) torch.Size([698, 1, 10]) torch.Size([698, 1])\npruning_in_iteration open status: True\nratio of positive domain = 368 / 698 = 0.5272206303724929\npruning-in-iteration extra time: 0.039684295654296875\nTensors transferred: pre=83.1602M lA=19.7178M alpha=4.5412M beta=0.0120M\nThis batch time : update_bounds func: 1.8278\t prepare: 0.0313\t bound: 1.7030\t transfer: 0.0830\t finalize: 0.0100\nAccumulated time: update_bounds func: 10.6944\t prepare: 0.1404\t bound: 10.1037\t transfer: 0.4057\t finalize: 0.0417\nbatch bounding time:  1.8279364109039307\nCurrent worst splitting domains lb-rhs (depth):\n-0.03685 (14), -0.03455 (14), -0.03322 (14), -0.03321 (14), -0.03289 (14), -0.03191 (14), -0.03046 (14), -0.03038 (14), -0.03017 (14), -0.02998 (14), -0.02988 (14), -0.02975 (14), -0.02924 (14), -0.02843 (14), -0.02806 (14), -0.02780 (14), -0.02778 (14), -0.02721 (14), -0.02711 (14), -0.02703 (14), \nlength of domains: 330\nTotal time: 3.3453\t pickout: 0.0123\t decision: 1.4591\t get_bound: 1.8280\t add_domain: 0.0459\nAccumulated time:\t pickout: 0.0538\t decision: 2.9472\t get_bound: 10.7034\t add_domain: 0.2266\nCurrent (lb-rhs): -0.03684568405151367\n1076 domains visited\nCumulative time: 14.198407888412476\n\nbatch:  torch.Size([330, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([330, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] [5, 281] \n(660, 3, 32, 32) torch.Size([660, 1, 10]) torch.Size([660, 1])\npruning_in_iteration open status: True\nratio of positive domain = 348 / 660 = 0.5272727272727273\npruning-in-iteration extra time: 0.034659385681152344\nTensors transferred: pre=78.6328M lA=18.5859M alpha=4.2939M beta=0.0126M\nThis batch time : update_bounds func: 1.4784\t prepare: 0.0313\t bound: 1.3605\t transfer: 0.0758\t finalize: 0.0101\nAccumulated time: update_bounds func: 12.1728\t prepare: 0.1717\t bound: 11.4642\t transfer: 0.4816\t finalize: 0.0519\nbatch bounding time:  1.478602409362793\nCurrent worst splitting domains lb-rhs (depth):\n-0.03540 (15), -0.03313 (15), -0.03182 (15), -0.03176 (15), -0.03135 (15), -0.03046 (15), -0.02893 (15), -0.02885 (15), -0.02852 (15), -0.02851 (15), -0.02840 (15), -0.02831 (15), -0.02783 (15), -0.02707 (15), -0.02668 (15), -0.02628 (15), -0.02600 (15), -0.02578 (15), -0.02564 (15), -0.02542 (15), \nlength of domains: 312\nTotal time: 1.7417\t pickout: 0.0117\t decision: 0.2040\t get_bound: 1.4787\t add_domain: 0.0473\nAccumulated time:\t pickout: 0.0655\t decision: 3.1513\t get_bound: 12.1821\t add_domain: 0.2739\nCurrent (lb-rhs): -0.03539681434631348\n1424 domains visited\nCumulative time: 15.942818403244019\n\nbatch:  torch.Size([312, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([312, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1876] [5, 454] [4, 251] [4, 251] [5, 354] [4, 348] [4, 348] [5, 354] [3, 1876] [4, 329] \n(624, 3, 32, 32) torch.Size([624, 1, 10]) torch.Size([624, 1])\npruning_in_iteration open status: True\nratio of positive domain = 271 / 624 = 0.4342948717948718\npruning-in-iteration extra time: 0.0359799861907959\nTensors transferred: pre=74.3438M lA=21.0283M alpha=4.0597M beta=0.0125M\nThis batch time : update_bounds func: 1.5552\t prepare: 0.0291\t bound: 1.4569\t transfer: 0.0579\t finalize: 0.0107\nAccumulated time: update_bounds func: 13.7280\t prepare: 0.2008\t bound: 12.9212\t transfer: 0.5394\t finalize: 0.0626\nbatch bounding time:  1.5553524494171143\nCurrent worst splitting domains lb-rhs (depth):\n-0.03438 (16), -0.03222 (16), -0.03080 (16), -0.03072 (16), -0.03030 (16), -0.02945 (16), -0.02792 (16), -0.02784 (16), -0.02758 (16), -0.02749 (16), -0.02735 (16), -0.02725 (16), -0.02681 (16), -0.02634 (16), -0.02595 (16), -0.02564 (16), -0.02526 (16), -0.02494 (16), -0.02473 (16), -0.02457 (16), \nlength of domains: 353\nTotal time: 1.7974\t pickout: 0.0115\t decision: 0.1848\t get_bound: 1.5554\t add_domain: 0.0457\nAccumulated time:\t pickout: 0.0770\t decision: 3.3360\t get_bound: 13.7375\t add_domain: 0.3196\nCurrent (lb-rhs): -0.03438138961791992\n1695 domains visited\nCumulative time: 17.743295669555664\n\nbatch:  torch.Size([353, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([353, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [4, 55] [3, 1876] [4, 329] [4, 329] [4, 329] [5, 40] [5, 40] [5, 40] [4, 251] [4, 251] \n(706, 3, 32, 32) torch.Size([706, 1, 10]) torch.Size([706, 1])\npruning_in_iteration open status: True\nratio of positive domain = 328 / 706 = 0.46458923512747874\npruning-in-iteration extra time: 0.038437604904174805\nTensors transferred: pre=84.1133M lA=22.5176M alpha=4.5932M beta=0.0148M\nThis batch time : update_bounds func: 1.8659\t prepare: 0.0345\t bound: 1.7431\t transfer: 0.0768\t finalize: 0.0109\nAccumulated time: update_bounds func: 15.5939\t prepare: 0.2353\t bound: 14.6642\t transfer: 0.6162\t finalize: 0.0735\nbatch bounding time:  1.8661003112792969\nCurrent worst splitting domains lb-rhs (depth):\n-0.03352 (17), -0.03126 (17), -0.02996 (17), -0.02949 (17), -0.02940 (17), -0.02893 (17), -0.02857 (17), -0.02701 (17), -0.02697 (17), -0.02655 (17), -0.02652 (17), -0.02643 (17), -0.02638 (17), -0.02599 (17), -0.02541 (17), -0.02507 (17), -0.02480 (17), -0.02435 (17), -0.02399 (17), -0.02387 (17), \nlength of domains: 378\nTotal time: 6.9872\t pickout: 0.0127\t decision: 5.0560\t get_bound: 1.8662\t add_domain: 0.0523\nAccumulated time:\t pickout: 0.0897\t decision: 8.3920\t get_bound: 15.6037\t add_domain: 0.3719\nCurrent (lb-rhs): -0.03351747989654541\n2023 domains visited\nCumulative time: 24.733479261398315\n\nbatch:  torch.Size([378, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([378, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 454] [5, 313] [4, 348] [5, 454] [5, 40] [3, 3316] [3, 3316] [5, 454] [5, 201] [5, 201] \n(756, 3, 32, 32) torch.Size([756, 1, 10]) torch.Size([756, 1])\npruning_in_iteration open status: True\nratio of positive domain = 335 / 756 = 0.44312169312169314\npruning-in-iteration extra time: 0.03327465057373047\nTensors transferred: pre=90.0703M lA=25.0791M alpha=4.9185M beta=0.0173M\nThis batch time : update_bounds func: 1.7900\t prepare: 0.0357\t bound: 1.6581\t transfer: 0.0843\t finalize: 0.0112\nAccumulated time: update_bounds func: 17.3839\t prepare: 0.2710\t bound: 16.3224\t transfer: 0.7005\t finalize: 0.0847\nbatch bounding time:  1.7902543544769287\nCurrent worst splitting domains lb-rhs (depth):\n-0.03262 (18), -0.03037 (18), -0.02908 (18), -0.02860 (18), -0.02845 (18), -0.02803 (18), -0.02768 (18), -0.02606 (18), -0.02603 (18), -0.02568 (18), -0.02562 (18), -0.02549 (18), -0.02546 (18), -0.02513 (18), -0.02448 (18), -0.02417 (18), -0.02393 (18), -0.02346 (18), -0.02311 (18), -0.02309 (18), \nlength of domains: 421\nTotal time: 7.2975\t pickout: 0.0132\t decision: 5.4410\t get_bound: 1.7903\t add_domain: 0.0530\nAccumulated time:\t pickout: 0.1029\t decision: 13.8330\t get_bound: 17.3940\t add_domain: 0.4249\nCurrent (lb-rhs): -0.032624125480651855\n2358 domains visited\nCumulative time: 32.03463363647461\n\nbatch:  torch.Size([421, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([421, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 313] [5, 40] [5, 40] [3, 3316] [5, 454] [5, 454] [3, 3316] [3, 3316] [3, 3316] [4, 329] \n(842, 3, 32, 32) torch.Size([842, 1, 10]) torch.Size([842, 1])\npruning_in_iteration open status: True\nratio of positive domain = 381 / 842 = 0.45249406175771967\npruning-in-iteration extra time: 0.0571599006652832\nTensors transferred: pre=100.3164M lA=27.4619M alpha=5.4780M beta=0.0209M\nThis batch time : update_bounds func: 2.0975\t prepare: 0.0708\t bound: 1.9034\t transfer: 0.1089\t finalize: 0.0134\nAccumulated time: update_bounds func: 19.4814\t prepare: 0.3418\t bound: 18.2258\t transfer: 0.8094\t finalize: 0.0981\nbatch bounding time:  2.0976755619049072\nCurrent worst splitting domains lb-rhs (depth):\n-0.03182 (19), -0.02933 (19), -0.02840 (19), -0.02830 (19), -0.02774 (19), -0.02761 (19), -0.02717 (19), -0.02689 (19), -0.02522 (19), -0.02516 (19), -0.02472 (19), -0.02463 (19), -0.02462 (19), -0.02461 (19), -0.02428 (19), -0.02392 (19), -0.02389 (19), -0.02370 (19), -0.02360 (19), -0.02323 (19), \nlength of domains: 461\nTotal time: 7.8845\t pickout: 0.0152\t decision: 5.7101\t get_bound: 2.0978\t add_domain: 0.0614\nAccumulated time:\t pickout: 0.1181\t decision: 19.5431\t get_bound: 19.4918\t add_domain: 0.4863\nCurrent (lb-rhs): -0.03182196617126465\n2739 domains visited\nCumulative time: 39.922614097595215\n\nbatch:  torch.Size([461, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([461, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 124] [5, 454] [3, 1876] [3, 1876] [5, 124] [4, 329] [4, 329] [3, 1876] [5, 40] [5, 124] \n(922, 3, 32, 32) torch.Size([922, 1, 10]) torch.Size([922, 1])\npruning_in_iteration open status: True\nratio of positive domain = 377 / 922 = 0.40889370932754876\npruning-in-iteration extra time: 0.035016775131225586\nTensors transferred: pre=109.8477M lA=32.4658M alpha=5.9985M beta=0.0237M\nThis batch time : update_bounds func: 2.3863\t prepare: 0.0425\t bound: 1.9920\t transfer: 0.3371\t finalize: 0.0139\nAccumulated time: update_bounds func: 21.8677\t prepare: 0.3843\t bound: 20.2178\t transfer: 1.1465\t finalize: 0.1120\nbatch bounding time:  2.3866283893585205\nCurrent worst splitting domains lb-rhs (depth):\n-0.03073 (20), -0.03011 (20), -0.02852 (20), -0.02758 (20), -0.02726 (20), -0.02685 (20), -0.02685 (20), -0.02654 (20), -0.02630 (20), -0.02599 (20), -0.02598 (20), -0.02500 (20), -0.02438 (20), -0.02394 (20), -0.02393 (20), -0.02375 (20), -0.02365 (20), -0.02356 (20), -0.02354 (20), -0.02328 (20), \nlength of domains: 545\nTotal time: 8.9105\t pickout: 0.0160\t decision: 6.4282\t get_bound: 2.3867\t add_domain: 0.0795\nAccumulated time:\t pickout: 0.1342\t decision: 25.9713\t get_bound: 21.8785\t add_domain: 0.5658\nCurrent (lb-rhs): -0.03073251247406006\n3116 domains visited\nCumulative time: 48.836565256118774\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 40] [5, 454] [3, 3286] [3, 1876] [3, 1876] [3, 3627] [4, 329] [5, 96] [5, 96] [4, 329] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 448 / 1024 = 0.4375\npruning-in-iteration extra time: 0.034693002700805664\nTensors transferred: pre=122.0000M lA=34.3125M alpha=6.6621M beta=0.0273M\nThis batch time : update_bounds func: 2.3545\t prepare: 0.0486\t bound: 2.0887\t transfer: 0.2005\t finalize: 0.0160\nAccumulated time: update_bounds func: 24.2222\t prepare: 0.4329\t bound: 22.3065\t transfer: 1.3470\t finalize: 0.1280\nbatch bounding time:  2.354865550994873\nCurrent worst splitting domains lb-rhs (depth):\n-0.02996 (21), -0.02933 (21), -0.02777 (21), -0.02681 (21), -0.02654 (21), -0.02613 (21), -0.02600 (21), -0.02574 (21), -0.02541 (21), -0.02520 (21), -0.02516 (21), -0.02418 (21), -0.02382 (21), -0.02315 (21), -0.02306 (21), -0.02292 (21), -0.02284 (21), -0.02276 (21), -0.02266 (21), -0.02249 (21), \nlength of domains: 609\nTotal time: 10.0003\t pickout: 0.0178\t decision: 7.5375\t get_bound: 2.3549\t add_domain: 0.0901\nAccumulated time:\t pickout: 0.1520\t decision: 33.5088\t get_bound: 24.2335\t add_domain: 0.6558\nCurrent (lb-rhs): -0.029959917068481445\n3564 domains visited\nCumulative time: 58.8412606716156\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 96] [3, 1876] [5, 96] [4, 55] [3, 3294] [3, 1876] [3, 1876] [4, 55] [5, 96] [5, 96] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 377 / 1024 = 0.3681640625\npruning-in-iteration extra time: 0.034850120544433594\nTensors transferred: pre=122.0000M lA=38.5420M alpha=6.6621M beta=0.0273M\nThis batch time : update_bounds func: 2.5841\t prepare: 0.0483\t bound: 2.2795\t transfer: 0.2036\t finalize: 0.0518\nAccumulated time: update_bounds func: 26.8063\t prepare: 0.4812\t bound: 24.5859\t transfer: 1.5506\t finalize: 0.1798\nbatch bounding time:  2.58441162109375\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02860 (22), -0.02777 (21), -0.02614 (22), -0.02582 (22), -0.02539 (22), -0.02521 (22), -0.02498 (22), -0.02473 (22), -0.02461 (22), -0.02451 (22), -0.02439 (22), -0.02413 (22), -0.02355 (22), -0.02347 (22), -0.02311 (22), -0.02241 (22), -0.02225 (22), -0.02221 (22), -0.02206 (22), \nlength of domains: 744\nTotal time: 9.1290\t pickout: 0.0177\t decision: 6.4278\t get_bound: 2.5845\t add_domain: 0.0990\nAccumulated time:\t pickout: 0.1697\t decision: 39.9366\t get_bound: 26.8180\t add_domain: 0.7548\nCurrent (lb-rhs): -0.029248714447021484\n3941 domains visited\nCumulative time: 67.97448539733887\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 96] [5, 124] [5, 313] [3, 3316] [5, 96] [5, 454] [3, 3316] [3, 1883] [3, 1883] [3, 1883] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 359 / 1024 = 0.3505859375\npruning-in-iteration extra time: 0.04036521911621094\nTensors transferred: pre=122.0000M lA=39.6143M alpha=6.6621M beta=0.0283M\nThis batch time : update_bounds func: 3.2470\t prepare: 0.0545\t bound: 2.7209\t transfer: 0.4546\t finalize: 0.0157\nAccumulated time: update_bounds func: 30.0532\t prepare: 0.5357\t bound: 27.3069\t transfer: 2.0052\t finalize: 0.1955\nbatch bounding time:  3.247286796569824\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02792 (23), -0.02777 (21), -0.02660 (23), -0.02582 (22), -0.02531 (23), -0.02498 (22), -0.02467 (23), -0.02451 (22), -0.02451 (23), -0.02447 (23), -0.02413 (22), -0.02389 (23), -0.02386 (23), -0.02370 (23), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (23), -0.02284 (23), \nlength of domains: 897\nTotal time: 11.1101\t pickout: 0.0178\t decision: 7.7263\t get_bound: 3.2474\t add_domain: 0.1187\nAccumulated time:\t pickout: 0.1874\t decision: 47.6629\t get_bound: 30.0654\t add_domain: 0.8735\nCurrent (lb-rhs): -0.029248714447021484\n4300 domains visited\nCumulative time: 79.0896646976471\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1883] [3, 1883] [5, 96] [3, 1883] [3, 1883] [5, 454] [5, 454] [3, 1883] [5, 454] [5, 40] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 290 / 1024 = 0.283203125\npruning-in-iteration extra time: 0.03539872169494629\nTensors transferred: pre=122.0000M lA=43.7246M alpha=6.6621M beta=0.0283M\nThis batch time : update_bounds func: 3.1081\t prepare: 0.0476\t bound: 2.6963\t transfer: 0.3469\t finalize: 0.0164\nAccumulated time: update_bounds func: 33.1613\t prepare: 0.5833\t bound: 30.0032\t transfer: 2.3521\t finalize: 0.2118\nbatch bounding time:  3.108431577682495\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02711 (24), -0.02660 (23), -0.02616 (24), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02413 (22), -0.02397 (24), -0.02389 (23), -0.02386 (24), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02307 (24), -0.02293 (24), \nlength of domains: 1119\nTotal time: 10.7098\t pickout: 0.0179\t decision: 6.8448\t get_bound: 3.1085\t add_domain: 0.7386\nAccumulated time:\t pickout: 0.2053\t decision: 54.5077\t get_bound: 33.1739\t add_domain: 1.6121\nCurrent (lb-rhs): -0.029248714447021484\n4590 domains visited\nCumulative time: 89.80370616912842\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 40] [3, 1258] [5, 40] [5, 40] [5, 40] [3, 1868] [3, 3051] [5, 454] [5, 40] [5, 40] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 341 / 1024 = 0.3330078125\npruning-in-iteration extra time: 0.04985809326171875\nTensors transferred: pre=122.0000M lA=40.6865M alpha=6.6621M beta=0.0303M\nThis batch time : update_bounds func: 3.7809\t prepare: 0.0648\t bound: 3.1564\t transfer: 0.5410\t finalize: 0.0179\nAccumulated time: update_bounds func: 36.9423\t prepare: 0.6481\t bound: 33.1595\t transfer: 2.8931\t finalize: 0.2298\nbatch bounding time:  3.781344175338745\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02563 (25), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02413 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02348 (25), -0.02327 (25), -0.02311 (22), -0.02309 (24), -0.02258 (25), -0.02238 (25), \nlength of domains: 1290\nTotal time: 10.9902\t pickout: 0.0190\t decision: 7.0683\t get_bound: 3.7814\t add_domain: 0.1214\nAccumulated time:\t pickout: 0.2243\t decision: 61.5760\t get_bound: 36.9553\t add_domain: 1.7335\nCurrent (lb-rhs): -0.029248714447021484\n4931 domains visited\nCumulative time: 100.79833889007568\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 3294] [5, 313] [4, 191] [3, 3051] [3, 3294] [4, 316] [3, 1883] [3, 1883] [3, 3051] [4, 191] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 370 / 1024 = 0.361328125\npruning-in-iteration extra time: 0.03614473342895508\nTensors transferred: pre=122.0000M lA=38.9590M alpha=6.6621M beta=0.0293M\nThis batch time : update_bounds func: 2.6506\t prepare: 0.0488\t bound: 2.3947\t transfer: 0.1864\t finalize: 0.0198\nAccumulated time: update_bounds func: 39.5929\t prepare: 0.6969\t bound: 35.5542\t transfer: 3.0795\t finalize: 0.2496\nbatch bounding time:  2.651118040084839\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02513 (26), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02413 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02301 (26), -0.02275 (26), -0.02230 (23), -0.02202 (26), \nlength of domains: 1432\nTotal time: 9.8415\t pickout: 0.0180\t decision: 7.0405\t get_bound: 2.6512\t add_domain: 0.1317\nAccumulated time:\t pickout: 0.2423\t decision: 68.6165\t get_bound: 39.6065\t add_domain: 1.8652\nCurrent (lb-rhs): -0.029248714447021484\n5301 domains visited\nCumulative time: 110.64555764198303\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 1883] [5, 124] [5, 124] [5, 124] [3, 654] [5, 124] [4, 191] [5, 40] [3, 3286] [4, 191] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 351 / 1024 = 0.3427734375\npruning-in-iteration extra time: 0.03739166259765625\nTensors transferred: pre=122.0000M lA=40.0908M alpha=6.6621M beta=0.0303M\nThis batch time : update_bounds func: 2.7899\t prepare: 0.0485\t bound: 2.4183\t transfer: 0.3063\t finalize: 0.0159\nAccumulated time: update_bounds func: 42.3828\t prepare: 0.7455\t bound: 37.9726\t transfer: 3.3858\t finalize: 0.2655\nbatch bounding time:  2.7901904582977295\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02463 (27), -0.02451 (22), -0.02413 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02274 (27), -0.02255 (27), -0.02230 (23), -0.02227 (27), \nlength of domains: 1593\nTotal time: 9.4785\t pickout: 0.0199\t decision: 6.5075\t get_bound: 2.7903\t add_domain: 0.1608\nAccumulated time:\t pickout: 0.2622\t decision: 75.1241\t get_bound: 42.3968\t add_domain: 2.0260\nCurrent (lb-rhs): -0.029248714447021484\n5652 domains visited\nCumulative time: 120.12824821472168\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 3051] [3, 1667] [2, 14023] [3, 1667] [3, 1667] [3, 1883] [3, 1883] [3, 1883] [5, 124] [5, 313] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 294 / 1024 = 0.287109375\npruning-in-iteration extra time: 0.03413724899291992\nTensors transferred: pre=122.0000M lA=43.4863M alpha=6.6621M beta=0.0332M\nThis batch time : update_bounds func: 2.7817\t prepare: 0.0496\t bound: 2.5016\t transfer: 0.2122\t finalize: 0.0174\nAccumulated time: update_bounds func: 45.1645\t prepare: 0.7951\t bound: 40.4742\t transfer: 3.5980\t finalize: 0.2828\nbatch bounding time:  2.7820370197296143\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02415 (28), -0.02413 (22), -0.02389 (23), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02288 (28), -0.02230 (23), -0.02224 (28), -0.02205 (28), \nlength of domains: 1811\nTotal time: 9.4852\t pickout: 0.0181\t decision: 6.5611\t get_bound: 2.7821\t add_domain: 0.1238\nAccumulated time:\t pickout: 0.2803\t decision: 81.6852\t get_bound: 45.1789\t add_domain: 2.1498\nCurrent (lb-rhs): -0.029248714447021484\n5946 domains visited\nCumulative time: 129.61730027198792\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [3, 654] [3, 3324] [3, 654] [4, 316] [5, 124] [3, 670] [5, 313] [3, 654] [5, 124] [3, 1876] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 352 / 1024 = 0.34375\npruning-in-iteration extra time: 0.03569316864013672\nTensors transferred: pre=122.0000M lA=40.0312M alpha=6.6621M beta=0.0352M\nThis batch time : update_bounds func: 2.8007\t prepare: 0.0514\t bound: 2.5113\t transfer: 0.2187\t finalize: 0.0184\nAccumulated time: update_bounds func: 47.9652\t prepare: 0.8465\t bound: 42.9855\t transfer: 3.8166\t finalize: 0.3012\nbatch bounding time:  2.8009958267211914\nCurrent worst splitting domains lb-rhs (depth):\n-0.02925 (22), -0.02777 (21), -0.02660 (25), -0.02660 (23), -0.02582 (22), -0.02498 (22), -0.02485 (24), -0.02451 (22), -0.02413 (22), -0.02389 (23), -0.02370 (29), -0.02369 (24), -0.02358 (23), -0.02355 (22), -0.02311 (22), -0.02309 (24), -0.02241 (29), -0.02230 (23), -0.02199 (22), -0.02184 (28), \nlength of domains: 1971\nTotal time: 9.5329\t pickout: 0.0182\t decision: 6.5912\t get_bound: 2.8011\t add_domain: 0.1224\nAccumulated time:\t pickout: 0.2986\t decision: 88.2764\t get_bound: 47.9800\t add_domain: 2.2723\nCurrent (lb-rhs): -0.029248714447021484\n6298 domains visited\nCumulative time: 139.154718875885\n\nbatch:  torch.Size([512, 32, 32, 32]) pre split depth:  1\nbatch:  torch.Size([512, 32, 32, 32]) post split depth:  1\nsplitting decisions: \nsplit level 0: [5, 313] [4, 191] [3, 654] [5, 313] [3, 654] [4, 55] [3, 1667] [3, 1883] [5, 313] [3, 681] \n(1024, 3, 32, 32) torch.Size([1024, 1, 10]) torch.Size([1024, 1])\npruning_in_iteration open status: True\nratio of positive domain = 251 / 1024 = 0.2451171875\npruning-in-iteration extra time: 0.03296542167663574\nTensors transferred: pre=122.0000M lA=46.0479M alpha=6.6621M beta=0.0381M\nThis batch time : update_bounds func: 2.9674\t prepare: 0.0516\t bound: 2.7237\t transfer: 0.1741\t finalize: 0.0172\nAccumulated time: update_bounds func: 50.9326\t prepare: 0.8980\t bound: 45.7092\t transfer: 3.9907\t finalize: 0.3184\nbatch bounding time:  2.9677443504333496\nTraceback (most recent call last):\n  File \"abcrown.py\", line 647, in <module>\n    main()\n  File \"abcrown.py\", line 570, in main\n    refined_betas=refined_betas, attack_images=all_adv_candidates, attack_margins=attack_margins)\n  File \"abcrown.py\", line 392, in complete_verifier\n    attack_images=this_spec_attack_images)\n  File \"abcrown.py\", line 206, in bab\n    timeout=timeout, refined_betas=refined_betas, rhs=rhs)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py\", line 561, in relu_bab_parallel\n    stop_func=stop_criterion, multi_spec_keep_func=multi_spec_keep_func)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/batch_branch_and_bound.py\", line 283, in batch_verification\n    branching_decision, rhs, intermediate_betas, check_infeasibility, dom_cs, (2*num_copy)*batch)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/branching_domains.py\", line 536, in add\n    [lb.append(new_lb[right_indexer + batch]) if new_lb is not None else None for lb, new_lb in zip(self.all_lb_alls, lb_alls)]\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/branching_domains.py\", line 536, in <listcomp>\n    [lb.append(new_lb[right_indexer + batch]) if new_lb is not None else None for lb, new_lb in zip(self.all_lb_alls, lb_alls)]\n  File \"/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/tensor_storage.py\", line 70, in append\n    new_tensor = self._allocate(new_size)\n  File \"/home/tristan/.local/share/autoverify/verifiers/abcrown/tool/complete_verifier/tensor_storage.py\", line 51, in _allocate\n    return torch.empty(allocate_shape, dtype=self.dtype, device=self.device, pin_memory=True)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_88_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.8581056594848633",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_88_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "UNSAT",
            "took": "9.035025119781494",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpn69e0i5n.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:08:47 2024 on Cerberus\nInternal results will be saved to /tmp/tmpn69e0i5n.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_88_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.39393386, -0.71249902, -1.62325943, -1.73206186, -0.67567652,\n         -2.47843599, -3.07558155, -1.90466976,  2.34261847, -0.12327250]],\n       device='cuda:0')\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.49835101, -0.59551758, -1.62404847, -1.84851956, -0.67073911,\n          -2.59058094, -3.11322832, -1.92009211,  2.25205374, -0.12188021],\n         [ 0.49835101, -0.59551758, -1.62404847, -1.84851956, -0.67073911,\n          -2.59058094, -3.11322832, -1.92009211,  2.25205374, -0.12188021]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[1.75370276, 2.84757137, 3.87610221, 4.10057354, 2.92279291,\n          4.84263468, 5.36528206, 4.17214584, 2.37393403]]], device='cuda:0')\nnumber of violation:  0\nAttack finished in 1.9560 seconds.\nPGD attack failed\nModel prediction is: tensor([[ 0.39393386, -0.71249902, -1.62325943, -1.73206186, -0.67567652,\n         -2.47843599, -3.07558155, -1.90466976,  2.34261847, -0.12327250]],\n       device='cuda:0')\nlayer /34 using sparse-features alpha with shape [2270]; unstable size 2270; total size 32768 (torch.Size([1, 32, 32, 32]))\nlayer /34 start_node /input.7 using sparse-spec alpha with unstable size 82 total_size 8192 output_shape (32, 16, 16)\nlayer /34 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /34 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /34 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /34 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /36 using sparse-features alpha with shape [82]; unstable size 82; total size 8192 (torch.Size([1, 32, 16, 16]))\nlayer /36 start_node /input.11 using sparse-spec alpha with unstable size 52 total_size 64 output_shape 64\nlayer /36 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /36 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /36 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /38 using sparse-features alpha with shape [1004]; unstable size 1004; total size 16384 (torch.Size([1, 64, 16, 16]))\nlayer /38 start_node /input.15 using sparse-spec alpha with unstable size 71 total_size 4096 output_shape (64, 8, 8)\nlayer /38 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /38 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /40 using sparse-features alpha with shape [71]; unstable size 71; total size 4096 (torch.Size([1, 64, 8, 8]))\nlayer /40 start_node /input.19 using sparse-spec alpha with unstable size 35 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /40 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /43 using sparse-features alpha with shape [35]; unstable size 35; total size 512 (torch.Size([1, 512]))\nlayer /43 start_node /input.23 using sparse-spec alpha with unstable size 68 total_size 512 output_shape torch.Size([512])\nlayer /43 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nlayer /45 using sparse-features alpha with shape [68]; unstable size 68; total size 512 (torch.Size([1, 512]))\nlayer /45 start_node /46 using full alpha with unstable size None total_size 9 output_shape 9\nOptimizable variables initialized.\ninitial CROWN bounds: tensor([[1.14224005, 1.87831354, 2.95792007, 3.11696863, 1.87635279, 3.81624746,\n         4.26846075, 2.82569957, 1.48256779]], device='cuda:0') None\nverified with init bound!\nResult: unsat\nTime: 7.398517370223999\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_97_eps_0.00784",
            "timeout": "300",
            "verifier": "nnenum",
            "config": "Configuration(values={\n  'BRANCH_MODE': 0,\n  'COMPRESS_INIT_BOX': True,\n  'CONTRACT_LP_OPTIMIZED': True,\n  'CONTRACT_LP_TRACK_WITNESSES': True,\n  'CONTRACT_ZONOTOPE': False,\n  'CONTRACT_ZONOTOPE_LP': True,\n  'EAGER_BOUNDS': True,\n  'GLPK_FIRST_PRIMAL': True,\n  'GLPK_RESET_BEFORE_MINIMIZE': False,\n  'GLPK_TIMEOUT': 60,\n  'INF_OVERAPPROX_LP_TIMEOUT': False,\n  'INF_OVERAPPROX_MIN_GEN_LIMIT': False,\n  'OFFLOAD_CLOSEST_TO_ROOT': True,\n  'OVERAPPROX_BOTH_BOUNDS': False,\n  'OVERAPPROX_GEN_LIMIT_MULTIPLIER': 1.5,\n  'OVERAPPROX_LP_TIMEOUT': 1.0,\n  'OVERAPPROX_MIN_GEN_LIMIT': 50,\n  'OVERAPPROX_NEAR_ROOT_MAX_SPLITS': 2,\n  'SINGLE_SET': False,\n  'SKIP_COMPRESSED_CHECK': False,\n  'SKIP_CONSTRAINT_NORMALIZATION': False,\n  'SPLIT_IF_IDLE': True,\n  'SPLIT_ORDER': 1,\n  'SPLIT_TOLERANCE': 1e-08,\n  'TRY_QUICK_OVERAPPROX': True,\n})",
            "success": "ERR",
            "result": "ERR",
            "took": "0.8441693782806396",
            "stderr": "",
            "stdout": "Traceback (most recent call last):\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 175, in main\n    network = load_onnx_network_optimized(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 294, in load_onnx_network_optimized\n    init = init_map[cur_node.input[1]]\nKeyError: '15'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/tristan/miniconda3/envs/__av__nnenum/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 262, in <module>\n    main()\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/nnenum.py\", line 178, in main\n    network = load_onnx_network(onnx_filename)\n  File \"/home/tristan/.local/share/autoverify/verifiers/nnenum/tool/src/nnenum/onnx_network.py\", line 764, in load_onnx_network\n    assert o in Settings.ONNX_WHITELIST, f\"Onnx model contains node with op {o}, which may not be a linear operation. \" + \\\nAssertionError: Onnx model contains node with op Div, which may not be a linear operation. Updated Settings.WHITELIST if you want to override this.\n"
        },
        {
            "network": "convBigRELU__PGD",
            "property": "cifar10_spec_idx_97_eps_0.00784",
            "timeout": "300",
            "verifier": "abcrown",
            "config": "Configuration(values={\n  'attack__attack_mode': 'PGD',\n  'attack__enable_mip_attack': False,\n  'attack__pgd_order': 'before',\n  'bab__branching__input_split__enable': False,\n  'bab__branching__method': 'kfsb',\n  'bab__branching__reduceop': 'min',\n  'general__complete_verifier': 'bab',\n  'general__enable_incomplete_verification': True,\n  'general__loss_reduction_func': 'sum',\n  'solver__bound_prop_method': 'alpha-crown',\n})",
            "success": "OK",
            "result": "SAT",
            "took": "5.385883569717407",
            "stderr": "",
            "stdout": "/bin/bash: /home/tristan/miniconda3/envs/__av__abcrown/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nConfigurations:\n\ngeneral:\n  device: cuda\n  seed: 100\n  conv_mode: patches\n  deterministic: false\n  double_fp: false\n  loss_reduction_func: sum\n  record_bounds: false\n  sparse_alpha: true\n  save_adv_example: true\n  precompile_jit: false\n  complete_verifier: bab\n  enable_incomplete_verification: true\n  csv_name: null\n  results_file: /tmp/tmpr2129vv4.txt\n  root_path: ''\nmodel:\n  name: null\n  path: null\n  onnx_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\n  onnx_path_prefix: ''\n  cache_onnx_conversion: false\n  onnx_quirks: null\n  input_shape: null\n  onnx_loader: default_onnx_and_vnnlib_loader\n  onnx_optimization_flags: none\ndata:\n  start: 0\n  end: 10000\n  select_instance: null\n  num_outputs: 10\n  mean: 0.0\n  std: 1.0\n  pkl_path: null\n  dataset: CIFAR\n  data_filter_path: null\n  data_idx_file: null\nspecification:\n  type: lp\n  robustness_type: verified-acc\n  norm: .inf\n  epsilon: null\n  vnnlib_path: /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib\n  vnnlib_path_prefix: ''\nsolver:\n  batch_size: 512\n  min_batch_size_ratio: 0.1\n  use_float64_in_last_iteration: false\n  early_stop_patience: 10\n  start_save_best: 0.5\n  bound_prop_method: alpha-crown\n  prune_after_crown: false\n  crown:\n    batch_size: 1000000000\n    max_crown_size: 1000000000\n  alpha-crown:\n    alpha: true\n    lr_alpha: 0.1\n    iteration: 100\n    share_slopes: false\n    no_joint_opt: false\n    lr_decay: 0.98\n    full_conv_alpha: true\n  beta-crown:\n    lr_alpha: 0.01\n    lr_beta: 0.05\n    lr_decay: 0.98\n    optimizer: adam\n    iteration: 50\n    beta: true\n    beta_warmup: true\n    enable_opt_interm_bounds: false\n    all_node_split_LP: false\n  forward:\n    refine: false\n    dynamic: false\n    max_dim: 10000\n  multi_class:\n    multi_class_method: allclass_domain\n    label_batch_size: 32\n    skip_with_refined_bound: true\n  mip:\n    parallel_solvers: null\n    solver_threads: 1\n    refine_neuron_timeout: 15\n    refine_neuron_time_percentage: 0.8\n    early_stop: true\n    adv_warmup: true\n    mip_solver: gurobi\nbab:\n  initial_max_domains: 1\n  max_domains: .inf\n  decision_thresh: 0\n  timeout: 300.0\n  timeout_scale: 1\n  override_timeout: null\n  get_upper_bound: false\n  dfs_percent: 0.0\n  pruning_in_iteration: true\n  pruning_in_iteration_ratio: 0.2\n  sort_targets: false\n  batched_domain_list: true\n  optimized_intermediate_layers: ''\n  interm_transfer: true\n  cut:\n    enabled: false\n    bab_cut: false\n    lp_cut: false\n    method: null\n    lr: 0.01\n    lr_decay: 1.0\n    iteration: 100\n    bab_iteration: -1\n    early_stop_patience: -1\n    lr_beta: 0.02\n    number_cuts: 50\n    topk_cuts_in_filter: 100\n    batch_size_primal: 100\n    max_num: 1000000000\n    patches_cut: false\n    cplex_cuts: false\n    cplex_cuts_wait: 0\n    cplex_cuts_revpickup: true\n    cut_reference_bounds: true\n    fix_intermediate_bounds: false\n  branching:\n    method: kfsb\n    candidates: 3\n    reduceop: min\n    sb_coeff_thresh: 0.001\n    input_split:\n      enable: false\n      enhanced_bound_prop_method: alpha-crown\n      enhanced_branching_method: naive\n      enhanced_bound_patience: 100000000.0\n      attack_patience: 100000000.0\n      adv_check: 0\n      sort_domain_interval: -1\n  attack:\n    enabled: false\n    beam_candidates: 8\n    beam_depth: 7\n    max_dive_fix_ratio: 0.8\n    min_local_free_ratio: 0.2\n    mip_start_iteration: 5\n    mip_timeout: 30.0\n    adv_pool_threshold: null\n    refined_mip_attacker: false\n    refined_batch_size: null\nattack:\n  pgd_order: before\n  pgd_steps: 100\n  pgd_restarts: 30\n  pgd_early_stop: true\n  pgd_lr_decay: 0.99\n  pgd_alpha: auto\n  pgd_loss_mode: null\n  enable_mip_attack: false\n  cex_path: ./test_cex.txt\n  attack_mode: PGD\n  gama_lambda: 10.0\n  gama_decay: 0.9\n  check_clean: false\n  input_split:\n    pgd_steps: 100\n    pgd_restarts: 30\n    pgd_alpha: auto\n  input_split_enhanced:\n    pgd_steps: 200\n    pgd_restarts: 5000000\n    pgd_alpha: auto\n  input_split_check_adv:\n    pgd_steps: 5\n    pgd_restarts: 5\n    pgd_alpha: auto\ndebug:\n  lp_test: null\n\nExperiments at Mon May 13 23:08:58 2024 on Cerberus\nInternal results will be saved to /tmp/tmpr2129vv4.txt.\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nUsing onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx\nUsing vnnlib /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib\nPrecompiled vnnlib file found at /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/vnnlib/cifar10_spec_idx_97_eps_0.00784.vnnlib.compiled\nLoading onnx /home/tristan/scriptie/auto-verify-fork/vnncomp/vnncomp2022/benchmarks/cifar2020/onnx/convBigRELU__PGD.onnx wih quirks {}\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n/home/tristan/miniconda3/envs/__av__abcrown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n  \"Using experimental implementation that allows 'batch_size > 1'.\"\nAttack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.001960787922143936, initialization=uniform, GAMA=False\nModel output of first 5 examples:\n tensor([[ 0.96105886, -4.99818325,  0.49796951, -1.34900188,  0.09806482,\n         -1.69785976, -2.05051923, -2.63068223,  0.86458349, -4.01851368]],\n       device='cuda:0')\npgd early stop\nAdv example prediction (first 2 examples and 2 restarts):\n tensor([[[ 0.90111589, -5.00151634,  0.45460641, -1.36289418,  0.06061684,\n          -1.70949721, -2.10207391, -2.64322639,  0.98923618, -3.94865346],\n         [ 0.90111589, -5.00151634,  0.45460641, -1.36289418,  0.06061684,\n          -1.70949721, -2.10207391, -2.64322639,  0.98923618, -3.94865346]]],\n       device='cuda:0')\nPGD attack margin (first 2 examles and 10 specs):\n tensor([[[ 5.90263224,  0.44650948,  2.26400995,  0.84049904,  2.61061311,\n           3.00318980,  3.54434228, -0.08812028,  4.84976959]]],\n       device='cuda:0')\nnumber of violation:  1\nAttack finished in 0.9923 seconds.\nPGD attack succeeded!\nResult: sat\nTime: 3.802003860473633\n"
        }
    ]
}